{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cdmyHksS6j7"
      },
      "source": [
        "### Full Name :moein samadi azad\n",
        "### Student Number :400105093\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnejUQ0nS6j8"
      },
      "source": [
        "# Problem\n",
        "In this project our goal is to develop a framework for simple neural network and multi layer perceptron. We are going to use only `numpy` and no other packages to build our own classes and network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuBQRToS6j8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "D_a46D-Gz3Rj"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "#  Do Not Add any other packages  #\n",
        "###################################\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import copy\n",
        "from abc import ABC, abstractmethod\n",
        "from sklearn.datasets import fetch_openml # just for downloading the dataset (MNIST)\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q81QcH9YS6j9"
      },
      "source": [
        "# Section 1: Modules implementation (65 Points)\n",
        "We are going to implement the required modules for a neural network. Each of these modules must implement the necessary functions, `_forward` and `backward`. In the following parts, we will implement the `LinearLayer`, `ReLU`, `batchnorm`, `dropout`, and `SoftMax` layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6ZtTQFS6j9"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nqh0jKU4TofC"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "#     Do Not change this cell     #\n",
        "###################################\n",
        "\n",
        "class Module(ABC):\n",
        "    def __call__(self, *args):\n",
        "        return self._forward(*args)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _forward(self, *args):\n",
        "        pass\n",
        "        \n",
        "\n",
        "def rel_error(x, y):\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w4gw0iTS6j9"
      },
      "source": [
        "#### Linear Layer (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_JnFHPpQo5H"
      },
      "source": [
        "The `LinearLayer` class represents a linear (fully connected) layer in a neural network. This layer computes the linear transformation of the input data as Wx + b, where W is the weight matrix, x is the input data, and b is the bias vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "moGEgpV999cl"
      },
      "outputs": [],
      "source": [
        "class LinearLayer(Module):\n",
        "    \"\"\"\n",
        "    A linear layer module that calculates (Wx + b).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim_in, dim_out, initializer, reg, alpha):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - dim_in: input dimension,\n",
        "            - dim_out: output dimension,\n",
        "            - initializer: a function that takes (dim_in, dim_out) and initialize a [dim_in x dim_out] matrix,\n",
        "            - reg: L2-regularization flag\n",
        "            - alpha: L2-regularization coefficient\n",
        "        \"\"\"\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.params = {\n",
        "            'W': initializer(dim_in, dim_out),\n",
        "            'b': np.zeros(dim_out),\n",
        "            'reg': alpha if reg else 0,\n",
        "        }\n",
        "        self.grads = dict()\n",
        "        self.cache = dict()\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        linear forward function, calculates Wx+b for a batch of data\n",
        "\n",
        "        Args:\n",
        "            x : a batch of data\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Note:\n",
        "            you need to store some values in cache to be able to\n",
        "            calculate backward path.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##        Forward function for linear layer        ##\n",
        "        ####################[Your Code]###################### \n",
        "        out = x @ self.params[\"W\"] + self.params[\"b\"]\n",
        "        self.cache[\"x\"] = x\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        gets upstream gradient and returns downstream gradient\n",
        "\n",
        "        Args:\n",
        "            upstream : upstream gradient of loss w.r.t module output\n",
        "\n",
        "        Note:\n",
        "            you need to calculate gradient of loss w.r.t module input\n",
        "            and parameters and store them in grads (don't return anything).\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##        Backward function for linear layer       ##\n",
        "        ####################[Your Code]######################\n",
        "        W = self.params[\"W\"]\n",
        "        x = self.cache[\"x\"]\n",
        "        \n",
        "        self.grads[\"x\"] = upstream @ W.T \n",
        "        self.grads[\"W\"] = x.T @ upstream\n",
        "        self.grads[\"reg\"] = 2 * self.params['reg'] * W\n",
        "        self.grads[\"b\"] = upstream.sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "G77b71yOTxvA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative error of delta-loss (for linear unit):\n",
            "1.064447248348604e-05\n",
            "Relative error of delta-loss (for regularization):\n",
            "9.417674890879862e-06\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "#     Do not change this cell     #\n",
        "#   output must be from o(e-5)    #\n",
        "###################################\n",
        "\n",
        "def check_gradient_linear(linear, h=0.00001):\n",
        "    np.random.seed(121212)\n",
        "    x = np.random.normal(size=(20, linear.dim_in))\n",
        "    upstream = np.random.random(size=(20, linear.dim_out))\n",
        "\n",
        "    new_x = x + h\n",
        "    new_w = linear.params['W'] + h\n",
        "    new_b = linear.params['b'] + h\n",
        "    new_linear = copy.deepcopy(linear)\n",
        "    new_linear.params['W'] = new_w\n",
        "    new_linear.params['b'] = new_b\n",
        "    new_val = new_linear(new_x)\n",
        "    old_val = linear(x)\n",
        "    delta_output = new_val - old_val\n",
        "    delta_loss_indirect = np.sum(delta_output * upstream)\n",
        "\n",
        "    linear.backward(upstream)\n",
        "    delta_loss_direct = np.sum(h * linear.grads['x'])\n",
        "    delta_loss_direct += np.sum(h * linear.grads['W'])\n",
        "    delta_loss_direct += np.sum(h * linear.grads['b'])\n",
        "\n",
        "    print(f'Relative error of delta-loss (for linear unit):\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')\n",
        "\n",
        "    old_val = np.sum(np.power(linear.params['W'], 2))\n",
        "    new_val = np.sum(np.power(new_w, 2))\n",
        "    indirect_loss = new_val - old_val\n",
        "    direct_loss = np.sum(h * linear.grads['reg'])\n",
        "    print(f'Relative error of delta-loss (for regularization):\\n{rel_error(indirect_loss, direct_loss)}')\n",
        "\n",
        "def initializer(x, y): return np.random.normal(size=(x, y))\n",
        "\n",
        "linear = LinearLayer(5, 10, initializer, reg=True, alpha=1)\n",
        "check_gradient_linear(linear, h=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUJ781r6S6j_"
      },
      "source": [
        "#### ReLU Layer (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QxT-LT0ROHq"
      },
      "source": [
        "The `ReLU` class represents a Rectified Linear Unit (ReLU) activation function, which is commonly used in neural networks to introduce non-linearity. This method applies the ReLU activation function to the input data x. The ReLU function is defined as ReLU(x) = max(0, x) meaning that it outputs the input directly if it is positive, otherwise it outputs zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RSsLumM29_eT"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit function\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        applies relu function on x\n",
        "\n",
        "        Args:\n",
        "            x : a batch of data\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Returns:\n",
        "            y : relu of input\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##         Forward function for ReLU layer         ##\n",
        "        ####################[Your Code]######################\n",
        "        y = np.maximum(0, x)\n",
        "        self.cache['x'] = x\n",
        "        return y\n",
        "    \n",
        "\n",
        "        \n",
        "\n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        calculate and store gradient of loss w.r.t module input\n",
        "\n",
        "        Args:\n",
        "            upstream : gradient of loss w.r.t modele output\n",
        "\n",
        "        Note:\n",
        "            you need to calculate gradient of loss w.r.t module input\n",
        "            and parameters and store them in grads (don't return anything).\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##         Backward function for ReLU layer        ##\n",
        "        ####################[Your Code]######################\n",
        "        x = self.cache['x']\n",
        "        self.grads['x'] = (x > 0) * upstream\n",
        "        return self.grads['x']\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1tUjGXccU4FC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative error of delta-loss:\n",
            "2.484727903125079e-14\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "#     Do not change this cell     #\n",
        "#   output must be from o(e-8)    #\n",
        "###################################\n",
        "\n",
        "def check_gradient_relu(rl, h=0.0001):\n",
        "    np.random.seed(11111)\n",
        "    x = np.random.normal(size=(10, 5))\n",
        "    upstream = np.random.random(size=(10, 5))\n",
        "\n",
        "    new_x = x + h\n",
        "    new_val = rl(new_x)\n",
        "    old_val = rl(x)\n",
        "    delta_output = new_val - old_val\n",
        "    delta_loss_indirect = np.sum(delta_output * upstream)\n",
        "\n",
        "    rl.backward(upstream)\n",
        "    delta_loss_direct = np.sum(h * rl.grads['x'])\n",
        "\n",
        "    print(f'Relative error of delta-loss:\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')\n",
        "\n",
        "relu = ReLU()\n",
        "check_gradient_relu(relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNHxH2KFS6kA"
      },
      "source": [
        "#### SoftMax Layer (10 Points)\n",
        "\n",
        "We could have a layer that calculates softmax for us. In other word, for input $x\\in\\mathcal{R}^N$ it would return $y\\in\\mathcal{R}^n$ where $y_i = \\frac{e^{x_i}}{\\sum e^{x_i}}$. But this method is not numerical stable because $e^{x_i}$ in this formulation can get very large easly and return `nan`. Instead of that we will implement a logarithmic version of softmax which instead of calculating $\\frac{e^{x_i}}{\\sum e^{x_i}}$, will calculate $\\log\\left(\\frac{e^{x_i}}{\\sum e^{x_i}}\\right) = x_i - \\log\\sum e^{x_i}$. In order to calculate second term you can use `np.logaddexp` but this function only works on two inputs. For more than two inputs, fill in the following function to be able to calculate log sum exp of an array of shape (b,n). `axis=1` means sum over columns and `axis=0` sum over rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6kBopBY9S6kA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def logsumexp(array, axis=1):\n",
        "    \"\"\"\n",
        "    Calculate log(sum(exp(array))) using np.logaddexp to ensure numerical stability.\n",
        "\n",
        "    Args:\n",
        "        array : input array\n",
        "        axis : reduce axis, 1 means columns and 0 means rows\n",
        "    \"\"\"\n",
        "    # Get the maximum value along the specified axis to improve numerical stability\n",
        "    max_val = np.max(array, axis=axis, keepdims=True)\n",
        "\n",
        "    # Subtract the maximum value to prevent exponential overflow\n",
        "    stable_array = array - max_val\n",
        "\n",
        "    # Exponentiate the stabilized array\n",
        "    exp_array = np.exp(stable_array)\n",
        "\n",
        "    # Sum the exponentiated values along the specified axis\n",
        "    sum_exp = np.sum(exp_array, axis=axis, keepdims=True)\n",
        "\n",
        "    # Compute log of sum of exponentials and then add back the max value used for stabilization\n",
        "    result = np.log(sum_exp) + max_val\n",
        "\n",
        "    # Remove the extra dimension added by keepdims if necessary\n",
        "    if not np.isscalar(result):\n",
        "        result = np.squeeze(result, axis=axis)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "R4v5_UBB-BCK"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        get x and calculate softmax of that.\n",
        "\n",
        "        Args:\n",
        "            x : batch of data with shape (b,m)\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Returns:\n",
        "            y : log softmax of x with shape (b,m)\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##      Forward function for LogSoftMax layer      ##\n",
        "        ####################[Your Code]######################\n",
        "        max_x = np.max(x, axis=1, keepdims=True)\n",
        "        stabilized_x = x - max_x\n",
        "        sum_exp = np.sum(np.exp(stabilized_x), axis=1, keepdims=True)\n",
        "        log_sum_exp = np.log(sum_exp)\n",
        "        log_softmax = stabilized_x - log_sum_exp\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.cache['x'] = x\n",
        "            self.cache['log_softmax'] = log_softmax\n",
        "\n",
        "        return log_softmax\n",
        "\n",
        "        \n",
        "\n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        calculate gradient of loss w.r.t module input and save that in grads.\n",
        "\n",
        "        Args:\n",
        "            upstream : gradient of loss w.r.t module output with sahpe (b,m)\n",
        "\n",
        "        Note:\n",
        "            you need to calculate gradient of loss w.r.t module input\n",
        "            and parameters and store them in grads (don't return anything).\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##     Backward function for LogSoftMax layer      ##\n",
        "        ####################[Your Code]######################\n",
        "        x = self.cache['x']\n",
        "        log_softmax = self.cache['log_softmax']\n",
        "        softmax = np.exp(log_softmax)\n",
        "\n",
        "        # Gradient of the log softmax\n",
        "        dx = softmax * (upstream - np.sum(upstream * softmax, axis=1, keepdims=True))\n",
        "\n",
        "        self.grads['x'] = dx\n",
        "\n",
        "        return dx\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YDqarqd9VDAY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative error of delta-loss:\n",
            "1.3446648037662018e-12\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "#     Do not change this cell     #\n",
        "#   output must be from o(e-7)    #\n",
        "###################################\n",
        "\n",
        "def check_gradient_softmax(sm, h=0.0001):\n",
        "    np.random.seed(321321)\n",
        "    x = np.random.random(size=(10, 5))\n",
        "    upstream = np.random.random(size=(10, 5))\n",
        "\n",
        "    new_x = x + h\n",
        "    new_val = sm(new_x)\n",
        "    old_val = sm(x)\n",
        "    delta_output = new_val - old_val\n",
        "    delta_loss_indirect = np.sum(delta_output * upstream)\n",
        "\n",
        "    sm.backward(upstream)\n",
        "    delta_loss_direct = np.sum(h * sm.grads['x'])\n",
        "\n",
        "    print(f'Relative error of delta-loss:\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')\n",
        "\n",
        "sm = LogSoftMax()\n",
        "check_gradient_softmax(sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRUDR2Dx8WP"
      },
      "source": [
        "#### BatchNorm Layer (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piMwBR-BSH7y"
      },
      "source": [
        "The `BatchNorm` class represents a batch normalization layer, which normalizes the inputs of each mini-batch to have zero mean and unit variance, and then scales and shifts them using learned parameters.\n",
        "<br>\n",
        "In `train` mode, this method:\n",
        "<br>\n",
        "Computes the mean and variance of the current batch. Normalizes the input batch using the batch statistics. Scales and shifts the normalized batch using the `gamma` and `beta` parameters. Updates the running mean and variance using the computed batch statistics.\n",
        "<br>\n",
        "In `valid` or `test` mode, this method:\n",
        "<br>\n",
        "Normalizes the input batch using the running mean and variance. Scales and shifts the normalized batch using the `gamma` and `beta` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nLsDd940S6kA"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(Module):\n",
        "    def __init__(self, dim_in, momentum=0.99, eps=1e-3):\n",
        "        \"\"\"\n",
        "        Batch normalization layer for neural networks.\n",
        "        Args:\n",
        "            dim_in: Input dimension.\n",
        "            momentum: Momentum for moving average of mean and variance.\n",
        "            eps: Small constant to prevent division by zero.\n",
        "        \"\"\"\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "        self.dim_in = dim_in\n",
        "        self.params = {\n",
        "            'gamma': np.ones(dim_in),\n",
        "            'beta': np.zeros(dim_in),\n",
        "            'running_mean': np.zeros(dim_in),\n",
        "            'running_var': np.zeros(dim_in),\n",
        "        }\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        gets x and calculates batchnorm of that.\n",
        "\n",
        "        Args:\n",
        "            x : batch of data with shape (b,m)\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Returns:\n",
        "            y : log batchnorm of x with shape (b,m)\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##      Forward function for Batchnorm layer       ##\n",
        "        ####################[Your Code]######################\n",
        "        if mode == 'train':\n",
        "            self.params['running_mean'] = self.momentum * self.params['running_mean'] + (1 - self.momentum) * np.mean(x, axis=0)\n",
        "            self.params['running_var'] = self.momentum * self.params['running_var'] + (1 - self.momentum) * np.var(x, axis=0)\n",
        "            normalized = (x - np.mean(x, axis=0)) / np.sqrt(np.var(x, axis=0) + self.eps)\n",
        "            self.cache['x'] = x\n",
        "            self.cache['x_normalized'] = normalized\n",
        "            self.cache['batch_mean'] = np.mean(x, axis=0)\n",
        "            self.cache['batch_var'] = np.var(x, axis=0)\n",
        "            return self.params['gamma'] * normalized + self.params['beta']\n",
        "\n",
        "        normalized = (x - self.params['running_mean']) / np.sqrt(self.params['running_var'] + self.eps)\n",
        "        return self.params['gamma'] * normalized + self.params['beta']\n",
        "        \n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        calculates gradient of loss w.r.t module input and save that in grads.\n",
        "\n",
        "        Args:\n",
        "            upstream : gradient of loss w.r.t module output with shape (b,m)\n",
        "\n",
        "        Note:\n",
        "            you need to calculate gradient of loss w.r.t module input\n",
        "            and parameters and store them in grads (don't return anything).\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##      Backward function for Batchnorm layer      ##\n",
        "        ####################[Your Code]######################\n",
        "        x_normalized = self.cache['x_normalized']\n",
        "        batch_mean = self.cache['batch_mean']\n",
        "        batch_var = self.cache['batch_var']\n",
        "\n",
        "\n",
        "        gamma = self.params['gamma']\n",
        "        beta = self.params['beta']\n",
        "\n",
        "        self.grads['beta'] = np.sum(upstream, axis=0)\n",
        "        self.grads['gamma'] = np.sum(upstream * x_normalized, axis=0)\n",
        "\n",
        "        dx_normalized = upstream * gamma\n",
        "        \n",
        "        dvar = np.sum(dx_normalized * (self.cache['x'] - batch_mean) * -0.5 * (batch_var + self.eps) ** -1.5, axis=0)\n",
        "        dmean = np.sum(dx_normalized * -1 / np.sqrt(batch_var + self.eps), axis=0) + dvar * np.mean(-2 * (self.cache['x'] - batch_mean), axis=0)\n",
        "        self.grads['x'] = dx_normalized / np.sqrt(batch_var + self.eps) + dvar * 2 * (self.cache['x'] - batch_mean) / ((self.cache['x']).shape[0]) + dmean / ((self.cache['x']).shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h77q5VHjx_VP"
      },
      "source": [
        "#### Dropout Layer (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbSjVP2OS3hT"
      },
      "source": [
        "The `Dropout` class represents a dropout layer, which randomly sets a fraction of input units to zero during training to prevent overfitting.\n",
        "<br>\n",
        "In `train` mode, this method:\n",
        "<br>\n",
        "Randomly sets a fraction of input units to zero with probability `prob`.\n",
        "<br>\n",
        "In `valid` or `test` mode, this method:\n",
        "<br>\n",
        "Returns the input data unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GwxxHZTkS6kB"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, prob=0.5):\n",
        "        \"\"\"\n",
        "        Dropout layer for neural networks.\n",
        "\n",
        "        Args:\n",
        "            prob: Probability of dropping out a neuron.\n",
        "        \"\"\"\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "        self.prob = prob\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        gets x and calculates dropout of that.\n",
        "\n",
        "        Args:\n",
        "            x : batch of data with shape (b,m)\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Returns:\n",
        "            y : dropout of x with shape (b,m)\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##       Forward function for Dropout layer        ##\n",
        "        ####################[Your Code]######################\n",
        "            # In order to maintain the expected value, the mask should be divided by 1 - self.prob; however, it will still work fine without it\n",
        "        mask = np.random.rand(*x.shape) >= self.prob\n",
        "        out = x * mask\n",
        "        self.cache['mask'] = mask\n",
        "            \n",
        "        return out\n",
        "\n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        calculates gradient of loss w.r.t module input and save that in grads.\n",
        "\n",
        "        Args:\n",
        "            upstream : gradient of loss w.r.t module output with shape (b,m)\n",
        "\n",
        "        Note:\n",
        "            you need to calculate gradient of loss w.r.t module input\n",
        "            and parameters and store them in grads (don't return anything).\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##       Backward function for Dropout layer       ##\n",
        "        ####################[Your Code]######################\n",
        "        \n",
        "        mask = self.cache['mask']\n",
        "        self.grads['x'] = upstream * mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4a58HpGS6kB"
      },
      "source": [
        "## Model (5 Points)\n",
        "We need a model class which gathers our layers togather and performs forward and backward on all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "d8Bi_eXkS6kB"
      },
      "outputs": [],
      "source": [
        "class MLPModel(Module):\n",
        "    \"\"\"\n",
        "    A multilayer neural network model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            layers : list of model layers\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "\n",
        "    def _forward(self, x, mode='train'):\n",
        "        \"\"\"\n",
        "        Performs forward on x\n",
        "\n",
        "        Args:\n",
        "            x : a batch of data\n",
        "            mode: the step of training which can be train, valid and test\n",
        "\n",
        "        Returns:\n",
        "            o : model output\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##                  Forward on x                   ##\n",
        "        ####################[Your Code]######################\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            x = layer._forward(x, mode)\n",
        "        return x\n",
        "\n",
        "    def backward(self, upstream):\n",
        "        \"\"\"\n",
        "        Perform backward path on whole model\n",
        "\n",
        "        Args:\n",
        "            upstream : gradient of loss w.r.t model output\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##                  Backward on x                  ##\n",
        "        ####################[Your Code]######################\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward(upstream)\n",
        "            upstream = layer.grads[\"x\"]\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            parametric_layers : all layers of model which have parameter (you need it for optimizer input)\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##         Find parameters of all layers           ##\n",
        "        ####################[Your Code]######################\n",
        "\n",
        "        parametric_layers = []\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'params'):\n",
        "                parametric_layers.append(layer)\n",
        "        return parametric_layers      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqwj1suVS6kC"
      },
      "source": [
        "## Loss functions (10 Points)\n",
        "We need to implement loss functions to be able to train our network. We will implement CrossEntropy loss function. But notice that we have implemented `LogSoftMax` in logarithmic way so input of the following class will be logarithm of probabilities.\n",
        "<br>\n",
        "The `CrossEntropyLoss` class represents the cross-entropy loss function, which is commonly used in classification tasks to measure the difference between predicted probabilities and true labels. This method calculates the cross-entropy loss between the predicted probabilities (converted to log probabilities) and the true labels. The cross-entropy loss is computed using the formula: $ -\\frac{1}{b} \\sum_{i=1}^{b} \\log(p_{i, \\text{target}_i}) $, where the value inside the parentheses is the predicted probability of the correct class for the ith sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HIBN2fmBpYGG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CrossEntropyLoss(Module):\n",
        "    def __init__(self):\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "\n",
        "    def _forward(self, logprobs, targets):\n",
        "        batch_size = logprobs.shape[0]\n",
        "        # Convert log probabilities to probabilities\n",
        "        probs = np.exp(logprobs)\n",
        "        # Compute the negative log likelihood loss\n",
        "        target_logprobs = logprobs[np.arange(batch_size), targets]\n",
        "        loss = -np.mean(target_logprobs)\n",
        "        # Store for backpropagation\n",
        "        self.cache['logprobs'] = logprobs\n",
        "        self.cache['targets'] = targets\n",
        "        self.cache['batch_size'] = batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self, upstream):\n",
        "        logprobs = self.cache['logprobs']\n",
        "        targets = self.cache['targets']\n",
        "        batch_size = self.cache['batch_size']\n",
        "\n",
        "        # Initialize gradients with respect to log probabilities\n",
        "        dL_dlogprobs = np.zeros_like(logprobs)\n",
        "        dL_dlogprobs[np.arange(batch_size), targets] -= 1 / batch_size\n",
        "        # Multiply by upstream gradient\n",
        "        dL_dlogprobs *= upstream\n",
        "\n",
        "        self.grads['x'] = dL_dlogprobs\n",
        "\n",
        "    def __call__(self, logprobs, targets):\n",
        "        return self._forward(logprobs, targets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2neGoIaxVVki"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative error of delta-loss:\n",
            "3.327145416815998e-13\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "#     Do not change this cell     #\n",
        "#   output must be from o(e-10)   #\n",
        "###################################\n",
        "\n",
        "def check_gradient_ce(ce, h=0.0001):\n",
        "    np.random.seed(123123)\n",
        "    target = np.random.randint(5, size=10)\n",
        "    probs = np.random.random(size=(10, 5))\n",
        "    upstream = 1\n",
        "\n",
        "    new_probs = probs + h\n",
        "    new_val = ce(new_probs, target)\n",
        "    old_val = ce(probs, target)\n",
        "    delta_output = new_val - old_val\n",
        "    delta_loss_indirect = np.sum(delta_output * upstream)\n",
        "\n",
        "    ce.backward(upstream)\n",
        "    delta_loss_direct = np.sum(h * ce.grads['x'])\n",
        "\n",
        "    print(f'Relative error of delta-loss:\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')\n",
        "\n",
        "ce = CrossEntropyLoss()\n",
        "check_gradient_ce(ce, h=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LudbmXBrS6kC"
      },
      "source": [
        "## Optimization (15 Points)\n",
        "\n",
        "Now that we have our network and loss function, we need to update model paremeters. We can do so by using `Optimizer` class that perform updating rule on model parameters. You need to implement `sgd`, `adam` and `momentum` strategy for this optimizers. Becareful to consider regularization update for linear units that require regularization.\n",
        "<br>\n",
        "SGD:\n",
        "<br>\n",
        "$$\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta)\n",
        "$$\n",
        "<br>\n",
        "Momentum:\n",
        "<br>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "v &= \\mu \\cdot v - \\eta \\cdot \\nabla_{\\theta}J(\\theta) \\\\\n",
        "\\theta &= \\theta + v\n",
        "\\end{aligned}\n",
        "$$\n",
        "<br>\n",
        "Adam:\n",
        "<br>\n",
        "\\begin{align*}\n",
        "m_t &= \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot \\nabla_{\\theta}J(\\theta) \\\\\n",
        "v_t &= \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot (\\nabla_{\\theta}J(\\theta))^2 \\\\\n",
        "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
        "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
        "\\theta &= \\theta - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Optimizer():\n",
        "    def __init__(self, layers, strategy, lr):\n",
        "        self.layers = layers\n",
        "        self.strategy = strategy\n",
        "        self.lr = lr\n",
        "        self.strategies = {\n",
        "            'sgd': self._sgd,\n",
        "            'momentum': self._momentum,\n",
        "            'adam': self._adam,\n",
        "        }\n",
        "        self.momentum_cache = {id(layer): {'velocity': {} for param in layer.params} for layer in layers}\n",
        "        self.adam_cache = {id(layer): {'m': {}, 'v': {}} for layer in layers}\n",
        "\n",
        "    def step(self, epoch):\n",
        "        self.strategies[self.strategy](epoch)\n",
        "\n",
        "    def _sgd(self, epoch):\n",
        "        learning_rate = self.lr(epoch)\n",
        "        for layer in self.layers:\n",
        "            for param, grad in layer.grads.items():\n",
        "                update = learning_rate * grad\n",
        "                layer.params[param] -= update\n",
        "\n",
        "    def _momentum(self, epoch, momentum=0.9):\n",
        "        learning_rate = self.lr(epoch)\n",
        "        for layer in self.layers:\n",
        "            for param, grad in layer.grads.items():\n",
        "                velocity = self.momentum_cache[id(layer)]['velocity'].get(param, np.zeros_like(grad))\n",
        "                velocity = momentum * velocity - learning_rate * grad\n",
        "                self.momentum_cache[id(layer)]['velocity'][param] = velocity\n",
        "                layer.params[param] += velocity\n",
        "\n",
        "    def _adam(self, epoch, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        learning_rate = self.lr(epoch)\n",
        "        for layer in self.layers:\n",
        "            for param, grad in layer.grads.items():\n",
        "                m = self.adam_cache[id(layer)]['m'].get(param, np.zeros_like(grad))\n",
        "                v = self.adam_cache[id(layer)]['v'].get(param, np.zeros_like(grad))\n",
        "                m = beta1 * m + (1 - beta1) * grad\n",
        "                v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "                self.adam_cache[id(layer)]['m'][param] = m\n",
        "                self.adam_cache[id(layer)]['v'][param] = v\n",
        "                m_hat = m / (1 - beta1 ** (epoch + 1))  # Correct bias for m\n",
        "                v_hat = v / (1 - beta2 ** (epoch + 1))  # Correct bias for v\n",
        "                update = learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n",
        "                layer.params[param] -= update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pDROgraatB0p"
      },
      "outputs": [],
      "source": [
        "class Optimizer():\n",
        "    \"\"\"\n",
        "    Our main optimization class.\n",
        "\n",
        "    You can add arguments to _sgd and _momentum function if you need to do so, and\n",
        "    pass this arguments to step function when using optimizer. Don't change __init__\n",
        "    or step function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, strategy, lr):\n",
        "        \"\"\"\n",
        "        Initialize the optimizer.\n",
        "\n",
        "        Args:\n",
        "            layers: Model layers (those whose parameters we want to update).\n",
        "            strategy: Optimization strategy ('sgd', 'momentum', or 'adam').\n",
        "            lr (callable): Learning rate function that takes the epoch as input.\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.strategy = strategy\n",
        "        self.lr = lr\n",
        "        self.strategies = {\n",
        "            'sgd': self._sgd,\n",
        "            'momentum': self._momentum,\n",
        "            'adam': self._adam,\n",
        "        }\n",
        "        self.momentum_changes = dict([])\n",
        "\n",
        "    def step(self, *args):\n",
        "        \"\"\"\n",
        "        Perform the specified update strategy on all layer parameters.\n",
        "\n",
        "        Args:\n",
        "            *args: Additional arguments specific to the chosen strategy.\n",
        "        \"\"\"\n",
        "        self.strategies[self.strategy](*args)\n",
        "\n",
        "    def _sgd(self, epoch):\n",
        "        \"\"\"\n",
        "        Perform stochastic gradient descent update on layer parameters.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current training epoch.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##         SGD optimizer implementation            ##\n",
        "        ####################[Your Code]######################        \n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'params'):\n",
        "                for param_name, param_value in layer.params.items():\n",
        "                    if param_name in layer.grads.keys() and param_name != 'reg':\n",
        "                        gradient = layer.grads[param_name]\n",
        "                        if param_name == 'W' and 'reg' in layer.grads.keys():\n",
        "                            gradient += layer.grads['reg']\n",
        "                        layer.params[param_name] -= self.lr(epoch) * gradient\n",
        "\n",
        "    def _momentum(self, epoch, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Perform momentum update on layer parameters.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current training epoch.\n",
        "            momentum: Momentum coefficient.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##       Momentum optimizer implementation         ##\n",
        "        ####################[Your Code]######################\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'params'):\n",
        "                for param_name, param_value in layer.params.items():\n",
        "                    if param_name in layer.grads.keys() and param_name != 'reg':\n",
        "                        gradient = layer.grads[param_name]\n",
        "                        if param_name == 'W' and 'reg' in layer.grads.keys():\n",
        "                            gradient += layer.grads['reg']\n",
        "                        if (layer, param_name) not in self.momentum_changes:\n",
        "                            self.momentum_changes[(layer, param_name)] = np.zeros_like(gradient)\n",
        "                        velocity = self.__momentum_change(momentum, (layer, param_name), gradient, epoch)\n",
        "                        layer.params[param_name] += velocity\n",
        "\n",
        "    def __momentum_change(self, momentum, key, value):\n",
        "        \"\"\"\n",
        "        Update momentum for a specific parameter.\n",
        "\n",
        "        Args:\n",
        "            momentum: Current momentum coefficient.\n",
        "            key: Tuple representing the parameter index and type.\n",
        "            value: Gradient value for the parameter.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated momentum value.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##   Momentum optimizer for a specific parameter   ##\n",
        "        ####################[Your Code]######################        \n",
        "        layer, param_name = key\n",
        "        self.momentum_changes[(layer, param_name)] = momentum * self.momentum_changes[(layer, param_name)] - self.lr(epoch) * value\n",
        "        return self.momentum_changes[(layer, param_name)]\n",
        "\n",
        "    def _adam(self, epoch, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Perform Adam update on layer parameters.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current training epoch.\n",
        "            beta1: Exponential decay rate for first moment estimates.\n",
        "            beta2: Exponential decay rate for second moment estimates.\n",
        "            eps: Small constant to prevent division by zero.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##         Adam optimizer implementation           ##\n",
        "        ####################[Your Code]######################\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'params'):\n",
        "                for param_name, param_value in layer.params.items():\n",
        "                    if param_name in layer.grads.keys() and param_name != 'reg':\n",
        "                        gradient = layer.grads[param_name]\n",
        "                        if param_name == 'W' and 'reg' in layer.grads.keys():\n",
        "                            gradient += layer.grads['reg']\n",
        "                        if (layer, param_name) not in self.momentum_changes:\n",
        "                            self.momentum_changes[(layer, param_name, 'm')] = np.zeros_like(gradient)\n",
        "                            self.momentum_changes[(layer, param_name, 'v')] = np.zeros_like(gradient)\n",
        "                        velocity = self.__adam_change(epoch, beta1, beta2, eps, (layer, param_name), gradient)\n",
        "                        layer.params[param_name] -= self.lr(epoch) * velocity\n",
        "\n",
        "    def __adam_change(self, epoch, beta1, beta2, eps, key, value):\n",
        "        \"\"\"\n",
        "        Update Adam parameters for a specific parameter.\n",
        "\n",
        "        Args:\n",
        "            epoch: Current training epoch.\n",
        "            beta1: Exponential decay rate for first moment estimates.\n",
        "            beta2: Exponential decay rate for second moment estimates.\n",
        "            eps: Small constant to prevent division by zero.\n",
        "            key: Tuple representing the parameter index and type.\n",
        "            value: Gradient value for the parameter.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated parameter value.\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##     Adam optimizer for a specific parameter     ##\n",
        "        ####################[Your Code]######################\n",
        "        layer, param_name = key\n",
        "        m = beta1 * self.momentum_changes[(layer, param_name, 'm')] + (1 - beta1) * value\n",
        "        v = beta2 * self.momentum_changes[(layer, param_name, 'v')] + (1 - beta2) * (value ** 2)\n",
        "        m_hat = m / (1 - beta1 ** (epoch + 1))\n",
        "        v_hat = v / (1 - beta2 ** (epoch + 1))\n",
        "        self.momentum_changes[(layer, param_name, 'm')] = m\n",
        "        self.momentum_changes[(layer, param_name, 'v')] = v\n",
        "        return m_hat / (np.sqrt(v_hat) + eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAFpEpkYS6kD"
      },
      "source": [
        "# Section 2: MNIST Classification (35 Points)\n",
        "\n",
        "Now that we can build a neural network we want to solve MNIST classification problem. This dataset consists of 70000 $28 \\times 28$ grayscale images in 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAtiFyXcS6kD"
      },
      "source": [
        "## Data preparation (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tDptlYzvS6kD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "MNIST dataset loaded and split into train and test sets.\n"
          ]
        }
      ],
      "source": [
        "#######################################################################\n",
        "##  Download MNIST dataset and split it to train and test (1 point)  ##\n",
        "##############################[Your Code]##############################\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# # Define a transform to normalize the data\n",
        "# transform = transforms.Compose([transforms.ToTensor(),\n",
        "#                                 transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# # Download and load the training data\n",
        "# trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# # Download and load the test data\n",
        "# testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# print(\"MNIST dataset loaded and split into train and test sets.\")\n",
        "\n",
        "# Download and prepare the MNIST dataset\n",
        "mnist_dataset = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
        "\n",
        "# Normalize the pixel values of the images\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(\"MNIST dataset loaded and split into train and test sets.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "W1Td102MS6kD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ali/anaconda3/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train set size = 42000, validation set size = 14000, test set size = 14000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "##############################################\n",
        "##  Split train set to train/val (1 point)  ##\n",
        "################[Your Code]###################\n",
        "\n",
        "MNIST = fetch_openml('mnist_784')\n",
        "MNIST.target = MNIST.target.astype(np.uint8)\n",
        "split_index = int(0.8 * len(MNIST.target))\n",
        "train_data, x_test = MNIST.data[:split_index].to_numpy(), MNIST.data[split_index:].to_numpy()\n",
        "train_label, y_test = MNIST.target[:split_index].to_numpy(), MNIST.target[split_index:].to_numpy()\n",
        "\n",
        "x_train, x_val = train_data[:int(0.75 * len(train_data))], train_data[int(0.75 * len(train_data)):]\n",
        "y_train, y_val = train_label[:int(0.75 * len(train_data))], train_label[int(0.75 * len(train_data)):]\n",
        "\n",
        "##############################################\n",
        "# Print shapes to verify\n",
        "print(f\"train set size = {len(x_train)}, validation set size = {len(x_val)}, test set size = {len(x_test)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Cj3DiuBcS6kE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABH8AAAVtCAYAAABqQuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD0n0lEQVR4nOzdeZiWVd048DMDDKuAsrnCACIobhCgogKKS4toahlamQZli2ZZva+hNJia7ba89WuxXMrE1BbNsFBBBVwTl1BxYRcZQBZlGZbh/v1R3c+cB2YYhplnhns+n+vyur7f59zLofedL898uc+5i5IkSQIAAAAAmVTc2BMAAAAAoOFo/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIbtcc2fUaNGhaKiot2+zqRJk0JRUVGYPn367k8KKCh1AFAHAHUAmjc1YNc0SvOnqKgo+q9169ahW7duYfDgwWH8+PFhypQpobKysjGmFoqKisKoUaPqdG5lZWW48cYbw5FHHhnatm0b9tlnn/D+978/zJo1q34nCRmgDgDqAKAOQPOmBhROUZIkScFv+p/uXFlZWQjh3//DrFmzJsyZMyfMnDkzbN68OQwZMiTcfvvt4ZBDDonOXbRoUdiwYUMYMGDAbs1h5cqVYeXKlaFnz56hXbt20dxGjhy5y12/JEnCeeedF+6+++7Qv3//MGbMmLBq1apw5513hoqKinDPPfeEs846a7fmDFmiDgDqAKAOQPOmBhRQ0ghCCEl1t162bFny4Q9/OAkhJAcddFBSXl5e8LmNHDlyl8/7/e9/n4QQkuHDhycbN25MP3/qqaeSkpKSpFu3bsk777xTjzOFPZs6AKgDgDoAzZsaUDhNrvmTJElSWVmZjBo1KgkhJJdffnk0NnLkyB2eW1FRkZSVlSW9e/dOSkpKktLS0uSqq65KKioqdvh/tLKysiSEkEybNi1JkiS5+eab03nl/1dWVrbTP9OJJ56YhBCShx9+eLuxj3/840kIIfnNb36z0+tAc6EOAOoAoA5A86YGFE6T3PC5uLg4XH311SGEEO64446Q7GRlWpIk4dxzzw3XXHNNaNmyZbj00kvDmDFjwi233BLGjh1bq3seffTR6aNmvXr1CmVlZel/O1vnV1FREWbNmhXatWsXTjzxxO3G3/e+94UQQnj44YdrNRdAHQDUAUAdgOZODahHBW83JTvv7iXJv7t1LVu2TEIIybx589LPd9Tdu+2225IQQnLiiScmmzZtSj9fvXp10r9//1p196rObVcf7frXv/6VhBCSww8/fIfjTz/9dBJCSIYNG7ZL14UsUwcAdQBQB6B5UwMKp0k++RNCCK1btw5dunQJIYSwYsWKGo+99dZbQwghXHfddaGkpCT9vHPnzmHixIkNN8n/WLt2bQghhE6dOu1w/L+fr1mzpsHnAlmiDgDqAKAOQPOmBtSPJtv8CSGkj3T9dwfw6syePTsUFxeH4cOHbzd2wgknNMjcgMJQBwB1AFAHoHlTA3Zfk23+VFRUhFWrVoUQQujWrVuNx65duzbss88+oWXLltuN9ejRo0HmV9V/u3f/7fLl++/nnTt3bvC5QJaoA4A6AKgD0LypAfWjyTZ/ZsyYEbZu3Rp69OgRSktLazy2Y8eOYdWqVWHr1q3bjZWXlzfQDHP69u0bWrRoEebNm7fDObz22mshhBAOOeSQBp8LZIk6AKgDgDoAzZsaUD+aZPNn27Zt4frrrw8hhHDBBRfs9PhBgwaFbdu2hVmzZm03NmPGjF26d3FxcaisrNylc9q0aROGDx8eNmzYEB577LHtxqdMmRJCCOHkk0/epetCc6YOAOoAoA5A86YG1J8m1/xZvnx5GDt2bJg+fXro2bNnmDBhwk7PufDCC0MIIVx99dVh8+bN6edr164N11577S7dv0uXLmHx4sW7NukQwmc/+9l0DhUVFennTz/9dLjzzjtDt27dwrnnnrvL14XmSB0A1AFAHYDmTQ2oX9svhCugSZMmhRD+3c1bs2ZNmDNnTpgxY0bYvHlzGDZsWLj99ttD165dd3qdCy+8MEyePDk88MAD4fDDDw9nnnlm2LJlS7jnnnvC0KFDw9y5c0Nxce36XKNHjw6TJ08OY8aMCYMHDw6tWrUKI0aMCCNGjKjxvLFjx4Y//vGP4e677w6DBg0KY8aMCW+//Xa48847Q2VlZfjVr34VOnbsWKs5QHOiDgDqAKAOQPOmBhRAwV8u/+9tuqP/SkpKki5duiSDBw9Oxo8fn0yZMiWprKzc4bkjR45MdjTtjRs3JhMnTkxKS0uTkpKSpFevXsmECROSJUuWJCGE5KyzzoqOLysrS0IIybRp06LPy8vLk/PPPz/p3r17UlxcnIQQkrKyslr9ubZs2ZL84Ac/SA4//PCkTZs2SefOnZP3ve99ycyZM2t1PjQn6gCgDgDqADRvakDhFCXJf96ZllFTp04Np512WrjyyivDDTfc0NjTARqBOgCoA4A6AM1bc68BTW7Pn7paunTpdp+9/fbb4corrwwhhHD22WcXekpAgakDgDoAqAPQvKkBO9aoe/7UpyuuuCI8//zzYfjw4aFbt25hyZIlYcqUKWHVqlXhkksuCcOGDWvsKQINTB0A1AFAHYDmTQ3Yscw0f84555xQXl4e7rvvvrBmzZrQpk2bMHDgwDBu3Lgwbty4xp4eUADqAKAOAOoANG9qwI5lfs8fAAAAgOYsM3v+AAAAALA9zR8AAACADKv1nj9Ffc9syHlQB8kb9zb2FGhm1IGmRx2g0NSBpkcdoNDUgaZHHaCQ1ICmpzY1wJM/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYS0bewKNbWjfXlH+qTPbpfG4cx6Oxn79x5Oj/Jd/2ZDGz8xb2ACzAxrb6MN7Rvk//t+T8QHFLdLw1E8PiYYenrOoweYFANSvMwbHvxf85UdPpPGSt0qjsc9/a98o/+vrVX6teueNep8bwO7y5A8AAABAhmn+AAAAAGSY5g8AAABAhjW7PX8G9Y7X8j7wo9ejvPNeK3LJtvjccR98KMrPOblHGncd0yIA2XDxyX3S+DMf2hAPVuYfnfvghs9vjkZun9IvjX/8t7z9fyo37c4Ugd10zMHx94FZtz4dH7Btux/2al39k9FpfMMfXtqteQFNSJUycGD3BdHQX34Q53c/OCKNP3JdA84J2C1Hleb+/j/u0FbR2E+vmln3CxcVpeHaNZ2joeGXHJLGryxZUPd77CZP/gAAAABkmOYPAAAAQIZp/gAAAABkWLPY8+fYfrl1fXdevywa69xpZXxwlX1+1q7rGg1t3tw6yrvt82YaH9//2Ghs5htVrrs1b88QoEmpusdPCCF85PRcIRhy2OxaXyf/2CGH5eI/PHF8NLZsxbxdmCFQ3678eN5XoPw9fmq/5U+47nO5PQG7dD4lGrt/5rtpPG1O3t5fQOG1PzANb/18+2jopCHz63zZZ+c2i1+rYI/QZ7/eaXzOcfHv8JecuyR3XK834hOTvE1/d0WSCzt1fDsauqNsXRp/9jvxnoNPvLaw7vfcRZ78AQAAAMgwzR8AAACADMvO84mt907DkX32ioZu/nru9e0H9sh/nLMoVOeVeaVR/sM7SqL8jhtyy74evenJaGzCj09O42/f9XK19wAaSJXHukMIYXTvXK/7FxPiRzH37fJ8lLdtvS5U54XXjojyFsW5ZzwH9v3XLk8TaDj7d4+XdD743VVpfED3hvl5/dLHH4zyeUtOTONpcxrklsAuOKk0933g2COWRmMH7Je3/GIXln9+8/MPp/GrC+PtIP70VOGWdUCz1G6/KL3ta5vT+LhBs2o4sTDPwhw54MU0Hn74qGjsidcKMoUQgid/AAAAADJN8wcAAAAgwzR/AAAAADIsM3v+/OayHmn8iTEP13Bk7R1z5DNR3v5PJ0T51CeGp/Gpwx+Pxgb22Y3XxAF1cs4xuVcnXviBuLyNGTEjl7TIO3EX1vT/8PfxnmLFRbl9w276eu2vAzS8Ni2SKO/fyx580Nx1bp/7e7tDuw0Nco+fXrk4yrfdkPt+8pen7f8DdVJlP89ffTr+Pv7Bk+JN9fbZu7zeb7+pom2Uv72mW5Tvv++ier9nffPkDwAAAECGaf4AAAAAZNgeu+xraN9eUX7WqNzr00Jx9a9vf3DWcVE+ZVb8+vbvf+WRNH7zrfges+fFa0PW3JOLTz0hXkdSVP0UgHry8ZG9o/yW62p6lWMVxXVf91Xjz3b+ZYFGVfbxkp0fVAvjv3F8lB95SJso/8LYh+rlPkD9+N6nB6bxlz7+YN7oW9WfuBvfD6rq0WNZlB/YvU+drgPkjDsu93f6J8+pn21edsXri/pG+Y/u6BTlvyyz7AsAAACARqT5AwAAAJBhmj8AAAAAGbbH7PkzqHe8/84DP3o9yjvvtSKX5L1l/S/TRqTxOd9aG42deXj8Gtiv/ejkNP7OPzbHF3rnjSh9bkGVZFu8JviDo55N4yF/OCwae2aeVzxCXeTv8fODL8c/k1WX5m+o6BgNLV2xbxp36rAuGuu299Jq75l/nXUb4gKzV7sq+wPUbWsAYDecMTj+fvCXHz1RJathb4881/z8lCj/xu1VXxs7Lxq7rF2/+OSq24QU2wMQGltS9ev9Lv3dHB/8+/tHpfHjL8Rjw4+Mf9bPf+/0aq8zZsTWNP7pY/G+Ifm/XwD/1mbv+GflgvdtqZfrfqPK3/eLlm2MxkYPbRXl539gehp/77fx7wSt9phOSo4nfwAAAAAyTPMHAAAAIMM0fwAAAAAyrEmvVDt4/9z+Hpd/qHU01rnTyihfsfKANF6yvFs0NvkfuXW2oWJFNHbvM6HGvK7atc3tLXTpuW2jsYu+Wz/3gObgnGNy+3ncct2seLCGdfwznhsY5e+7ckEaX3xyn2jspq9Xv+fP135ydJTf/cRrUZ5/LaCR1XHvrXiPn5olSd4HldUm2x8L7LaO+8R7gZwxMP65u2jMi9We+/ba3B6Ay1ftE409PadHlF/8s3dyyeY10ViXzvE+hDU59ZjH07hXu+HR2MJ38o8GQgjh0W/Ge3S+5/Dnqj847+/+Fav3T+Of/OHQaOz6e5bkks3xfsA3P1Ua5ZNuy/28vr5sSTQW2sQ9h9FTRqbxR973SPVzbUSe/AEAAADIMM0fAAAAgAxrWsu+WnWI0m9/OvcKxQ+e9HA09s67XaL8k984KI3/9nr8x9q7pL4mWDc991vfuBOAPUj+MqoffOXlao/Nfw171aVeX/5p7e/59JxBUf67v+2Vxv/3wKIaz7151rtpfNHsodHYCYOerv0kgDqZcHHdXv26aFn+ks2NOzwuhBBCy3ZRunenRv5iAc3Q0aW5ZeA3XxUv1ThyQN4yrxqWf9583+Fp/L835S/3XF3teX32i5d5Tfr0g9XfBKiT4w7J/Zz36/Vqrc+ruswrhBD2PbvqmuuXaj+BdxdE6evv7viwEELYt32bKO/UoX5eRd+QPPkDAAAAkGGaPwAAAAAZpvkDAAAAkGFNas+f4/vEr1v84EmPVnvsuV/tF+UPz6l+X47qV+8CTc3XPrkiyju2W1XtsdfeNCTKv3N39fsDVTX95fjdyzdPaB8fsDZ+nXuNKnLzrdhcWvvzgHrx9Mudo/yYI2p33ue/3S3vk+q/R3zpjAOj3F4fUHgnDMzttXVkvycKfv95q+N3sv948ugo/8LYh2p1nUkfax3lF39v9+YFWXLF2NzPece9qv8dIF/+69x3aZ+fOvrAUXH+3hNnNfg9d5cnfwAAAAAyTPMHAAAAIMOa1LKvGz67Lf6guCgNH5x1XDT08JwFBZjRLihukfdB7h2TRQGoyaDeudc6duqwMB6s8qPV4sT98s6s3TKvfPPfml+n83amqCheThbyywJQ7y49P2+pRQ2veL7rwZFp/Lc3ajgwz6fOXr6r0wJ2V5suUdqvV6tckv/3a9738BdeOSyNj/ufTtFYxer817vXTXH+P6FXnUIN8/nsr5v+66ChqfvGz0+J8uvvWdJIM9mzePIHAAAAIMM0fwAAAAAyTPMHAAAAIMMafc+fM4fk9voYMvD5eHBbbv+M+2eWhCZtW97eAVXm/tyrexV4MtC0Hd6zNMrvvPbtNO7a+a344Npvy9E42uReF926Vd5km/rcYQ/0l2v7xh8U5+/Hk/vBe+WNw6KRsde9Wqd7FoXa7+f11+knRPnP/vZ6ne4Jzd0NF+wb5Zee92Au2e7v1/iDX/2pcxpXrK6fn8E+e3fMm0/1+41V3eMnhBAuur59lfm8US/zgSz4yrkDovyc0x6pksXPqZQvz+39OXvuuvhCm9fW99R26tcPzovyYYefnMbjPzS92vMacz9gT/4AAAAAZJjmDwAAAECGaf4AAAAAZFij7/nTtnVu1Vvrkg3R2FsreqbxLx9rAptntOoQpdee36tKFu9T8sDM49P4S7esaMhZwR7n+5+P874H1m0fjqbg4uG5Pb1OGDSzEWcC2XXywNz3gUN75+3xk7/nXpU0yduqZ1ecN7w0jbvuHa/rr2k/r9/cu7XuNwVSg/uvr/O5+3RqnUtatokHt1bU+bq19elvtY/y5xcsbPB7wh4p/+/pZFu1h/5pWm5/oHufeamBJlR326pOvYY/x258NdltnvwBAAAAyDDNHwAAAIAMa/RlXzWp2Jx7ZLNRXouYt8yr7LxeUT5h3NQ0XvRm/OrZn95V5ZnwCsu+oK6u+MHIKlnjLw/rf0BplH/rC/N2fGAI4bVF/dN42ebGfLEj7NkOK819H2iwZaJtukTpqce0TeN9OpbXeOr4b+SWev/l6eprAlB7DzxeEuWnHFP7c48/qsqSsfsPiAffqf53iv2794nyc4a2SOO9O5XkHx6568Hc95XBB8drQ5/2dnegCfDkDwAAAECGaf4AAAAAZJjmDwAAAECGNek9f/48vbRKNqcg9xzUO7evz+UfahuNffyMqVF+19RRaTz2uvw9CBbV99SgWVrz7pZGvX/+Hj9/+uaaKO/a5a00Xrb8wGhs7Nf3ziVrLfiHQvvLI/vnfbK62mN/9MkeUf7Jsx6q9X3KV3u9O9S3T529vNbHPvHce6L8E9+r8u/bNezxk2/wgfFePT/66sxan3vb/blXyP9ttt8DYEeOKo330L3k3CWNNJM6yNsP+Ioz4v3EPjHm8WpPvffhE9L4B/cvq9957QJP/gAAAABkmOYPAAAAQIZp/gAAAABkWKPv+VNUNSmOsnDuyfPS+Cu/bJj7f/GsQ6J84qdyewt17rQyGrvlzydH+bjvv9Iwk4KMKypK4g9aVH/sb74xK41vPXG/hplQm25ReudXOqfxh05/tMZTX5vfP43HfG2feOzNBbs9NaDufvnAhmrHvv7RgVF+6fkPxgfEW39E7npwZJT/7Y0aDgbq5Ok58T5c/fu8XO2xxw5+LsonfHhUGj/4ZLzHyKnHtk/jS8/P29urOG+foW3Vz+/7t54S5X+bXZj9SWFP9vyChVH+i3sGRPm3v/RaIaezS/L3+PnuFdNrfe7mLVWeudm8tp5mtOs8+QMAAACQYZo/AAAAABnW6Mu+osUf2+KlIAd0z70m8fuXjIrGfvePdVG++N3cupHR/eNnNM87NffHHDzgzWis5wGPRPn8xQen8f2PHR6N/fq+6h8fB2rve7+Lf9YHDcg92r1Px/Jqz/vHd+JHt5MkXip6/4xWafzGm5ujsS98JHds/rKz1q3iJRsnDMot9dqwvmM0du1NQ6L8T49vTGPLvKBhFFX9Uc9fJlqc/0Hu53n04XGNOOqcw9J4u2VeNVzntr+cFI1c/D3LvqGhLV2Z92tKjasr48HLPvJQlbjWp23/QZX0rZUHRUN/m/VuTRMCMuCyM/ql8XWfe7LW573zbrwVxK/v3VRvc9odnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLBG3/OnJi1abEnjL14wNRo775R474+16zqk8aF9av+qxUefPibKp/0zt7/HN273ykZoCP94YVGUf+RrpWl85w3xsVX3ABp93BPxYN7S/FPiH+fqVb+1x7/n98RxaTz57/GeIbdOq/5Vs0DDSKpu07ULe3T86uqZ1V90F64z6XdNY60+NCeD+68v+D3Xr+8U5Yveyr3a+VM3xHsAPv5q/MpqoGGNPf35NB45uHs0duG18c/ns/l7hlXj1F5bovzGK+K9vA7a94U0bt1mYzRWsbF9lL9bpX58dGK8R9hD/4p/92ksnvwBAAAAyDDNHwAAAIAMa/RlX/94Jdd/mvXc0Ghs+NFPV3ve/vvGj07tn/ea+KpWrMo9svm7KYdFY1/5paVd0NgenpP7ee57Ub9o7PL3HpHGkz6b92rmOlq6rGeUT/9nnyj/+I+rPPK5YWm93BOou3fWb03jdRs7R2MdOqypl3u88kb8/eAXf+yWxgtXNo3HtaE5Of36eI32A1fllmSfeszj9XKPux4cGeXTno6Xf/7i729UyVbXyz2BnAVL46VUVb+j5/++37nTyh3GIYTw9K11nEBR3rMwybZqD81/fftV/zcwyn/2t9erZE3ze4MnfwAAAAAyTPMHAAAAIMM0fwAAAAAyrNH3/Fn99rw0PnVS32jsK+89NY2v+Uz8qveafPe2U6P8l3/bkMbz3rLHDzRl76x6I8qv/X0uXrxseDT2xQveifIjDv5XGr/w2hHR2A9/v1caz18Wr+l/9OW5dZorUBi3TZ+fxm1bHx6N/ezqGfVyj4EX5e/nYX8PaFTvxN8HLvpebn++uycOicaOG/RMtZcZ/43jo7x8dW4Psb+9EX8fyL8n0LDufmJhlH/kxdzen+fs27T2zbn6pzXt8bNn8OQPAAAAQIZp/gAAAABkmOYPAAAAQIYVJUmS1OrAvmc29FzYRckb9zb2FGhm1IGmRx2g0JpaHfjaeYdF+XWfeyiNFy3rE419/tvdqr3O32Y3rb0FdoU6QKE1tTqAOkBhNVQN6H9AaRo//stXo7FOHd+u/xsWxc/C3PzHUXF+//o0nvl6eXxu5ab6n89uqE0N8OQPAAAAQIZp/gAAAABkWKO/6h0AoK5u+MNLefl+VbKNeUfvuUu7ACDr5r65II33+UBJ3uh+oeG9XIB7NB5P/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIZp/gAAAABkmOYPAAAAQIYVJUmSNPYkAAAAAGgYnvwBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM2+OaP6NGjQpFRUW7fZ1JkyaFoqKiMH369N2fFFBQ6gCgDgDqADRvasCuaZTmT1FRUfRf69atQ7du3cLgwYPD+PHjw5QpU0JlZWVjTC0UFRWFUaNG7fJ5L7/8cigrKwtnnXVW6NmzZ/pn27p1a/1PEjJAHQDUAUAdgOZNDSicoiRJkoLf9D/dubKyshBCCJWVlWHNmjVhzpw5YebMmWHz5s1hyJAh4fbbbw+HHHJIdO6iRYvChg0bwoABA3ZrDitXrgwrV64MPXv2DO3atYvmNnLkyF3u+v3whz8MX/rSl0KLFi1Cv379woIFC0JFRUXYsmVLaNmy5W7NFbJIHQDUAUAdgOZNDSicRm3+7OjW5eXl4bLLLgt33XVXOOigg8IzzzwTunfvXtC51eX/wHPnzg1r1qwJRx55ZGjbtm0oLS0NCxcubPT/A0NTpQ4A6gCgDkDzpgYUUNIIQghJTbeurKxMRo0alYQQkssvvzwaGzly5A7PraioSMrKypLevXsnJSUlSWlpaXLVVVclFRUVSQghGTlyZHR8WVlZEkJIpk2bliRJktx8883pvPL/Kysr2+U/Y69evZIQQrJly5ZdPheaA3UAUAcAdQCaNzWgcJpk67m4uDhcffXVYfr06eGOO+4IN954Y40bOSVJEs4999xw//33h379+oVLL700bNmyJdxyyy1hzpw5tbrn0UcfHcrKysI111wTevXqFS666KJ0rC7r/IDdow4A6gCgDkDzpgbUo8boOIWddPeS5N/dupYtWyYhhGTevHnp5zvq7t12221JCCE58cQTk02bNqWfr169Ounfv3+tuntV55Z/bF00le4eNFXqAKAOAOoANG9qQOE02Ve9t27dOnTp0iWEEMKKFStqPPbWW28NIYRw3XXXhZKSkvTzzp07h4kTJzbcJIEGpQ4A6gCgDkDzpgbUjybb/Akht+lTTY91hRDC7NmzQ3FxcRg+fPh2YyeccEKDzA0oDHUAUAcAdQCaNzVg9zXZ5k9FRUVYtWpVCCGEbt261Xjs2rVrwz777LPDnbN79OjRIPMDGp46AKgDgDoAzZsaUD+abPNnxowZYevWraFHjx6htLS0xmM7duwYVq1aFbZu3brdWHl5eQPNEGho6gCgDgDqADRvakD9aJLNn23btoXrr78+hBDCBRdcsNPjBw0aFLZt2xZmzZq13diMGTN26d7FxcWhsrJyl84B6p86AKgDgDoAzZsaUH+aXPNn+fLlYezYsWH69OmhZ8+eYcKECTs958ILLwwhhHD11VeHzZs3p5+vXbs2XHvttbt0/y5duoTFixfv2qSBeqUOAOoAoA5A86YG1K/tF8IV0KRJk0II/+7mrVmzJsyZMyfMmDEjbN68OQwbNizcfvvtoWvXrju9zoUXXhgmT54cHnjggXD44YeHM888M2zZsiXcc889YejQoWHu3LmhuLh2fa7Ro0eHyZMnhzFjxoTBgweHVq1ahREjRoQRI0bUeN7KlSvDV77ylSgPIYRx48alm1JdeeWVYcCAAbWaBzQX6gCgDgDqADRvakABNMb75UMI0X8lJSVJly5dksGDByfjx49PpkyZklRWVu7w3JEjRyY7mvbGjRuTiRMnJqWlpUlJSUnSq1evZMKECcmSJUuSEEJy1llnRceXlZUlIYRk2rRp0efl5eXJ+eefn3Tv3j0pLi5OQghJWVnZTv9M8+fP3+7Plf9f/r2gOVMHAHUAUAegeVMDCqcoSf7zzrSMmjp1ajjttNPClVdeGW644YbGng7QCNQBQB0A1AFo3pp7DWhye/7U1dKlS7f77O233w5XXnllCCGEs88+u9BTAgpMHQDUAUAdgOZNDdixRt3zpz5dccUV4fnnnw/Dhw8P3bp1C0uWLAlTpkwJq1atCpdcckkYNmxYY08RaGDqAKAOAOoANG9qwI5lpvlzzjnnhPLy8nDfffeFNWvWhDZt2oSBAweGcePGhXHjxjX29IACUAcAdQBQB6B5UwN2LPN7/gAAAAA0Z5nZ8wcAAACA7Wn+AAAAAGRYrff8Kep7ZkPOgzpI3ri3sadAM6MOND3qAIWmDjQ96gCFpg40PeoAhaQGND21qQGe/AEAAADIMM0fAAAAgAzT/AEAAADIMM0fAAAAgAzT/AEAAADIMM0fAAAAgAzT/AEAAADIMM0fAAAAgAzT/AEAAADIMM0fAAAAgAzT/AEAAADIMM0fAAAAgAxr2dgTAGgI3/v0wCj/0iempfELLx0Wjb1vQscoX7ZiXsNNDAAAoMA8+QMAAACQYZo/AAAAABmm+QMAAACQYfb82QVfOy/eJ+S6yx7KJcUtorFTPz0kjR+es6hB5wX82/7d+6TxRWNeiAe3VabhkYfEY4MPPDbK/7ai/ucGNIKWbaL05P7d07hs/LZobOSXlhRkSkABFOX+ffu3/9svGjpr1Ow07vXRw6Ox1W/b8w+onf/50KFRfsMXp+fiX50cjV1965xCTGmnPPkDAAAAkGGaPwAAAAAZpvkDAAAAkGH2/KnBxSf3ifKrxj0ZH1BZbRKShpkSUIOl76xN44efGhCNnXva8kJPB2hs7Q6I0qm/mJXGby2Lx1p27hvlW9e80XDzAhpWSac0HDl4fjTUvsO7aXzu4Pi0m6Y26KyAPVnbHlH6hbGvxeNJbi/Br174aDQ0/82hafzrBxtvbzFP/gAAAABkmOYPAAAAQIZZ9lWD/XvEr4ht23pdI80EqJWKt9Nw4bJ9G3EiQFO3375vRvkRe8fLwGavKeBkgPq1aXUavjy/NBo6YP/Fadylc+tCzQjYE7XI1YivvG/vaGi/Hs9Ve9rSFQdG+eNzt1VzZGF58gcAAAAgwzR/AAAAADJM8wcAAAAgw+z5k+f0o3qm8RfP/1eNx77w2hFp/L4JHaOxZWtX1u/EgJ1rn1tfe1Q/e3QBNSgqauwZAAVw05/j/JTjc/EhPZOCzgXYs4zqn3u9+7e/NK3W533hu92j/KXFC+prSrvFkz8AAAAAGab5AwAAAJBhzX7Z18hDe0b5r65alsb7dCyv8dwf/n6vNF62Yl79TgzYZe1Lcq9jLN1/ea3PO6pfhyh/dkmfNPazDRmVxMs92pVY/gFZNO216v+t+2Pvnxnln//NsCivWP1Gg8wJaJr2794nyr9z2fpan/vAY8PT+L4XV9XbnOqTJ38AAAAAMkzzBwAAACDDNH8AAAAAMqzZ7/nzkVPivT4O2G9htcdOnXVclN86bX6DzAmom/VV1ubffN9h0dh1l75a7XnXff7BKH9n/Yg0/un99TQ5oEk7+uCSKJ85t5EmAjSsoqI0bNl6azT0iXjLn/CLvxdiQkBTcf/170T5kQNerPbYte90ifKf3FmZSzavrdd51RdP/gAAAABkmOYPAAAAQIZp/gAAAABkWPPb86dj3yi95LyH4vEqS/XeXrtvNPT9321rqFkB9eyGO1+K8usubaSJAI2nclOUrl27dxp36rQ6Guu1X+uCTAloZElS7VCrlv5dHJqzIw+dE39QfbkIP5k8KMofeH5ONUc2HSocAAAAQIZp/gAAAABkWLNY9rV/9z5p/KdJ62t93g/vGBjlU198qZojgSavuEWVpLLaw4AMWb8kSqc9fVgaf/CUmYWeDQDQhHx7fPz7fihaHudVln098NjwaKjszsUNNKuG48kfAAAAgAzT/AEAAADIMM0fAAAAgAxrFnv+nDm4KI2HHDa7xmP/NuP4NP7mvWsaakpAoW2rss/Pthre2wgAAGTSDz+b2//v3JNfjQeT+HeEZ+ccncYf+FbeczOb36nvqTU4T/4AAAAAZJjmDwAAAECGZXLZ1znH9IryG75Q/Svapz15TJSP+W5uiVjYsLRe5wUANE1dOlXu/CBgz1dU5bt+Yhk4ZN2x/eLewLknv5bG+++7KO/o+NmYX/9lr1zyzuv1PbWC8+QPAAAAQIZp/gAAAABkmOYPAAAAQIZlZs+f/bv3SeO7vjez1ufNe7N9/MHal+trSgDAHuKc0c9H+bjvt26kmQANyj4/0Kx89LT49/3991tSJYufhZkz97Ao/+0T2aoXnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLDM7Plz5Yfa5JLK2p/33T9srP/JAE1PcYsqSc1FYviRub74T+9voPkABffos7mf7Q+e0ogTAZqklxZsauwpAPXgC2ceksaf+/BD8WAN2/gc95XOUb5+9Rv1OKvG58kfAAAAgAzT/AEAAADIsD122deg3r2i/L3DF9fqvD9MHRnlr735ar3NCWjCtlVZ6rWt5tc2jn3v9DS+/nfDo7GXFi+ox0kBhbR0xeZqx0paxss99u12aBovWzGvweYENB2zl+2xvxpBs9a1a58o/+SZb+eSFvGx27ZW2d7hDydFY+tXv1Tvc2tKPPkDAAAAkGGaPwAAAAAZpvkDAAAAkGF77MLWB25cGOVdO79V7bGPPjssjc///uoGmxPQdP3k96PT+LKxD9b6vIvf2z7Kv/qrepsSUGBbK6vf76uoaFuUd2hV895gAEDj6L1f7yi/9/o1UX5YvznVnvudW09O46tuqf64LPLkDwAAAECGaf4AAAAAZJjmDwAAAECG7bF7/nTtkrfHT2X1x/7qTy1yScWKhpkQ0KS9vmhLY08BaGR/eiq3X+BLrx4ajR12yMtRftkH26bx5T9r2HkBTUPHvL2+7BQKTdMRB8T79NW0x0++vz/5bn1PZ4/hyR8AAACADNP8AQAAAMiwPWbZ1y+/FD+eHYqX5x1R/bqvh1/R44Lm7v/ufy2NP39eXE8O6fNKteddcdG0+Dr3HRPlC5fNr4fZAYX2wOP7RXnP/d6M8stvWlLI6QBNwJj3xL8a/d/9jTQRoEadO9T+9/ups46L8kcXb6vmyOzTFQEAAADIMM0fAAAAgAzT/AEAAADIsCa958+g3r3S+L3HvREPbov3+NmyqXUa3/j7E6OxZWsX1//kgD3Wi693j/JDSl+u5sgQatpPDMiObUnev4dtWd84EwHq1co1y6L8xVcOS+MjBrxU6OkA9WDSJeW1PvY3f8n7YN2i+p3MHsSTPwAAAAAZpvkDAAAAkGFNetnXPu2TND6g+8Iaj134Vm6J2Nd+M6fB5gTs+W67f3OUn3tKI00EaDI6dlwT5R865pA0vvuJmr+DAE3Y1g1RurGiU7WHnjQ0iXKveoemY+BBpWncod38Go+d+NPRafyHx19pqCntcTz5AwAAAJBhmj8AAAAAGab5AwAAAJBhTXrPH4CG8MziFlH+wqtHRvmRh7xQyOkAjeD80+dG+ZaKVlH+4uKiQk4HKJDZc/dO42FHx2Md2m4t7GSAWju2f+65lU4dVtV47OYt23JJsq36A5sZT/4AAAAAZJjmDwAAAECGaf4AAAAAZFiT3vPnkSW59ffTnjkmGjvpmCcLPR0gI5atmBflgz6Vf8T+NZw9v76nAzSCac/0jfLD+uwd5Ws3+/cxyKLr7qxI4yMOHhyN/Wlai/zDgSbi1w/mvr9fPf7AaKx92w1R/vDsjQWZ057GNxsAAACADNP8AQAAAMiwJr3sa+uaN9L4lK/kj+6Xl7/b0NMBADLi49+au5MjVhRkHkBhLV2eWzpy4uWNOBGgznqPzV/WVZSXLyzUVPYonvwBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAMK0qSJGnsSQAAAADQMDz5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhe1zzZ9SoUaGoqGi3rzNp0qRQVFQUpk+fvvuTAgpKHQDUAUAdgOZNDdg1jdL8KSoqiv5r3bp16NatWxg8eHAYP358mDJlSqisrGyMqYWioqIwatSoXTonSZLwwAMPhMsuuywcffTRYe+99w5t2rQJ/fv3D1/84hdDeXl5w0wW9mDqAKAOAOoANG9qQOEUJUmSFPym/+nOlZWVhRBCqKysDGvWrAlz5swJM2fODJs3bw5DhgwJt99+ezjkkEOicxctWhQ2bNgQBgwYsFtzWLlyZVi5cmXo2bNnaNeuXTS3kSNH7lLXr6KiIrRt2zaUlJSEESNGhKOOOipUVlaGhx9+OLzwwguhR48e4bHHHgv9+vXbrTlDlqgDgDoAqAPQvKkBBZQ0ghBCUt2tly1blnz4wx9OQgjJQQcdlJSXlxd8biNHjtylczZv3pxcd911yapVq6LPKysrk0suuSQJISRnnHFGPc4S9nzqAKAOAOoANG9qQOE0ueZPkvz7f5hRo0YlIYTk8ssvj8ZGjhy5w3MrKiqSsrKypHfv3klJSUlSWlqaXHXVVUlFRcUO/49WVlaWhBCSadOmJUmSJDfffHM6r/z/ysrK6vxnffPNN5MQQtKhQ4c6XwOySB0A1AFAHYDmTQ0onJa1fUKokIqLi8PVV18dpk+fHu64445w44031riRU5Ik4dxzzw33339/6NevX7j00kvDli1bwi233BLmzJlTq3seffTRoaysLFxzzTWhV69e4aKLLkrHdnWdX1WtWrUKIYTQsmWT/J8amix1AFAHAHUAmjc1oB41Rscp7KS7lyT/7ta1bNkyCSEk8+bNSz/fUXfvtttuS0IIyYknnphs2rQp/Xz16tVJ//79a9Xdqzq3XX20qybf+ta3khBCMnbs2Hq7JmSBOgCoA4A6AM2bGlA4TfZV761btw5dunQJIYSwYsWKGo+99dZbQwghXHfddaGkpCT9vHPnzmHixIkNN8mdePrpp8M111wT9tprr3Ddddc12jxgT6UOAOoAoA5A86YG1I8m/cxh8p8XkdX0WFcIIcyePTsUFxeH4cOHbzd2wgknNMjcdubVV18NY8aMCVu2bAmTJ08Offv2bZR5wJ5OHQDUAUAdgOZNDdh9TfbJn4qKirBq1aoQQgjdunWr8di1a9eGffbZZ4dr53r06NEg86vJq6++Gk466aSwatWqMHny5HDmmWcWfA6QBeoAoA4A6gA0b2pA/WiyzZ8ZM2aErVu3hh49eoTS0tIaj+3YsWNYtWpV2Lp163Zj5eXlDTTDHXv55ZfDqFGjwsqVK8Ndd90Vzj333ILeH7JEHQDUAUAdgOZNDagfTbL5s23btnD99deHEEK44IILdnr8oEGDwrZt28KsWbO2G5sxY8Yu3bu4uDhUVlbu0jn/9eKLL4ZRo0aFVatWhT/+8Y/hrLPOqtN1AHUAUAcAdQCaOzWg/jS55s/y5cvD2LFjw/Tp00PPnj3DhAkTdnrOhRdeGEII4eqrrw6bN29OP1+7dm249tprd+n+Xbp0CYsXL961SYcQnnvuuXDSSSeFd999N/zlL38JH/jAB3b5GsC/qQOAOgCoA9C8qQH1q1E3fJ40aVII4d/dvDVr1oQ5c+aEGTNmhM2bN4dhw4aF22+/PXTt2nWn17nwwgvD5MmTwwMPPBAOP/zwcOaZZ4YtW7aEe+65JwwdOjTMnTs3FBfXrs81evToMHny5DBmzJgwePDg0KpVqzBixIgwYsSIas9ZvXp1GD16dFi1alUYPXp0ePzxx8Pjjz++3XFf/OIXQ+fOnWs1D2gu1AFAHQDUAWje1IACaIz3y4cQov9KSkqSLl26JIMHD07Gjx+fTJkyJamsrNzhuSNHjkx2NO2NGzcmEydOTEpLS5OSkpKkV69eyYQJE5IlS5YkIYTkrLPOio4vKytLQgjJtGnTos/Ly8uT888/P+nevXtSXFychBCSsrKyGv888+fP3+7PtKP/5s+fvwv/K0G2qQOAOgCoA9C8qQGFU5Qk/3lnWkZNnTo1nHbaaeHKK68MN9xwQ2NPB2gE6gCgDgDqADRvzb0GNLk9f+pq6dKl23329ttvhyuvvDKEEMLZZ59d6CkBBaYOAOoAoA5A86YG7Fij7vlTn6644orw/PPPh+HDh4du3bqFJUuWhClTpoRVq1aFSy65JAwbNqyxpwg0MHUAUAcAdQCaNzVgxzLT/DnnnHNCeXl5uO+++8KaNWtCmzZtwsCBA8O4cePCuHHjGnt6QAGoA4A6AKgD0LypATuW+T1/AAAAAJqzzOz5AwAAAMD2NH8AAAAAMqzWe/4U9T2zIedBHSRv3NvYU6CZUQeaHnWAQlMHmh51gEJTB5oedYBCUgOantrUAE/+AAAAAGSY5g8AAABAhmn+AAAAAGSY5g8AAABAhmn+AAAAAGSY5g8AAABAhmn+AAAAAGSY5g8AAABAhmn+AAAAAGSY5g8AAABAhmn+AAAAAGSY5g8AAABAhrVs7AkAADSEDx9XGuWtS4rS+Mh+7aKxL188LcofnDEsjX/7t6Jo7JXF26L8mXkLd2eaAEADOahH7ygfWrqtmiO3t2JtksaPvbKo3ubUWDz5AwAAAJBhmj8AAAAAGab5AwAAAJBh9vxp2yNK27TpkMYfG5JEY106t47yb/95cS7Zsq7+5wbskgtH5db0njQk/nk9uv/KND784Dk1XufRZ4em8eiJW+PBDUt3Y4bAbivpFKWDD+wc5RM/kftqc/rwf0ZjrdtsrP66eVsAnDL88SpxPPb6gkOi/MGnDkvjz9+c931gw1tV7rGl+vtDM3TOMb2i/NPn5PbXOvX4J+ODi6rsvZXE39FrcvcDJ0T5g0/lftjvmx1f58C9KqP8maUbcknFilrfE2hY37x4YJR361z9Pj6H9n4nyo97z+xa32fp0gPS+JPf6BmNTX1xz9sDyJM/AAAAABmm+QMAAACQYZo/AAAAABlWlCS1WzRb1PfMhp5Lgzmwe24fkP/5UNtobPhRK6N80IDnan3dH/7+1DT+8i9q3kOkISRv3Fvwe9K8NXod6Ng3Su/6Yty/PufUR9N49Zru0dhjz8Z7dFR1yjEvRHm7Drk9O1569dBo7IhPrqrdXAtEHaDQClEHjiqN9wE5/rBWaTx6WHzsB0c/1uDz2R0Tfzo6jf/6+Ppo7IWFC+vlHuoAhVbXOpC/x88t17wc5e3br63znOrilTcOi/L2beN9wZavyu0xtnlLzVulfu1nLdL4sVcKvxeIOkAhNdR3gX4HlKbxgXn7+Hz09PZpfPHZD8cnFrcIdbFyRfcax7t2W57GmzaURGMf/PKRUf6PFxp3D6Da1ABP/gAAAABkmOYPAAAAQIZl5lXv/as8InbpB9tFYxefmXtVZNs278YnFhdF6cIlB6fx2nXxdY48JF4acv7puUdVf/nX0mhs7psLdjZlYBc9cUO8ZKLvQfFr17/5q1PSeOKf4mPDu29Ue93+B8Svi3z61y+l8WF9/xWNTRh7SpR/c/JLAahfVZd5hRDCT75Wt6Vd8xcfHOWVlXX7N68De7wZ5W3arq/myO1d+/mH0nj12hOjsRfqZ9UX7DG6dIp/9Sj0Mq98A/rW/Hf4QfvX/lq/+0ZpGp83IV7e9uTrfthhR47sFf+s/O7rud/VBw54Of/wKuJlXn9/9Jgo/8cT8feI6ryxZFOUF8WtgfCn7+d+12gdtwbCpE9tje95Wa1u2ag8+QMAAACQYZo/AAAAABmm+QMAAACQYXvOnj/t4kW3P7m4c5R/9P3PpXGnDvHr22uS/4rH0V/dO3fLlkk09trk+NweXZak8b4dD4zG5sbbAwB19N6jeqbx0COfjMZu/+uoKJ9425w63SN/j67v/XZoGpd9Nn6V5CfPjNftfzOvLgD1766/j0zjD5/+SDS2dFnPKP/lnw5J42v/uDwaCxUr6nT/y854T5T/8H8frdN1oLn7+YQZjT2FBnPgfgvS+LFfLonG/jbj2DT+4He2xCeua9zXQ0Njmvqj+Ht11VerV6xvE429urBPGn/h+/EGPI+9Gf/eHt59tU7z6d61T5Rv25bbWygLT81k4c8AAAAAQDU0fwAAAAAybI9Z9nXxsfFjX58778E6XWfugniZ18iv7B3lK1fOS+M++/Wu0z2A+tOqZe6di3PnHRqN/WlaRYPc8y8z16Vx2Wfjsbat8+7Zbr9cvOGtBpkPNDc/e3RznE/PLZMou/m4aOzdLfG/Yy1dXrflnzV59rVNOz+oGuvXd0rjVe9sreFIyL4JPz05yj/2vvJqj531Qo8ov+Mf66s99qiD498Txn+w+i0gBvTOvT66uOW2ao/bHS1axT/rY07KLXc78tfHRmMvrAvQbP2/uwfmfZLLl5RvjEZumjovNLQfjI9fEV9cUlTNkXsmT/4AAAAAZJjmDwAAAECGaf4AAAAAZNges+fPWaNqv97u9UX9o/zJf+VeE/+VX8evV6y6x0++Iw5smHXAQO3d9+LqXPzpFvHgpoWhIWzYUn292bfH0ij/zIjSNP75Aw0yHWh+anj18dwNDXTPlvGeId/8eN80Hnva/Dpf9n9+dGQa3znz9TpfB7Lg23e9nJfXdPSqWl93+ktx/qN7qz/2M+87Po3btq7538GvGpeb796dl9dwZO2ddXyHKH+hYb7KwB5h0u/qf5++XfH+QT2j/H0n/KvW5074f3tMKyXlyR8AAACADNP8AQAAAMgwzR8AAACADNtjFqp99MY4/9LLp0b5I7PfTePH3moVH7x2bp3u2a1zi50fBDSsze8U/JavlS9L4xdfOSwaO2JAvLFA6QHxPiHAnuGUI+J1/p89tyTKPzj6wTpd97X58b6DNz2e1Ok6QMP4+ZTa7711499z+4a2bH1INHbzZ+Jfo84c+Vwad+iwptprnnZMPHbt72s9HaA+tGidhmeNbB8Ndd57df7RqWmPD4ny6Qu21u+8CsCTPwAAAAAZpvkDAAAAkGF7zLKv9avfiPLr7mj4ew45rPXODwKyZ2tFGm7Z2qkRJwLUp4tH90njX179eDRW3HJbna/79Z+NTuP7H18fjW1d80b+4cCeYmN5Gm6tWBkNPTa7NMo/OKp2NeRnd7Xa+UFA/WkR/07/rYsOTuPxH55W46lPP3dkGo/9Vl7rZMOi3Z9bgXnyBwAAACDDNH8AAAAAMkzzBwAAACDD9pg9f3bHZWf0S+O2beI/clFRfGxS5Y2sRx68tsbrTn/m2DR+ZO5bdZ8g0LS06pCGbVtvrvHQdRsaejLArjiqtFcav/+Y+BWu3/hMbm3/ruzxU7Exvs5fpseve731oY1pvGT5wlpfF9iDtNsvSv/f1TNqfeqMZ4am8R0vtqi3KQE797nTD4ryr36y5n1+qvrzI3un8cqVL9fbnBqLJ38AAAAAMkzzBwAAACDD9txlX633jtJjDuqYxv/7sfgVimeNfrT66xTnPXq5rbLaQ998qzTKP/ndKudu21L9PYA9Sr/uXdP40H5P1njsw/98t3YX7dg3Sk8rjWvGewbklprdPTNeS/bamwtqdw9oDlq2idIB++4b5XdcsyaN+/d5otrLbNsa//vX5i1tqz326p/Fy7xu/POrO5slkAWdcn93/+Uru3Be3q8Tv5tS5VXTa9/YvTkBO7dXaRp+4ox11R4246nBUf7ze+L2yB2zXq/XaTU2T/4AAAAAZJjmDwAAAECGaf4AAAAAZFjT3vOnyrr+Y3v3iIbuvH5ZlB/YY04ab9jYKRp7863ca18fm10ajY0Z+VyUt2+7ptrptGgRL+D90HElafz9+9rFB2/1/mdosqq8yj2EEHp16RblJx5aVOtL/fjLFWn8zEuHRmPvOXRVGnftvDQa63nggihf925u37I+B8Trjz91Y62nA5n3tXP6RPl1lz1U63MffuLYNL7n4Xh/wJ9PqWldvz1+IJOK4zow7uT4ldCfOGNTGh//nqdrvNSWTbl9fa7/zYnR2K/+MSf/cKAe9dq3d5Tfd/07aTxwQN4r2qvs8Xv/zL2ioTtm7Pmvc6+JJ38AAAAAMkzzBwAAACDDNH8AAAAAMqxp7fnTMt4354ODc/tw3PPdx2s89eqfnZLGM19YF409+vKiXLLX5mjsmdLSKB804Llq77Fv18VR/p0v5vKlK4ZHY3c8uTKXbInnA+yCktweXvt26hINndA33odr0IDcXj4nD1kVqtO2TVwHjhjwRJ2nN6j/82ncea+Dqz3upr/Ea5GnPt09ype+k+vFL1me7fXGsFNt4p/1o/fN/WxfNGZx/tHV+sfM46L87G+3SOOK1TXt8QM0B/l7/PyybGadr/Xos4PS+Nrf2+MHCumCEW2jfOCAJ9N42uNDorE7/5Hb6+tX/2he37k9+QMAAACQYZo/AAAAABnW+Mu+qrzO/doL4mURE8ZNrfa0e6fHr1C84a+517mF9Uvigzv2TcPHvxm/gn3QYc9H+ZZNufnkv6ZxYJ8tUf7hU6en8e+unxWNXfDYCWn84zv3icbeXl/9a6Sfnbew2jHIrCpLu66/4MBo6MwRb6XxYYfUvPyzJu+80zmN310Xv9ZxW/yjnf/m18jP7zwpym++f30aP1Pjz+9LO5si8B+fO3nvKP/J1x6r9blTZ+WWer3vmo3x4Mby3ZoXsOf58tn9o/zz5+WWjnbf5/n8w6u19p14OeqATwyI8uVbqv9+D9Svs4b2ivIJn3w6yrdtTtL4gVnx1jK/+scrDTexJs6TPwAAAAAZpvkDAAAAkGGaPwAAAAAZVvg9f/I207juo7n9eL72yXiPn3Ubcmv+r/zxEdHY/3ssflVz1X1+jjk4XgP4/S9UpPGwI/4Zjb3yxsAov/z7ub1AHnwx7zWN7faL0t/+LbevwLknx6+XO3f07DR+/4mrQ03mv9kvjQ++oMZDIZP+WpZbR/++kQ9HY1sqcjXjLw8eH43Nf6t1lP/98XfTeFPePj4vLM+Vu9Vvz4vG5twSr9sfcHBuLfDr8+PXt3/+VyviC1fk5UCdDDiwNI2/euGbtT7vgceGR/kHvl3l37XqaY+ffbv1ifJ92myL8rJP5OpU6f7vhtp6d0NJlF9zUy5+7JVFuzBDaN5OHNAzyi/5YO57+ZH9VkZjvQ58o9bXnfHM0DT+4R0torHlK+flHw4UyPe++HaUt9trfZQvXZrbQ/R7f2y+e/zk8+QPAAAAQIZp/gAAAABkmOYPAAAAQIYVfM+fz5we78dTdZ+f9Rs7x8d+89A0vuO5rdHYGYcURfn5px+SxmeNivf1adsmt/5+4v87JRr75YMVUb6ypvW7G96K0vuf3XEcQggPPpmb+4dPidcI5/vyz6vuHTC/xmMhi9436ok0nregbzT2kYm5/YCenfd63W/SIrc/0PUXxXt99drviSgvX57b3+ujZfvE16lYWPc5AKlBvePvA7+flNsfr+cBtd9L4/Ul8Z57I/Zfl8aPrq3+vKvPj+tAixr+Oexj749/7vv0fK3W86vJJ78e71f02Cu+A0BdHNAl/gE+/wPTa3Xe+vWdovwnk4dG+VV3VSki6/39D03F0uWdo7xPaTy+f7fcvnlfO290NHbDH15qoFk1fZ78AQAAAMgwzR8AAACADCv4sq+J46t/pLlFcby064vn55ZkXT1uUzQ2oHfea9hrcPVPc0u9brg771HtbXnvg64nv39sQZW4QW4B2ZEkabj6nfbR0LMLl9XtmiUdo/SPE7qn8VmnVP86+RBC+PjXc6+HfGaex7yhIcx+a0OUP/jUAWl8SJ/av5b10vMfivIL3tsjjde82zH/8FSfA6fGHxQX7fjABrRv1zYFvydk0cRP1bDGswY/uyte5vXg0+/GB2x5p65TAhrQZT+Mt1WZ/fu8A1rlxt97XPxzvGlL/zR+fXG8Bcy9z8Tf+79wZm5rmcdejPsRs+fX8DtC+wOj9IrT2ldzYM1+8Ke5dTqvOp78AQAAAMgwzR8AAACADNP8AQAAAMiwoiSpstlGTQf2PbNebvj0z3tE+eBDZ9fpOn+eNiLKZz6fW9f3pyc2R2Pzly/NJZXxWr09WfLGvY09BZqZ+qoD+ebcsncaDzg43uvjprtHpXGXTvEeXS+83jrKF7+1MY2/9NF4/f/Ag19M41nPxWv8L/9RvP3Zs3vQPj/qAIXWUHUgtOqQhrddcUA09NEzpjfMPQvg6p/kXjG75t34+8n/ezBvT7Mt60JdqAMUWoPVgdpqGe+Xteov8Z4enTq+XS+3+cMDI9N4Q0Xt/83893+P9zR76NU1uSTZFh9cx5/7fOoAhdToNaBNlyj92afiHsMlY6fX6jJvr+wW5f96o2eUjxz2TBq/Pv/gaOzNFZ2rvW671vHvLEOPfiGN17/bIRrbsjXee7SqLmdUP5avNjXAkz8AAAAAGab5AwAAAJBhBX/V+9AvxfmFxxyfxocf3C4aW7km91jkd6bGj0qH9UvjfGv8eCWw5xh40eo0LvvoydHY1y5+LI2LiyujsTNHhmrd/eDxUf61nx6Xxvc/u+cs64Jmo8rSh7sfil+92m3v3M/vacc/XpDpLFzSN40/PqlrNDZz8dbaX2hjlde0JpXVHwfU2jnviZd4tGu9tJojd895732kTudd9MHqx2Y9OyTKv3HTPlE+9cVFdbonNCsV8dLOvz8RL6Vq0SL3S8JhveNlocOH5Lad6dJ1RTQ2Mi8PoSiNDu7zRjRycJ/qp3f3lPj3kOdfy83nu3+Iv+O8vnR+9ReqZ578AQAAAMgwzR8AAACADNP8AQAAAMiwgr/qnfrjlY4UmjrQ9KgDFFqj1IE2uVexXjC0fTR0YI+2UX7D5Q9Xe5mf3pF77foTL9a8V+D88tzXo8dfbdr7hKkDFFpT+z5w0cm9o3zcWbm9QnvutzIaO3C/BQ0+n9cXHBLl7dtuTOOFb3WPxvr1fDPKu59VFOpCHaCQmloNqMmB3eP6MKR0WzVH1p8/vxjv6xM2ljf4Pb3qHQAAAKCZ0/wBAAAAyDDNHwAAAIAMa9nYEwAAqFHFijT8/WMrajgwhO/cvV8Noy/V04SApuSWh+fn5bl42MG9orEjSg+o9jqnHdsiyj90+qO1nsOEH5+cxn+ctTEa694ht8fIzGWtorFxQ/vmXWlere8J7NyS5fPz8kaaSBPgyR8AAACADNP8AQAAAMgwy74AAIBMeur1hXl59cf++sG8D66raRlpvperHXmthrO2uydAA/HkDwAAAECGaf4AAAAAZJjmDwAAAECGaf4AAAAAZJjmDwAAAECGaf4AAAAAZJjmDwAAAECGaf4AAAAAZJjmDwAAAECGaf4AAAAAZFhRkiRJY08CAAAAgIbhyR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNP8AQAAAMgwzR8AAACADNvjmj+jRo0KRUVFu32dSZMmhaKiojB9+vTdnxRQUOoAoA4A6gA0b2rArmmU5k9RUVH0X+vWrUO3bt3C4MGDw/jx48OUKVNCZWVlY0wtFBUVhVGjRu3yeX/+85/DRz7ykTBgwICw9957h7Zt24Z+/fqF888/PzzzzDP1P1HYw6kDgDoAqAPQvKkBhVOUJElS8Jv+pztXVlYWQgihsrIyrFmzJsyZMyfMnDkzbN68OQwZMiTcfvvt4ZBDDonOXbRoUdiwYUMYMGDAbs1h5cqVYeXKlaFnz56hXbt20dxGjhy5y12/iy++ODzyyCNh6NChYf/99w8lJSXh9ddfD/fff3/YvHlz+OUvfxnGjx+/W3OGLFEHAHUAUAegeVMDCqdRmz87unV5eXm47LLLwl133RUOOuig8Mwzz4Tu3bsXdG51+T9wRUVFaNOmzXafv/jii2Ho0KGhTZs2Yfny5aGkpKSeZgp7NnUAUAcAdQCaNzWggJJGEEJIarp1ZWVlMmrUqCSEkFx++eXR2MiRI3d4bkVFRVJWVpb07t07KSkpSUpLS5OrrroqqaioSEIIyciRI6Pjy8rKkhBCMm3atCRJkuTmm29O55X/X1lZ2W79eY8++ugkhJAsWbJkt64DWaIOAOoAoA5A86YGFE7L3e4eNYDi4uJw9dVXh+nTp4c77rgj3HjjjTVu5JQkSTj33HPD/fffH/r16xcuvfTSsGXLlnDLLbeEOXPm1OqeRx99dCgrKwvXXHNN6NWrV7jooovSsbqs8/uvV199NcydOzd07do17LfffnW+DjQ36gCgDgDqADRvakD9aZLNnxBCOOGEE0LLli3D8uXLw4IFC0Lv3r2rPfZ3v/tduP/++8OJJ54YHnzwwfTxqW984xvh2GOPrdX9jj766HD00UeHa665JpSWloZJkybVad4PPvhgmDFjRti8eXOYP39+uO+++0IIIdx0002huHiPe7kaNCp1AFAHAHUAmjc1oH402eZP69atQ5cuXUJ5eXlYsWJFjf8HvvXWW0MIIVx33XXRurnOnTuHiRMnho997GMNPt//evDBB8O3v/3tNN93333DLbfcEk4//fSCzQGyQh0A1AFAHYDmTQ2oH0265Zz8Z9Onmh7rCiGE2bNnh+Li4jB8+PDtxk444YQGmVt1vvWtb4UkScK6devCs88+G04++eTwvve9L1x//fUFnQdkhToAqAOAOgDNmxqw+5ps86eioiKsWrUqhBBCt27dajx27dq1YZ999gktW27/IFOPHj0aZH470759+zBo0KBw++23h9NPPz1MnDgxPP30040yF9hTqQOAOgCoA9C8qQH1o8k2f2bMmBG2bt0aevToEUpLS2s8tmPHjmHVqlVh69at242Vl5c30Axr773vfW9IkiQ88sgjjT0V2KOoA4A6AKgD0LypAfWjSTZ/tm3blj4KdcEFF+z0+EGDBoVt27aFWbNmbTc2Y8aMXbp3cXFxqKys3KVzdubNN98MIYQddh+BHVMHAHUAUAegeVMD6k+Ta/4sX748jB07NkyfPj307NkzTJgwYafnXHjhhSGEEK6++uqwefPm9PO1a9eGa6+9dpfu36VLl7B48eJdOmfTpk3h+eef3+HY008/HX7+85+HFi1ahPe+9727dF1ortQBQB0A1AFo3tSA+tWoLef/vjJt27ZtYc2aNWHOnDnpq9CGDRsWbr/99tC1a9edXufCCy8MkydPDg888EA4/PDDw5lnnhm2bNkS7rnnnjB06NAwd+7cWr9KbfTo0WHy5MlhzJgxYfDgwaFVq1ZhxIgRYcSIEdWes3HjxnD00UeHI488Mhx++OHhwAMPDBs2bAgvv/xyePjhh0MIIXz3u98NAwYMqNUcoDlRBwB1AFAHoHlTAwogaQQhhOi/kpKSpEuXLsngwYOT8ePHJ1OmTEkqKyt3eO7IkSOTHU1748aNycSJE5PS0tKkpKQk6dWrVzJhwoRkyZIlSQghOeuss6Ljy8rKkhBCMm3atOjz8vLy5Pzzz0+6d++eFBcXJyGEpKysrMY/z+bNm5Nrr702OeWUU5IDDjggad26ddKmTZukb9++ycc//vHkiSee2JX/eaBZUAcAdQBQB6B5UwMKpyhJ/vPOtIyaOnVqOO2008KVV14ZbrjhhsaeDtAI1AFAHQDUAWjemnsNaHJ7/tTV0qVLt/vs7bffDldeeWUIIYSzzz670FMCCkwdANQBQB2A5k0N2LHMbDN/xRVXhOeffz4MHz48dOvWLSxZsiRMmTIlrFq1KlxyySVh2LBhjT1FoIGpA4A6AKgD0LypATuWmebPOeecE8rLy8N9990X1qxZE9q0aRMGDhwYxo0bF8aNG9fY0wMKQB0A1AFAHYDmTQ3Ysczv+QMAAADQnGVmzx8AAAAAtqf5AwAAAJBhmj8AAAAAGVbrDZ+L+p7ZkPOgDpI37m3sKdDMqANNjzpAoakDTY86QKGpA02POkAhqQFNT21qgCd/AAAAADJM8wcAAAAgwzR/AAAAADJM8wcAAAAgwzR/AAAAADJM8wcAAAAgwzR/AAAAADJM8wcAAAAgwzR/AAAAADJM8wcAAAAgwzR/AAAAADJM8wcAAAAgw1o29gQA6sthB5Wm8fuHtonGPnX2m2n85L/2j8ZeeK2o2mt+795F8QdbN9R9ggAAAI3Akz8AAAAAGab5AwAAAJBhmj8AAAAAGZaZPX8O3r93GrdukURjJx6e+2P+9OpZ8YnbKuvl/nf/fUSUf+Rbuf1F7BECDeOS0/tG+fe+9EIat9trfbXnHdznjSj/aA33eP61IVE+9cVF1RwJbKdNtzQcd8Je0dCmLfHf1Uf0a5fGe7WNxy46M/d399Qn3hONvbm8dZ2mVr4q3utrypProvyp1xfW6bpAIyjpFKXnDOqcxl+7uCIaG3zEi7kkiWtNTcaXHRfla9dV/zvEq2/F9eVfixbU+j4ADcWTPwAAAAAZpvkDAAAAkGGaPwAAAAAZVpQktVvsWtT3zIaeS40GHlQa5R89pX2Ujz1tfhoXF2+Lxg7ad14uKY7X4IZttV/ruytu+tPoNL7kl2viwQ1v1cs9kjfurZfrQG01dh3Yzl6lUbrsttweHd26l9fLLdau6RzlH73q4Cif8lzj7gGkDlBou1IHrr94YBpfOe7hhphO/amM9+94fu5RaXzn1K7R2O2PxHuILFk+PzQmdYBCa6jvA/0OKE3jr3y4ba3Pa9cm/u5/wQem19OM6ub5l4+M8vMmdUjj15c2TL1QByikJvc7AbWqAZ78AQAAAMgwzR8AAACADNtjXvX+jXHxVD940tRGmkntjD/7oTS+/e/HRmOPvlzo2UBGvbsgSif9Ivfq9+9fsTYaa9M+t0xj0eJe0VjPg6p/pXOnzmuifNSQ+HXVU56rxTyhmfrQyXVbFvn2292j/NlXetfpOnMXtovy/r02pPHee8VLt4Yc+XyUH3XYv6rE8XVfnj8sypcsr9P0gDxTf7AsjQ/av3GXU+6Oow59Icr/+evOaXzTn4dEY1/+xZxCTAmapjZd4rxVpzS8aGi8PcthfeK/0wvhx/fmvjc09hLv+uDJHwAAAIAM0/wBAAAAyDDNHwAAAIAM22P2/Jn2dPyK9g+eVP2xb63oGeU3/bl/Ghflvem9phfdH3vE+ig/9dhZNU8SaFQ/f+CNNB5/dvxK9kGH5/bvWPNu+2gsrhg1+7/7Nuz8ICCEEMJpX829Iv3IA/aJxl54s/p/fyrfFI9VrH6jmiN3Q9seUTrvN/G+Qr0Oqn5t/2nHxjXk3mfqb1rQnP3+gdzeff/7ydrvr7Fmbdco/8YvB6bxp85eEY0devBLdZxd3XXosCaNT3pPvEnYwINKo3zO4gUNPyEooI+OyP39OnRg22jshKNXRvmgw2YWZE61dcaJh6bxYV/oGw+ubYDvJg3Mkz8AAAAAGab5AwAAAJBhe8yyr/97IH5d7OQnT6j22DVb47VdW9fU8RWK7faL0gU394nyg/adV+2pf/jHqDR+9PWldbs/UGffuTV+HeRXLzwijQcf8WKdr9u+VQ1rRYHIwmXzq8SNOJEd+OjQuEb0OuiFao4MYUtFqyj/7QPrqzkS2B03/yP3s/X//ja81udtqoy/+y9f+Woa3/l4/P39f88dncY/+NPGaOyur8d5nwPfSuMu+9RPETto3/Io37dTaZTPWVwvt4Em47Ybnswl2yrjwbx03qJ+1V7nkX8elMYr1sTPsLy+aGP+4bXSvzRehvblTzwYj/d5OY0vHzkyGvvRvXW6ZaPy5A8AAABAhmn+AAAAAGSY5g8AAABAhu0xe/6Eyk1RunJl9fvt1JePDGod5V06vV3rc5csr7I/wJZ19TUloJb+MGtBlP/xpdzrGZ/6zsBo7KiBtd8XrOwTJVF+/vW7PjegQFrG+/r85JLSNB73wVm1vszwTw+K8mfnLdytaQE7dszBub17fvdI7V/1XpNlK+LfGb708+qP/ci1vaP8d187MI2H78aePxs3dEjjz3wzvsdD/1JPyLa5rx+SxhWb4j30vnNr+yifPHNBDVd6uYax2uvaNbcP2LeO3Fbr895cvmnnBzVxnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLA9Z8+fAjn/xNI0/sQZ8Vi7tmtrfZ2v3r6qnmYE1MXHRsZr6g/t0yaNjzosf6+PolBbj7+4O7MCGtqpR/RM4w+NjvcSGP+hadWeV7k5/vewy759bBo/u2RFPc0OqEl97fMT2nRJwwM7doyG/jCx+n07Ou21JsoH9H2pTrdft65zlH/2m4em8T1PLKjTNWFPddgn1tQw2vB/vx7YPf6d4K6vb0zjYUfNrPHcu/8+Ihc//079TqwRePIHAAAAIMM0fwAAAAAyrNkt+8pfCvKVj8ePbw0ofTaNW7WqqPV1n/7X4PiDrRt2fXLALul/QGmU33Ndbmnmob3zlna1alElqf0yr3x/enJLnc8F6t+x/XpF+QM/eSKXtKz9v3ElSXzs0pVbc8m2zXWaG1AgVZZ5hRDCbZd3TeOPnjG9IFNYszZ3z0uuOzgau9tSL2hYrfeO0g8elVvu+bMrF0ZjPbq9WevLfuPWKt/7N5bXbW5NiCd/AAAAADJM8wcAAAAgwzR/AAAAADJsj9nzZ//ufaL8Eye1ifLRQ9fV6jonHZO3D8i2pNZzWPNutyj/2k8GpPFdz+RdZ/OyWl8XqJvDD4p/7vr1fD2XRHv81J8vfbBdlF/xiwa5DVBLZ53YIf5gF/b5iU5rvTXK//yjp9L4ydmDorH7ZnSO8vufWJ/GLyyM9xYAGl7Hdp2jvFD7/FT1lR/0T+O7n5hX8PtDc3b9BftH+ZWffLBervvDL+R+11i3oW+Nx/7zlVx/4jcPbYzGli5vGjXBkz8AAAAAGab5AwAAAJBhTXrZ15G9cq9vveeby6OxPge+Wser1v0Vz9OePjTKf/n316s5EiiEe56Il1d85cZj0/i7X3wyGmvVNl7SUVf7dfWqd2hK/jbr3Sg/tHR4Gh935BvRWNeudXtN6zGDZufl8fh1n6tM4+/cemo09rV7KnLJ2ng+QP14Z0tllD/x3HvS+Nij/1mQOVz7uUVp/PyCXtHYs/MsB4WGdPABmxvkuicf+0Stjz3z5Fw8oNeoaOxj32mVS7Y13u8SnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLAmvedPVUVFea9SL67j3j3F+a9/rtzhYTty9imPRfmYv+b2F7nvn9byQmP7yV9fS+P5S+NNOTq2r75mtGwRj/3kf19K4w4d380/HGhCHntlUZx/PRd379onGuvZMbcPx76d4+u8//j2UX7JeQ/lkqKd/FtZi9x3i//55MPR0HsGDEvj0/437ztIUvvvIEAN3l0QpR/7Zu80PrbvcbW+zC3XxPsDtSyp/T4i+/VYnMbnjuwfjT3bNN7yDJl1za3xPjr/74/H1Ok63TrFf9+fd2pur57fPxDXgwO7l0R51f1Gz//A9Ghs/+657wInfzXeyzhUbqrLVOvEkz8AAAAAGab5AwAAAJBhmj8AAAAAGdak9/x5YWFuH50TLo/X7V90Us8of/jZDWm8YXMd9wMKIVx4Wm7N/5c/PrXO1wEa11+f3YV9uPL28+i1/5A0nvT5adHY4AFvRfm+3XK1adkKi/qhKVm+cl5eXv2xf302zme9MDyNP3NuvM7/uMF5B9dg9PCn0vgrZ4+Mxr73x1dqfR2g9ua/Nb9KXPvz7nhvryj/yadzv298buxD+YdX638ujPf++vsTQ9P40ZcX5R8O7KaXFi+I81qeN+LQuKfw6XPifYY/+u3c3nzLV9b8u8XjL+X2G/3JFfH3hpFDc98FDts/3ocsf+4NyZM/AAAAABmm+QMAAACQYU162VdV+cspvvWHhrnP//x+/zT+8scb5h5AE9OyXZTmL/WqasvWuGxu2tYgMwIa2e8eyS0b+d2M1tHY1G8Ni/KTj3sq1EbvA/ybGzRpW9ZF6R+n5/IL3tc1Guvcqfp1pMUt4y8Hdd+QAqhvww/JLfWa9Kl4mdfEX8TH5i8fr8kz83LLwu74xyHR2LCjcvHff/BmNHbQh2t9i93mWwgAAABAhmn+AAAAAGSY5g8AAABAhu0xe/4UythBJY09BaDAvvmxXnmfVL++9zf3do/y1W/PbYAZAU1K5aYo/ecrHaL85PitrdV6Y0nlzg8Cmoy5K3K/Km3ZUvvfEf7vjtFR/shrC+prSsBuuuzDbdK4Xet3orGZb9ZPL2D2qxVRvnVz7rr777so7+j96uWeteHJHwAAAIAM0/wBAAAAyDDNHwAAAIAMa/w9f1rm1tydeXSPaOjeF9bkks1rG+T2nzylT5T/8Cv/apD7ADXYK/dzeM+X4570Hx/O7ZFx+6Pz6+V2bfbuG+WfP++ZWp9716zN9TIHoPaq/sxecXrraGze4g1RPnnmgvqfQHGrKD2q3/pan7ptSy5+du7G+poR0AD67Nc7ym+/Kvcz263r0lpfZ93GoviDrRt2fCBQcF075/bxG3rk7GjsviuPj/If3NEzjafNyd+rJ/aR40vT+L3HtYnGWpY0jd8fPPkDAAAAkGGaPwAAAAAZVvBlX6MO6xnlV1yQm8IHTpwRjfU494Q0XrlyN5Z97VWahh8fHD+G+b0vvRLl7duuqfYyGzZ2ivKNm5O6zwlI3fKZ3OsPP3jqo9HYgNL+afzW23H9mL+qRZy/lVsWNrRv/Pr2fvvnfva/9NF10ViHju9WO7fv/PqkKF+8enG1xwL1o2XneGnmzG/nXsV69GHx8uwWpw1qmEl0ys3h+rPjx7dPO+HhWl/mhdcOT+PpL9X8yDhk3alHxH+Pf+cLtV8OdfkP2qXx4jUtajgyhPnrc/++3T7v0O5ttqXxl85uG4297/j4Z7RPz9dqPT9gz/Dcqx3S+ORj47H3j5wZ5aOGdEzj8lXda7xur/2eTOPiltuqPW582fF5n8yr8br1yZM/AAAAABmm+QMAAACQYZo/AAAAABlW8D1/vnd5RZQPGvBctcdOODe3DnfdxoF1vuepw1al8bCjno0Ht1W/b88/Hh8e5b/5Szz+4IsL6jwnIOdX9+bW/Pc5YHA0dvzQ3M/s1F/G581fGL+S9V+v5/boOHnYi9FY+73ifX4iSVwHXnr10DT+2u+XxcduficADevmz8RfT/L3+alqUI/4Neyz36yyP9/mGvYLLIn38bvq3AOi/H8u/Gcad9hrJz/3Rbk9xda/2yEa+uqP2uYfDc3WXu3ivTePHPBiNUdub9ovd37Mfz3y9LA07r53/Pf/oQe/VPsL7YJf3X1yGj/1rxq+cwCN6qu3LEzjNiWjo7HPjX0oytu1z/3937t93X8H+Nnk3H1unt54e4l58gcAAAAgwzR/AAAAADKs4Mu+dsXlF0xtgKvGj5u+tSJ+5eSfpx+Sxpf+ckl8quUe0CBmzs29WnXWi/ESz9//Pbf88qdXz4rGeveaX2NeW6tXd43yIz65qpojgUJ49NmtUX7BGdUf+8xvn4rzF45K4zXv7l3teZ33ipehDzly2i7MMFZ1qdeH/mdANPbwHK93h0IbOfSpnR+0i1aviV/zvG1b/G/oN969MY3nvrkwAE3UltyyzMt+sSAauuy246L8f9+bW7q9d8faPzfz/Nz1UX7HzFdySVL9a+Abmid/AAAAADJM8wcAAAAgwzR/AAAAADKs4Hv+fPrb8StPPz0m99qzT53zUP7hdTJ3wWFRvn5j6zR+dHa8t8etD8SvYnxhYcO8/hGonSt/PSf+oFVuL432bU6s8dwj++Xij531WLXHvbM2fsXzey/vk3eEtfrQmKY8F6+H/929I9L4Y2c+WuO5Q458vt7ns21LnH/ntpOj/K8zc98lHn9V/YDqLFgR57+8K/5ZuvjMmWncqvWmQkwp8us/xvOprMzFn/31u/HB6/P2Bg0LGmROQAPauiHO310Qpd++q3BTKQRP/gAAAABkmOYPAAAAQIZp/gAAAABkWFGSJEmtDux7ZsPMoMp+Hpec3CMauvZzr6dxl85vRWN/+MeoKJ/+TG5R7q+fjG+xdc0buznJpil5497GngLNTIPVAepMHaDQGqUOVPmucP6x8d59JxzdLsrfWJL7PvCBEzZWe8lXFnSodiyEEGY8l9sHYO6b8VelZ+c1rX191AEKraHqwBVn90/j714xvV6uOfGno6P8pXnrqz32z88sjT/I3/CrCVMHKCS/EzQ9takBnvwBAAAAyDDNHwAAAIAMa/xlX9SZxzspNHWg6VEHKDR1oOlRByg0daDpUQcoJDWg6bHsCwAAAKCZ0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyDDNHwAAAIAM0/wBAAAAyLCiJEmSxp4EAAAAAA3Dkz8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGbbHNX9GjRoVioqKdvs6kyZNCkVFRWH69Om7PymgoNQBQB0A1AFAHai9Rmn+FBUVRf+1bt06dOvWLQwePDiMHz8+TJkyJVRWVjbG1EJRUVEYNWrUbl9n5cqVYb/99gtFRUXhhBNO2P2JQcaoA4A6AKgDgDpQGC0b5a7/UVZWFkIIobKyMqxZsybMmTMn/Pa3vw2//vWvw5AhQ8Ltt98eDjnkkOic2267LWzYsGG3733ppZeGsWPHhp49e+72tXbkkksuCevWrWuQa0OWqAOAOgCoA4A60MCSRhBCSKq79bJly5IPf/jDSQghOeigg5Ly8vKCz23kyJG7dY1bb701CSEkP/vZz5IQQnL88cfXz+QgQ9QBQB0A1AFAHSiMJtf8SZIkqaysTEaNGpWEEJLLL788Ghs5cuQOz62oqEjKysqS3r17JyUlJUlpaWly1VVXJRUVFTv8P1hZWVkSQkimTZuWJEmS3Hzzzem88v8rKyur9Z9t4cKFSadOnZJx48Yl8+fPV+ShGuoAoA4A6gCgDhRGoy77qk5xcXG4+uqrw/Tp08Mdd9wRbrzxxho3cUqSJJx77rnh/vvvD/369QuXXnpp2LJlS7jlllvCnDlzanXPo48+OpSVlYVrrrkm9OrVK1x00UXpWG3X+CVJEi666KLQqVOn8IMf/CCsWrWqVucB21MHAHUAUAcAdaCeNEbHKeyks5ck/+7UtWzZMgkhJPPmzUs/31Fn77bbbktCCMmJJ56YbNq0Kf189erVSf/+/WvV2as6t7o+1vWDH/wgKSoqSqZOnZokSdLonT1oytQBQB0A1AFAHSiMJvuq99atW4cuXbqEEEJYsWJFjcfeeuutIYQQrrvuulBSUpJ+3rlz5zBx4sSGm2QVL730UpgwYUL4zGc+E0455ZSC3BOyTh0A1AFAHQDUgd3XZJs/Ifz7MakQQo2PdIUQwuzZs0NxcXEYPnz4dmOFeI3ali1bwsc//vGw3377he985zsNfj9oTtQBQB0A1AFAHdg9TXLPnxBCqKioSNfEdevWrcZj165dG/bZZ5/QsuX2f5wePXo0yPyquuGGG8Ls2bPDtGnTQocOHRr8ftBcqAOAOgCoA4A6sPuabPNnxowZYevWraFHjx6htLS0xmM7duwYVq1aFbZu3brd/4HLy8sbcJb/9uyzz4YkSard+GnmzJmhqKgodOrUKaxZs6bB5wNZoQ4A6gCgDgDqwO5rks2fbdu2heuvvz6EEMIFF1yw0+MHDRoUHn744TBr1qwwYsSIaGzGjBm7dO/i4uJQWVm5S+eceuqpoWvXrtt9vm7dunDnnXeGHj16hDPOOCO0a9dul64LzZk6AKgDgDoAqAP1pDF2mQ417OZdXl6efPjDH05CCEnPnj2TFStWROM72s37lltu2eFu3mvWrNnl3by7deuW9OrVq85/tqoaezdvaMrUAUAdANQBQB0ojEZ98mfSpEkhhH938tasWRPmzJkTZsyYETZv3hyGDRsWbr/99h12zPJdeOGFYfLkyeGBBx4Ihx9+eDjzzDPDli1bwj333BOGDh0a5s6dG4qLa7e39ejRo8PkyZPDmDFjwuDBg0OrVq3CiBEjtusYAvVDHQDUAUAdANSBBtYYHafwn87ef/8rKSlJunTpkgwePDgZP358MmXKlKSysnKH5+6os5ckSbJx48Zk4sSJSWlpaVJSUpL06tUrmTBhQrJkyZIkhJCcddZZ0fHVdfbKy8uT888/P+nevXtSXFychBCSsrKyOv05G7uzB02ZOgCoA4A6AKgDhVGUJP95X1pGTZ06NZx22mnhyiuvDDfccENjTwdoBOoAoA4A6gDQnOtA7Z512gMsXbp0u8/efvvtcOWVV4YQQjj77LMLPSWgwNQBQB0A1AFAHdhek3zbV11cccUV4fnnnw/Dhw8P3bp1C0uWLAlTpkwJq1atCpdcckkYNmxYY08RaGDqAKAOAOoAoA5sLzPNn3POOSeUl5eH++67L6xZsya0adMmDBw4MIwbNy6MGzeusacHFIA6AKgDgDoAqAPby/yePwAAAADNWWb2/AEAAABge5o/AAAAABlW6z1/ivqe2ZDzoA6SN+5t7CnQzKgDTY86QKGpA02POkChqQNNjzpAIakBTU9taoAnfwAAAAAyTPMHAAAAIMM0fwAAAAAyTPMHAAAAIMM0fwAAAAAyTPMHAAAAIMM0fwAAAAAyTPMHAAAAIMM0fwAAAAAyTPMHAAAAIMM0fwAAAAAyTPMHAAAAIMM0fwAAAAAyrGVjTwCgOXjg26VRXpQ3fvr/LijUVIBddPD+vaP8p5cnUX7Wt1qkccXqNwoyJwCAXeHJHwAAAIAM0/wBAAAAyDDNHwAAAIAMs+dPPTntyJ5RfsukJWk88rJ47LU3FxRiSkAj+874gWk86j2PRWM3/fmEQk8HCq9tjzRs06ZDNFSxYVV87KbVhZhRnbx/SKsoP+XY6VH+pdNOSeMb7m4dn1y5qaGmBQBQa578AQAAAMgwzR8AAACADNP8AQAAAMiwBt/z56SB8X43++xVFOX3PLGwoadQEEf1axflT77Yu0pWWdjJAI3iuk8MjPIvjM3t87N5a5tobMbsDQWZEzSmSed2TeOJn5oajV3x/VFR/qN7m+6ePy+8XlHj+HWfezCNJz96fDQ2/635DTInoLC6d+0T5f/8f4uj/H1fPiCN/7VoQSGmBLBLPPkDAAAAkGGaPwAAAAAZ1uDLvo47In6168EHbovye55o6Bk0oKIWadhrv7iP1mu/d9K4OLQv2JSAxnPM4euivFWr3FKRh59+TzQ2eaalIDRvkz7zYpQvWnZIGv/pqaa1JLx7Z/9WBk3Zwfv3jvJ3Nue2mVi+cl693OOXX4i3rti0pSTKV25UJyBrDjuoNI2/+pF4C4cLxzwc5ad+7tg0fnjOogadV12pUgAAAAAZpvkDAAAAkGGaPwAAAAAZ1uB7/nziA/FrEGc+f0A1R+55Ou5dmsafHTstGrvlTyPTeO6brxRqSsB/nDSwZ5R/+YJcuTvjW/HeY+HdBXW+zwUnlqbx0f3jfQXmLjgsjSf8Iu+e0Mx1bP92lN/09dfT+M0vHhyNPfV6I+wB1KZbGl563tZan3beCfGeAN++q95mBPzHucf2ivKbJ82J8ok/OzqNf3Rv3e9zfP/cd4n3nxBvVHrtTaOifNmKeA5AE9Wpbxoe1Cb+fr64PN6T86mbcvsTtm2/IRr70ndGRfnDc16t03Q+cnxplN85c0GdrlMbnvwBAAAAyDDNHwAAAIAMa/BlX8XFSUPfotH87orq/2yvL9FXg8b0f199N8oH9H0pjUf+7pho7JGX636f//lE7j77dFkZjX3mm7lXzz63oGm9uhoKYfGyjbU+tvNeK9J4woUDorEPfqfKMs51hXl96uD926Xx8YMeL8g9gdr50OjWUX7X1MFR/qN762fLhfcdu1cat2gVL/+8d9a6erkH0MBaxPXi7xMq0/iQXuXRWO+x8altO+S+x0yZflw09uOpa+tlem1bF9XLdWpDhwIAAAAgwzR/AAAAADJM8wcAAAAgwxpkz5+jSnOvX9y/24t5owc2xC0bRecOm6sde2S2dcDQmNZvLIk/2Jbbo6tN3tCuGNQ7fr1snwOrbBi0rTIaa92qcGt4oSn69cOL07h7l1Oises+92C1540Z+ViUj3/shDS+aWo9TW4n5q7OfUV6dcGh0dghpdVvFPaHGRUNNifg30YMfiPKb/nrgGqO3D37da3y93qRfzOHPUbL3L59t3817j+ccnzuO8aMp98TjY0+PO/nPMntCfSd38X7foWN8X5BewJVDAAAACDDNH8AAAAAMkzzBwAAACDDGmTPn9Pe0zaN27Tb2BC3aByd+kZp34MWVHvoq6sa5H9aoAZlHx2Yxu85NN5P5MXXjkjjv89vVfuLtukSpV86r02Ut2+7Jo1nPDMsGvv948tqfx/Iom1b0vCG+9ZEQx9738AoH9B7TrWXueTc9Wl80xN94sF359V9fjXov3dubX9Ne/wAhdF+79z38LZt8n4mk9Agxp7+VBpvXN8uGlu/2b+hQ5NR0ilKJ52X2+dn7Acejsaefv6oNB45qXU09oWRLaq9xYvLG+b3+9XvbmuQ6+6IqgUAAACQYZo/AAAAABnWIM8u9etZ/euNX57fEHcsjN98Kl4qsm+PpWn8yuvxKyZXbqz+NfBA/ejaNV7+ccm5ucfAt2yNH+O84sb2ueSd+BWxNfm/8T2i/KPvj5eTLV6Wm8PILy2p9XWh2dmwNEofmx2/Pr2mZV+DD52dxoO6HheNzX53F+ZQ5dWvl55+QI2Hnn3ypl24MNDQ1q/O/d399pp9orF2bfLWfbXqkIu3rKvzPatuX/HPF4+Mxl5flvd3fj3dE6iFlvEyzNu+tG+Uf3RMbqlX/u/pX/1JSe4yeW2L6z73XJQvWlKaxqsrGub3+788vbBBrrsjnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLCCv4/8+dc2FPqWNWu3X5SOHZTbJ+TDp8b/85xx4sxqL3PDzZ3jD9bvwZsbQRN1ZK9eUf6Ha1dGeY8uufX33//tqdHYw3Oq308k3xVn90/jT5/zSI3H3vCbqvuG1H4vIWjunnkp3lPnU+fU7rxjBsT7783O++v2xAE903jooW2jsXZtcn+vX/OZqbW7YS28+NoRaTx/XfX7HgL14w8Pxt8HrvxkvB9ft86j0vhrN8f7dCwur9t39H69FkX5P751SJT/4PbcvkMPPG/PH2hIZx7dLco/OubRao994sV4/879u1Sk8TXHtYnG2neINxKc+dxhuWT9glrPb1DvuEYdsn/uu8FfXopfJ1+xunC/P3jyBwAAACDDNH8AAAAAMqzgy7723qtu/aajS+NHp4rzLjPqqNwjW/t1jf9YJa1yj2FeNOaf8XWKXo/yDRW510HPfO7gaKxic/xKuQ4tV6fx06/nvWISqJsW8SvaLx6ZW1Z10zV5Sy+L48cmw7bcI5XDj1wTDf3vh3Ovlf72nxdHY206xI+Dnnty7pHPoqJt0dhNfxod5b/4+8sB2HU3TZ0X5ccfdXIaX3jWtGrP++mEx/Ly/CPeqv6mVWvGtvpbnnVE/3+l8bhjjo/Gfv1g/tHA7rrqrnhpxsEHjIjy88/ILQE5f0z8s37HfSemccXm+BeKPgfmLdcqWp6GHTuuiYbunxF/X3ng+VdrnjRQb+59dmmU//qeUVE+7tzpaXzR2fEWDhedXfv7DOyTu8+PP3dYDUeGcNKQZWk8oPdT0VhxlRXrZ39xWDR27zO1n8/u8uQPAAAAQIZp/gAAAABkmOYPAAAAQIY1yJ4/GzdVWVubxPtl/PTK+FVmX3lz/1pdc/DAJ+IPiuK+1bYtuXjdxs7R2Iuv9k3jX9wzJBp7Ie/V8/f/Kzf31WuXRGPL7or3/GnX5p00nvvmgh3OG9g1Hz8hrgk3lc3IJdvyDg6VUTZ3Xm5fn+OOejoaO+6oXDzmxKHR2EH7xj/rB/bIvQZ22cqDorFLfmiPH2gIP/njxjS+cExD7aNXpWZsa5h7vOfQ+LWx9vyBBrAufu36R77ZKso/+uixaTxmRDx2SK811V52n055e/4kuTpxzpfifTr+8uzC2swUaAhVf/kPIXz6/+L9PKfMzP28HtmvQzS2f7fcLxTjPzS95tskuZ7DmBHzazgy9vBTx0T5S/NzfYSX39qQf3jBePIHAAAAIMM0fwAAAAAyTPMHAAAAIMMaZM+fy3/2UhovLj8pGjv28K11uubCt06M8r/NjDf/eGVR7rqzXo3XAYewpJq4Zp8+vW+Ud+s2K8pfn39wlezdWl8XiJ1/Qmka33JN/HO2ZUtu/4y313aLxj4x6YAof3tdbs+uGz4zPBo79djcdfP3AwrFRXFepbzs2yWuGQv/UBrlI76Q26No4bLarwUGCq/qvmBJEv/c3/dYvN/YO+tze31c+1kb90CTlrf/x+2Pzq8S1/4yF47qHeU3Xzcvjf/yat6vTZWban9hoGFtiffr+tNT66rE8aFXnX9YtZf58e/i3sWXfv5SNUfuzIJaH/n+QT2j/G+z83sZ9ceTPwAAAAAZpvkDAAAAkGENsuyrqu/d80r8wT0Nfcf6M2JQzf/z3P1Q1Ue05jTsZCDDLhqTi99Yckg09q1bcku9fvPgvBCr/rHIK/6vNMp/2ir3ysUR73my9pMril8H/eCT8SPhC5fl1TigoFaujpdrzX9z3zT+8eT4teu/f2xBDVdaHWWD+/RK42s/W+fpAXuQg/Zr29hTAOpbp3grl6+PfyyNn5g9KBqr+zKvunt5WYuC3cuTPwAAAAAZpvkDAAAAkGGaPwAAAAAZ1uB7/mTZ3x73eneoD/fPaJXGv388fp37ypX5+/zUTo+O26L8qP5vVHvsR688LspfqOENiy+9vbZO8wFq9uyyzWl8059GR2N9DtiQxi/Pbx+N3XT/+ih/YeHCBphd7b3v+PnxBzdX2R9wXcO9vhWog1YdonTMCaui/PmXDs8lG1cWYkZAPfvI4ZVR3rL11jT+07SOeUcvLcCMYvPfmr/zg+qJJ38AAAAAMkzzBwAAACDDNH8AAAAAMsyeP0Cj+/F9r9XPhdrtn4Znnhiv4+/U8e00fnXeodHY5JkL6uf+QN1teCsNL/nhWzUc2LT13C9vf7EW++/4QKDRtWzfI8qHHjkjyq//1Sm5ZPOcQkwJqA9tcnuIXnxmPLRkaW4vvu89tKVQM2oSPPkDAAAAkGGaPwAAAAAZZtnXrigqitKD92+RxjPnFnoyQL7/eX+nNP7C+Q9GY8uWH5TGw6/oknem17cDseXrcv8+9mZ5aTR2QI8Ftb7Ozy8qSePP/F/reLByU12mBtSTTwxJahy/b+a6As0EqE9fPH3vND71+Meisc9dOzyXrM1bqp1xnvwBAAAAyDDNHwAAAIAM0/wBAAAAyDB7/uyKJF4XXJy3BxBQWPt26xPl4z+4JJck8c/nz+8ZkMar3/a6VqBmS5bPT+OPXN0rGvvD9fH3gf27L6z2Op8656E0/swvjogH7fkDjeqIfm1qHH/6jep/toGm4/CepVH+xQsWpfE9fz8hGvvFtPJCTKlJ8uQPAAAAQIZp/gAAAABkmGVfu+E9h+UeFb354UacCDRT0773dpT3PWhuGv/mz6OjsWt/b6kXUDePvxov/fjQ1+JlYH/6TmUa9+iyJFTn5L7to/zhOW9XcyTQUI4uzf38XvbRadHYI08Nyzt6cQFmBOyuce9vF+Vvrci96v3/7s5bYr35nUJMqUny5A8AAABAhmn+AAAAAGSY5g8AAABAhtnzZ1d4tTs0Kbf8df8o/+alL6fx3x/fUOjpAM3Ek6/HewB99Ye90/jy87tFY/fP6JrGD7+xrGEnBuxUlw5JLkm2RWPPvNSxwLMB6qL93n2jfNwHn4nyn/5haBo/+rJ9P//Lkz8AAAAAGab5AwAAAJBhmj8AAAAAGWbPnxo89NSWKD//jKSaI4HG8O27Xs7Lq+4BFO/JAdBQbn90fpU4f7S8oHMBanby0L3SeOXb+0Vj/3P7ykJPB6iDr7y/TZTfO31QlH/tnopCTmeP4ckfAAAAgAzT/AEAAADIMMu+anDzw/Py8h55R7xUuMkAAAD15qV5B8UfbFzcOBMBdm6vPmn42Q/Fv4d/7Ot5P8trFxViRnscT/4AAAAAZJjmDwAAAECGaf4AAAAAZJg9fwAAgGbhqpvnNPYUgDq48/IWaXzdTQdHYw+++Fqhp7NH8uQPAAAAQIZp/gAAAABkmGVfAAAAQJP1kess7dpdnvwBAAAAyDDNHwAAAIAM0/wBAAAAyLCiJEmSxp4EAAAAAA3Dkz8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGab5AwAAAJBhmj8AAAAAGbbHNX9GjRoVioqKdvs6kyZNCkVFRWH69Om7PymgoNQBQB0A1AFo3tSAXdMozZ+ioqLov9atW4du3bqFwYMHh/Hjx4cpU6aEysrKxphaKCoqCqNGjdrl8/77/zDV/ffAAw/U/2RhD6YOAOoAoA5A86YGFE7LRrnrf5SVlYUQQqisrAxr1qwJc+bMCb/97W/Dr3/96zBkyJBw++23h0MOOSQ657bbbgsbNmzY7XtfeumlYezYsaFnz567fa2qPvGJT4TS0tLtPj/44IPr9T6QFeoAoA4A6gA0b2pAASSNIISQVHfrZcuWJR/+8IeTEEJy0EEHJeXl5QWf28iRI3f5vLKysiSEkEybNq3e5wRZpA4A6gCgDkDzpgYUTpPb86dHjx5h8uTJYdSoUWHx4sXhm9/8ZjRe3bq+TZs2hUmTJoU+ffqE1q1bh969e4err746bNq0aYePa+Wv67vlllvS6z7yyCPRY1mTJk1qiD8qUA11AFAHAHUAmjc1oH416rKv6hQXF4err746TJ8+Pdxxxx3hxhtvrHEjpyRJwrnnnhvuv//+0K9fv3DppZeGLVu2hFtuuSXMmTOnVvc8+uijQ1lZWbjmmmtCr169wkUXXZSO7co6vxkzZoRnnnkmVFZWhtLS0jB69OjQtWvXWp8P/Js6AKgDgDoAzZsaUI8a43GjUMOjXf9VUVGRtGzZMgkhJPPmzUs/Hzly5Hbn3nbbbUkIITnxxBOTTZs2pZ+vXr066d+//w4f16ruUawdHVsb/71e/n+tW7dOrr766mTbtm27fE3IMnUAUAcAdQCaNzWgcJrcsq//at26dejSpUsIIYQVK1bUeOytt94aQgjhuuuuCyUlJennnTt3DhMnTmy4SVZx1FFHhd/85jdh3rx5YePGjWHhwoXhV7/6VejcuXO47rrrwlVXXVWQeUCWqAOAOgCoA9C8qQH1o8k2f0L49yNbIYQaH+sKIYTZs2eH4uLiMHz48O3GTjjhhAaZW76zzz47XHzxxaF3796hTZs2oWfPnmH8+PHhb3/7W2jVqlX43ve+F1auXFmQuUCWqAOAOgCoA9C8qQG7r8k2fyoqKsKqVatCCCF069atxmPXrl0b9tlnn9Cy5fZbGPXo0aNB5ldbgwcPDsOGDQtbtmwJjz/+eKPOBfY06gCgDgDqADRvakD9aLLNnxkzZoStW7eGHj16hNLS0hqP7dixY1i1alXYunXrdmPl5eUNNMPa++//g65fv76RZwJ7FnUAUAcAdQCaNzWgfjTJ5s+2bdvC9ddfH0II4YILLtjp8YMGDQrbtm0Ls2bN2m5sxowZu3Tv4uLiUFlZuUvn1GTLli3h2WefDSGE0KdPn3q7LmSdOgCoA4A6AM2bGlB/mlzzZ/ny5WHs2LFh+vTpoWfPnmHChAk7PefCCy8MIYRw9dVXh82bN6efr127Nlx77bW7dP8uXbqExYsX79I57777bpg7d+52n2/evDl88YtfDIsWLQoDBgwIQ4YM2aXrQnOlDgDqAKAOQPOmBtSv7RfCFdCkSZNCCP/u5q1ZsybMmTMnzJgxI2zevDkMGzYs3H777aFr1647vc6FF14YJk+eHB544IFw+OGHhzPPPDNs2bIl3HPPPWHo0KFh7ty5obi4dn2u0aNHh8mTJ4cxY8aEwYMHh1atWoURI0aEESNGVHvO22+/HQ499NAwZMiQcOihh4b99tsvrFixIkybNi3Mnz8/dO3aNdxxxx21ngM0J+oAoA4A6gA0b2pAATTG++VD3vvuS0pKki5duiSDBw9Oxo8fn0yZMiWprKzc4bkjR45MdjTtjRs3JhMnTkxKS0uTkpKSpFevXsmECROSJUuWJCGE5KyzzoqOLysrS0IIybRp06LPy8vLk/PPPz/p3r17UlxcnIQQkrKyshr/PGvXrk0uu+yy5Jhjjkl69OiRtGrVKmnfvn1y5JFHJv/7v/+blJeX78r/PNAsqAOAOgCoA9C8qQGFU5Qk/3lnWkZNnTo1nHbaaeHKK68MN9xwQ2NPB2gE6gCgDgDqADRvzb0GZOZ5w6VLl2732dtvvx2uvPLKEEIIZ599dqGnBBSYOgCoA4A6AM2bGrBjjbrnT3264oorwvPPPx+GDx8eunXrFpYsWRKmTJkSVq1aFS655JIwbNiwxp4i0MDUAUAdANQBaN7UgB3LTPPnnHPOCeXl5eG+++4La9asCW3atAkDBw4M48aNC+PGjWvs6QEFoA4A6gCgDkDzpgbsWOb3/AEAAABozjKz5w8AAAAA29P8AQAAAMiwWu/5U9T3zIacB3WQvHFvY0+BZkYdaHrUAQpNHWh61AEKTR1oetQBCkkNaHpqUwM8+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABmm+QMAAACQYZo/AAAAABnWsrEn0CCKW8V5+wNqfeqED7RL43ati6KxQ3pujvKL/29bGv/sU/H/lB8767E03rIxHrv21ydE+fV3vFTr+QEAAADsCk/+AAAAAGSY5g8AAABAhmn+AAAAAGRYk97zp3vXPmncvlUSjY0YEB877PC2ady5w9ZobOz7H6mX+Sx6s0+U3/S5g9L4vPdOj8bWv9spjZ95qX809sSL6+plPgBA09K1a/xd4Stn5b6fDD9ibTR2/NBno3zm04PTeMQXlzTA7ID6cvX5A6sdu+bSafVyj9MuGRLlD/1rUb1cF2iePPkDAAAAkGGaPwAAAAAZ1qSWfQ3u0yvKp/54bhp37rSy0NMJoTJOy36+b5RvqNiUxvc8dGw0tnRVbpnaivVxj+21NxfUz/yAJmXfbvFyj04l29L4vJHto7FLz5tT7XX+8sihUf7pG1+uh9kBdTX2+NIoP3pA/PNcdTnX8UOfqPN9Fpe3q/O5QP2rurRr+6Vcyxv8/v/4xTNR3uL47g1+T6DAWsZ/9/fp1qPaQ+e9NX+3buXJHwAAAIAM0/wBAAAAyDDNHwAAAIAMa1J7/jy7oijKV6zqksb1tefPzH8OjfLV77aO8tOPezqNN21pG43dNn331tgBe77Tj+oZ5e8/vkMaf/wDz0VjnTqtziXJtlBbw4/cu05zA2qvpleyhxDC2NPeSOODDnyqzvep+vr2n90Vf+2aPHNB3tGv1vk+wO4bfXj8d3x9vbK9Jg/NjH83eXR2h2qODCGE6vcLBBpZScc0HHpQ/F3+2ENL0vjIfvF3gaP7r4rywQNnVXuLFifutzsz9OQPAAAAQJZp/gAAAABkWJNa9hXeXRClE36ae/X7qcecHI29+PqWKP/J1x6r9rLPvDgojUdcuTEerFgSpQMPyj2e/bkP5r9y9aVq7wFkx//7Qu5V60f2WxuNHTvo6fzDq7Xu3dzjn7fc955o7PlXK6L8N49WWdq6eXUAGtaPPhV/BRp7RvXLOyb/9YQov+/RzdUfu91SriU7OgxogvJfrV7VrizPevzFd+Nz/7WohrsurNXcgMZ1zMG9onzM8XENOOPEZWl8RP8n4pOLqjxzswtbQfxj5nF5nyyo9bk74skfAAAAgAzT/AEAAADIMM0fAAAAgAxrWnv+5Pnjk7k1sH98Me+1ZhvLo3Rg39Fp/JnzHorGfjy5yutbK5aFmsxZvCCNP/+TWk4U2LPsFb/i+acXtYnyT5+X2/tj1apu0dhTzx0d5T+4vXUaP5e3pH/tplx/fflKe4ZBY/vWuIFpfPzRb0Rj+fv6/OyPuX25Zs71CnbYk1R9ZftxR+wVjV13R/WvSz/tkiFRHu/VY28eyIKTB+bqQ+cORdHYpR+J9+MZecw/c0kS9x/yzVvYN43vmxZ/pyiqcpskic978fX495D7Zq1L46deX1DjPXeVJ38AAAAAMkzzBwAAACDDNH8AAAAAMqxJ7/kT2fBWjcNr1xVVO3bhB3IL625/rEU8mFTu1rSAPc93x7aN8s+c92CU33jbKWn8lduWxydX1FyLgKZj7PGlUf7VT+b285r59OBo7PJfbY3ylSvzNvECmqx/fKdXlI8+/ulqj73uju7VjsV7/AB7rPYHpuHrv1odDfXaL1cfilvGe/xsJ8k9K/PXacOjobO+k9dH2Lw2F2+K9xVsKjz5AwAAAJBhmj8AAAAAGbbnLPvaiQl35F7h/p4Bx0Vjpwx/PI3fd9Qx0diU/HczA3uu1nun4cRz94+GLvxA7hWtX/3Rumhs1pePjfI/Pbc4l2x+px4nCBTS5z68tdqxWS92ivJT+q+P8sVdcq+Cnfn/27vz6KiqbIHDOyEkEBCBEMKYARRQUCEMModBcWpQUBQcUBoE28ZGsVVEMKAo7dDigGPbragIraKtiEFllkEFQdSAzDMkEEKYkhCS3PdH97u3dkGFSqgpJ79vLdc6O/vWrePrVzuV7T3nbOS7AhBKXI9yFyl5mVfqtJ5uP/F81DuAciK8sgon3dZMxUP7brbHDevvVLlTJ6Ps8SaX49lFRBb8WF/FXy13/maYt+WknsOJPd7PN0Tw5A8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwYzZ80fyD9nDEVNrqNQvl9a0x6+NPahy83+4SMU/bzplj1/9apt+D+ssR8EBCKonbnH2+Xnsbn18+4wve9jj//zsdlw7+/oAxpjxqLPuv0v7ZSrnerx7RtYplfvbXzL0tT87e4ps/EcTlcvKcvt+ACCgvnlzdYn5Bcvb2+PJM9njBzBNt2Z6b57xI/T3fglzecbF0qlxrzr7A7/wxQ6dLFrvg9mFLp78AQAAAAAAMBjNHwAAAAAAAIOZs+zLxc6M7Sr+46QW9vjtxzfp3ICFHu9TtYo+GnLaQv2IeP7hrWWdIgA/eGzkIiewdG/700X5TsAyL6Dc6tJcH/E85U96SbbrUq8Hn+2qch8sd45+z8rS3wfq1Wmp4of+6NST0f/oWLbJAggK16Pfv3m2vcotXVvdHq/89ZjKLfhtl38nBsAn9h+tpOI9+xNV3KiB58/y6ME7nPsc1MvHZi7bISbjyR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAgxm554+72d/vtMeb/9RM5aaM1P2vq7utsMfP3L9I5RrH9VbxhFlN7fHRbPb/AYLt+zWt7XHH1j+p3IsPZtrj/Kf1niHz1rHGHygv2reoomL349xd9/l58XO9rw8AM6RO0/tyThq1yMOVev+f/8be35dj4oHQtGWf3uO3y6gkFb96XyN7fE2X71XOdT+gvwyqrXIz9VcK4/DkDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDBwizLsry6sGk/f88lOKo1UuGQ9pXt8TuTlutrw8NU+PWyzvb42kf1usNAsLZ+EfD3RMUWjDrQqVmCPV657aBOFubq+LxEezj+D9VUatK98+3x8eM1Ve7yuy9S8e97dpR6nsFCHUCghdr3gTp1mqg4K2tbme6z4996L6HGjZw9AeKu7+iT9/AX6gACLdTqgLvxg1t6zJW0P5C7PiPb2eMFv4X2/oDUAQRSqNcAV30u1Xt9pr36gz3esOVilWs19HBA5uQP3tQAnvwBAAAAAAAwGM0fAAAAAAAAg1WIo95LdGKPCt9b7IzfLqqscpXCC1V8xeU/2uPerdqqXKg/GgqEkiq1mtrjJU+dULmkhjvs8YMv6OUd7y9xW255zLl28tymKjXpXmdcvXqOysVWL1bx72eZL4DQUdYlWPdf30zFjRvp812f+5dz5HNWFsc9A+VJSUe0r/zVWcr1yBC9pYP7sfDfvLnaHlfqUtdHswMQSGFhnnPLfq7n9pPyu+zLGzz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwCrfnz2WJCSq+vkt1Fbe7KM8eV6q8v8R7rdvUyh4vSN/rg9kBFdPWt3fY45rn6bW2D7+UbI/fX7LF63tO7FfFYy5taRcVf7crz8OVAPxlUJdEFc9avsPv7+m6z8/9t+q9+Xbv0UfBPv85dQEwkeu+nJ3W6iPhe3dxu3Z5e5dopx9nBUBE5Mkhzmdy085clTttr08vjRroueWxM6OEDYEMxJM/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAYzcs+fCxsmqvjevtH2+Kbem1WuQT295r8kxYW6V7bvoMt+QVam9xMEoLw009mHI3XEDyr38qPLXcYl32fj1ub2uHnThSq3fdcF9njiP4v0C3NL3t8LgO/NePZHFd+7ytnf67WP9deT0uwH9Ldhzn4BD/1xkVv2gHPPL7uqzOh/FKo4K2ub1+8JoPzo3crZ36t7m+NBnAkAd+NGON/fN21tpnK/7HT27l23o+Q9uGrUbmqPU9r96vG6RWtOlHaK5RpP/gAAAAAAABiM5g8AAAAAAIDByu2yr4iaTVU8KqWSPb53oF7C0TRhZZne4/uf26r4ufcrq/g/P24t030BaM9+ssEeFxa1V7lLL3COYLy688YS7xNzfo49/mJBZ5V7+C1nqdfm/XvLMk0APuR+tHqX9mvscXx9nevb3Xn0u0trvVy7cSP35dvO0q7lLkvJREQGTI60x1lZm0o1XwCBNX6wPobddYnWM+9ZKud6fLs712VeIiLfvLna6zksXVv97BcB8JlZc7vZ40HXfadyaz5wxhs2t1C5GtX08q2GDV3+/rd0vZj85hX2+PvN6WWdarnEkz8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABgvtPX/Od/b1uTL+lEr9/f4cFbds9luZ3mL5T3p/kZdmOXsHzf5hj77YcjseGoDPvfBZyfv6eI89uYBQNvbleiq+d2Ade+y6/4+IyCCXfX3cj2jf/Y3eA3DOsmP2ePlGz/uAAAg9rvv8TBq1yON1vbuU5q4Hzn7J/yxYrv8umDyzYu0HAgTbg28X2uPc/B4qd1mzbHvcttXPKrd+s94jbMY/e9rjPZknVe7VxUfOcZblF0/+AAAAAAAAGIzmDwAAAAAAgMGCv+zrvER7+OF9kSqV3CLDHl+YVPalIEtWdbDH0/6t+12frs3RFxdU3MfAAAAIlFnLd7jFrlHdEl7JEe2AqUpa6uUPqdN6qphlXkBwZRzcZo/vnuqWjIi2h0mxnVVq++Gj+tp8PstnwpM/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAbz+54/nZolqPgvA6N0/lLnGNbGDbaX+X3ycqvb479/0FHlUj929g6S/ENlfg8AAAAA/uG6B09J+/+479XTvc1xFS9d6/xdUPI+PuwLApQbhbn2cPv+svcNKjKe/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwmN/3/LmuU3UV33z1fK9f++vGVvZ4ztJ6KldUrK+d+NkRJzjB+l0AAACgPHHdn2fyzLolXOn2XX+mf+YDACbhyR8AAAAAAACD0fwBAAAAAAAwmN+XfY2fnu4W1y/Fqw95GAMAAAAAAMAbPPkDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAYLsyzLCvYkAAAAAAAA4B88+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgsHLX/OnRo4eEhYWd830mTpwoYWFhsnjx4nOfFICAog4AoA4AoA4AFRs1oHSC0vwJCwtT/0RFRUlsbKwkJyfL8OHDJS0tTYqKioIxNQkLC5MePXqU+fUZGRnywAMPSPPmzaVq1apSq1YtSU5OlrFjx/pukoABqAMAqAMAqANAxUYNCJwwy7KsgL/p/7pzqampIiJSVFQkOTk5kp6eLsuXL5eCggJp166dzJgxQ5o1a6Zeu2vXLsnNzZUWLVqc0xyysrIkKytL4uPjJTo6Ws0tJSWlTF2/5cuXyx/+8AfJzc2Va6+9Vpo3by55eXmyZcsWSU9Plx07dpzTnAGTUAcAUAcAUAeAio0aEEBWEIiI5emtMzIyrIEDB1oiYjVu3NjKzMwM+NxSUlJK/br9+/dbMTExVkJCgrVx48bT8gUFBT6YHWAO6gAA6gAA6gBQsVEDAifkmj+WZVlFRUVWjx49LBGxRo8erXIpKSlnfG1+fr6VmppqJSUlWZGRkVZiYqL12GOPWfn5+Wf8Hy01NdUSEWvRokWWZVnWO++8Y8/L/Z/U1NSz/jvdd999lohYaWlpZ70WAHUAAHUAAHUAqOioAYETUarHhAIkPDxcxo8fL4sXL5aZM2fK1KlTS9zIybIsufHGG2Xu3Lly4YUXyqhRo+TUqVPy7rvvSnp6ulfv2bp1a0lNTZVJkyZJQkKC3HXXXXbOm3V+M2fOlFq1aslVV10l69evlwULFkhubq40bdpUrr76aqlevbpX8wDwX9QBANQBANQBoGKjBvhQMDpOcpbunmX9t1sXERFhiYi1bds2++dn6u699957lohY3bp1s06ePGn//PDhw1bz5s296u65zq20j3Zt27bNEhGrffv21ujRo0/rDsbExFhz584t1T0B01EHAFAHAFAHgIqNGhA4IXvUe1RUlMTExIiIyMGDB0u8dvr06SIiMnnyZImMjLR/XrNmTZkwYYL/Jvk/Bw4cEBGRNWvWyFtvvSXTpk2TAwcOyL59++TZZ5+VI0eOyI033igbNmzw+1wAk1AHAFAHAFAHgIqNGuAbIdv8EfnvI1siUuJjXSIia9eulfDwcOncufNpua5du/plbq6Ki4tF5L87kz/++OPy5z//WWJjY6V+/fry0EMPyV/+8hfJz8+XF1980e9zAUxDHQBAHQBAHQAqNmrAuQvZ5k9+fr5kZ2eLiEhsbGyJ1x45ckRq164tERGnb2EUFxfnl/m5qlmzpj3u37//afn//9mPP/7o97kAJqEOAKAOAKAOABUbNcA3Qrb5s2zZMiksLJS4uDhJTEws8doaNWpIdna2FBYWnpbLzMz00wwdTZs2tf+fy/V/7P9Xq1YtERHJy8vz+1wAk1AHAFAHAFAHgIqNGuAbIdn8KS4ulqeeekpERG699dazXt+mTRspLi6WFStWnJZbtmxZqd47PDxcioqKSvWayMhI6datm4iI/Pbbb6fl//9nSUlJpbovUJFRBwBQBwBQB4CKjRrgOyHX/Dlw4IAMGjRIFi9eLPHx8TJu3LizvmbIkCEiIjJ+/HgpKCiwf37kyBF58sknS/X+MTExsnv37tJNWkTuu+8+ERF5/PHH5cSJE/bPc3Jy7DkMHjy41PcFKiLqAADqAADqAFCxUQN86/SFcAE0ceJEEflvNy8nJ0fS09Nl2bJlUlBQIB06dJAZM2ZInTp1znqfIUOGyKxZs2TevHnSqlUr6devn5w6dUpmz54t7du3l40bN0p4uHd9rt69e8usWbOkb9++kpycLJUrV5bu3btL9+7dS3xd//79ZejQofLOO+/IJZdcItdcc40UFRXJl19+KXv37pUbb7xRbr/9dq/mAFQk1AEA1AEA1AGgYqMGBEAwzpcXt7PuIyMjrZiYGCs5OdkaPny4lZaWZhUVFZ3xtSkpKdaZpp2Xl2dNmDDBSkxMtCIjI62EhARr3Lhx1p49eywRsa6//np1fWpqqiUi1qJFi9TPMzMzrcGDB1t169a1wsPDLRGxUlNTvfr3Ki4utv7xj39Y7dq1s6Kjo62qVatabdu2taZNm+bx3weoqKgDAKgDAKgDQMVGDQicMMv635lphvr222+lT58+MnbsWJkyZUqwpwMgCKgDAKgDAKgDQMVW0WtAyO35U1b79u077WeHDh2SsWPHisiZj1kDYBbqAADqAADqAFCxUQPOLKh7/vjSmDFjZN26ddK5c2eJjY2VPXv2SFpammRnZ8vIkSOlQ4cOwZ4iAD+jDgCgDgCgDgAVGzXgzIxp/gwYMEAyMzNlzpw5kpOTI1WqVJGWLVvKsGHDZNiwYcGeHoAAoA4AoA4AoA4AFRs14MyM3/MHAAAAAACgIjNmzx8AAAAAAACcjuYPAAAAAACAwbze8yesaT9/zgNlYG39IthTQAVDHQg91AEEGnUg9FAHEGjUgdBDHUAgUQNCjzc1gCd/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYF4f9V5RXNAgyR6nPXdQ5SIqFak4aVBeQOYEAAD8a+LtLVU84U+L7PGu3Qkql3RXdf3ifP19AQAAINTw5A8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwSr8nj8vjNRr/G+7Jt0e16m1T+U+XZDi9urN/poWgCBqHOfs/TVlaKTK3XLVUhW3uv1ye7xx7w6/zgvAOQqvbA/H39JMpR6+c7mK5y7qaI9/TK+qcpfVO67idTt8ND8AAXVtm3gV3z84TMULVkXb4182n1C5tJ93+W9iAOAHPPkDAAAAAABgMJo/AAAAAAAABqsYy77Ob2oPl0w8qVJd283X1xZb9nDt761VauBUfdQ7ADN0a6Ef+577orP8MyMrTuWmzuil4o1HjvlvYgB86vZujezxpD8vVLmn3tKf7cffSxcAZruiw3kq7t1J/13Qu7Pz38kXrOigcmm/x+ib5R/y7eQA+EdYJXs4flALlZo0apGKx77Q3R4/N/t3/84rAHjyBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgRu75c0GDJBU/M8I5trFr8o9uV+sjHR9+sac9XrclT196bJtP5gcguPq1S1DxR39bq+KXZ3Wzxw+/v0+/+CT7gADlRccL9Wf9hTHO7/FV6y5Tucdn7gzInAAEV9+2Tl0Y3n+V16/r3el7FVer2lXFJ9jzBwhJjerq3sCkO6Ls8V03uO//q8NOlxQ6wWxfzyzwePIHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGBG7vkTd16Rim/oudzr1+7POmmP5/+6y2dzAhBcTeo7631nPr1O5b79vp2KH/7n705g6XoCoPwYel20imNq7rfHz713kb74VEYgpgQgwBLq6f0+pj+x0R5Xq3bE6/u8+5+eKj5xdPu5TQxAQPx9hG553HTVIo/XFhZEqvibH/wypaDhyR8AAAAAAACD0fwBAAAAAAAwmDHLvlyPd3/38YM6GR4mntz0YEcVf/bjDl9OC0CwRNZQ4ZtjnLMb1/6ul3v0fTpPv5alXkC51atlvD0efsNClXv/y172+LnZvwsA8/3ler388/wa3h/J/vV3zt8JT314UieL3GIAIcO1N9C6uffLuh9+qZOK30jb5LM5hQKe/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwmDF7/tzW01nP26SRXpv3+YLu9viuly2VO5q91b8TAxAUT9/WWMXd2nxnj+vfoo92l9xtgZgSgABokeByTGuE/m9ce7MqBXg2AILh6aEt7fFtV68v832ufZTj3IFy4fymKpz7zAF7fEGi9/v2rNuS77MphSKe/AEAAAAAADAYzR8AAAAAAACDldtlX0tfbKTidi2X2+Mtu5qr3F/fdI5tPprN45uAsSpXt4d3/kEf4/zN985Sr8OHWOYFmOqqTs7y7n379XeF9745EejpAAiC1s1y7XFsnX2eLyzS4fMfXOF2QbrvJgXAby6oVqxjL5d6LVnVQcWL91b22ZxCEU/+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABis3Oz5079Dgoq7tP1eX1DsrPH/6Nt4lTpckOe3eQEIHY8OcD77NaqtUrkn/lXkfjkAE1TXv/Pbt3T29vvHZxep3Ma97N8BmKhl40QVX3LBHq9e9+bs3ip+5G1qBFAe9e8U5fW1rvv89JoUqZNHtvpqSiGJJ38AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADBbae/5Ua2QPO7Sq7vXLjhzXe3scPrStTG//l74Xqrh+bKSHK0Ue/RdrhIFgu/LyY/Z48epLVG71trLVAQChbVL/81QcV3e/Pd6f1STQ0wEQBEOvqabiBvV2efW637YW+GM6AAKseUKYx9zm7c1V3OcJl7/pS7PHj0tvQkRk2tAaKs445Mzh7fl6z+GMg6HxdwhP/gAAAAAAABiM5g8AAAAAAIDBQnvZV/Epe9i2xXGdC6/kdrGz1GvFb7lev8X91zdTseWcGC+jbtmnck3iN3u8z8PD9Hxq9e1kj49mm31kHBAsPS7WRzynJP9gj1ve3sn9cq/1buXcN+OI7pGn795R5vsC8L2beu/zmPvwB8tjDoA5Ol1yONhTABBI5zdV4ZUdPS+reuuz+iouzNno9dt0vDDBHk8drZeJdrhsgcfX/bGfnl+Tu13+Zjnu3bJUf+DJHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACDhfSeP/1aVrHHvS9fqZPF+ji37bsvsMcZx9z3A3K0SUpQcadL9H4AN125xONrT5yoqeKd+5zj3i6+UB/1Pjc13x4PnKyPmg2Vo96A8u7GntVVvG7Tpfb498wsj68b2kt/Jp97QO/nVavWKnt8Kr+yyo15oYOKX/tqi3eTBeAb5yWqsHr0fhUv/qG9PT6aE5h19XXrODXlQBa/4wF/u7Bhooobxek6IGHe/fftnu303xPVoy/St3FJP/PZTv3iQu/3GAXgW2OvilRxo/o7VJx7wjmGfc1Gfex6iarr/UQfvs35O6DDZd97fZuERnrP34gIZ9+hQu9n43M8+QMAAAAAAGAwmj8AAAAAAAAGC61lX1XjVBgfF+nhQpG9+/XyrffmJtrjbfv1EqwLGiTZ49E3RancTVctUvHBrIb2OG15c5V75VP9yFjNqs6SsW/f0POreZ7rtfo9AfjGPTfpz+9tj13uBKcO6osjou3hY8MzVereKfo4xo/WNrDHg9tWVbl/pq5W8c6My+zx3DVuj4QD8LluDYtV3KiBXtr14ocu3w+KT5X5farUcurC4zdUUbma1fWS8duvc+pC/kldM16e1coeT56pv58AKJtLG+vPoPuSD9FpjwZcudQtdrvAZfnYY8POU6nvf9FLxPo84fKmQTzKGagI4uPCSsxv3pVojxev9/7z+M+R0Sq+vvciD1eKLF2lt4JYt9mpEffd6vkY+GDiyR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg4XUnj9/uEivqX/pYc/Hrr/x6QUqfnqWyzr68/X+Hc+McNYE3tBzocodPRaj4plfO+t3H/inXh/Yqn4dFb/816Mu96mtcvN/rGePMw6uP23+AEqvZeNEFYdX1nv3FBZ5XuTfuYnz+f1qmT4i/qMVnj+jM5fpuP0nXVT8wK3H7PHcNR5vAyBAtu45WabXDeyUqOJn/uIcHZ1QXx/Z+ubs3iqePqedPb53sN4f4MZeTp2aPLNMUwMQAqpVO6Li3p30sc9/vaqnPX7+W/23iBzVNQRAGdRwPlc39Py9xEs37arlEh30eJ27+Hqej4Xfl6GPgb/l6coqvuaSUhwpHyQ8+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgsJDa86dFUlWvr316luc9OpZM1Ov9uyb/6PHaGx+6UMUL0537dmmu1/UtfWuFx/s8O/1KFT/6r3QPVwIoq4a1ikvMr98b5jG3YrdTF1a8X/bS9/ZXJ1T864c/uET1y3xfAL7RoI53n++bOiaoeMZTq1S8Zbfz/aDXny5XuSUb9HeQJvWT7PG9g/X7fDjPdb/ALK/mBqBk+7J1fDinropr1Tzg1X1++q21irftPV/FA6/+zus5PXO/s99Xr3adVe7aKS57ALH/D1AmVSo547jYvSVe+59FBV7ds31T/V3g8ks2qNh1n59B4/X3/ANZ21R8x3W6dxCKePIHAAAAAADAYDR/AAAAAAAADBZSy77Or1ZJ/yDcWcLxydfd3a7erKI2Sc4jW00a7fN4nweeS1GphembVHxBA+fR7XcfdzsWLlwvKXG918tfsMwLCLbfD5VwxGJepudcKaw/VLZjpAEExu4DhZ6T5yXaww8mr1Gpd+fo7xkj3sxxgtxd+j6Vq6vw/XGe68KvW3I9zwdAmazctFPFG3c0VnHH1t4t+1q6to6KX/iP/h4xdVYHe/zqQzrX5uJ1Hu97VVe9VcSITl3t8VtfezU1AOfgrn7OeNZyt6TLkfFP3F2kUtWqHVHxyzPb2+PlG/Xf+2P6N1dx19Yr7fHm7TpXeCo0/n7gyR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg4XUnj+nKbbsoSWej3A+/WXh7j+why2b6H2Fdvy7ioqrRm23x1t2N1C5tvd1VfHR3MNezwnAuQtzLwOn/cD/rm8ZqeKjx2oHfA5ARbYrR/8eP3Kklop7t3f245m71u37QF6GPfzX5/p3+tzlJ/S1ufudcaUolZo/RX/uO7b+wR6Pe6mXys1L13uTADh3KRfpI5UvbrrZw5Ule+CO+Sq+rksLFQ+dXNMeh4VZUlZ3XJtvj9nzByib/AJn35yN2y5SueZN9BHtPdo6+/pdfVlrlTuad8oe9+nyg5Qk3+XE+Im3t1S5sXd9p+Id+xLt8bWPxOgbndguoYAnfwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMFlJ7/sz74ZiKxw1zxgOvXKxyr3zcUcWXJDn/KjWqH/X4HsP7L9A/CNd7hhzMcvb5+du7lVXuaPZWj/cF4H+W+3L7037gBxF6X7ChfXXZfOfzZJco3f/zASq4nRl63fz+g3EqHn3HQnv865ZOKvfOgm32+N5X1usbV9X3+UNygj1+5eEDKhffaJWKH33R2efn2U/0vgMAfG91hv6O/vPvF6i4e/sfy3TfZk1+V/Hyd132GCsu8vo+uSdqqPidOVEergTgtRN77OHi1RerlPueP5WjnP2B3p24W+UKi/TegSWZdO/8s1/0P699Us8eb9u/yevXBRJP/gAAAAAAABiM5g8AAAAAAIDBQmrZV0GhXoJ1Iq+mPa5WNUfllr7tdixbcdmWf7gf0zzza+fYuDk/rXe/HEAQLd2tH/Pet7+Riu/pcZ49fiMts+xv5LLU6/V7k1SqSUO9/OOG52uW/X0AnLPxr+nlFe8/GW2P3xinl2eljmjo8T6VI7JUXC9unz3OPV5d5f76fDcVT/2S49yBQDpxWG/FcM2TTVX89ePt7XHXdroOBMKX37VR8b/mh+YSEKC8mjgzX8Xdk/UysIsucP6Oj62zT/zhsVd6qfjluds8XBk6ePIHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGAhtefPqq16zfzt4539d+4dqPcDurLzSq/v+49Pe9vj9dv1MY2/bdXrBRems88PEKry3db4T3lHH+36wpjVLlE7lXtjyQl7nFwvUuUuTdR98IfvPGyP807qfUC6PlRTT+qonhOAwPrsR/3dYfxrbe1x19bFKtf/yuUe73PoUF0Vvzazpz1+44tclUvfzf4dQChx/36QMtHZA2j8tVeo3ENDnO8K1avn+GwOK9Y43zsGT/P+WHgApXcgS++v0+uhJioec73zub+++x6Va9bkd4/3/XBuDxWvWOd8ll9fVqgvPrFFx8WnPN43VPDkDwAAAAAAgMFo/gAAAAAAABgszLIsr85ID2vaz99zQSlZW78I9hRQwYR6Hbj3WmcZ2PMPrFa5qCp5Hl939FhtFU+dkWyPn/j3dn1xoV7+EWzUAQRaqNeBiog6gEAr13Wgapw9fOjaWip1RQe9HcTJU85/Jz8vWi/p+Gp5NRU/902BExwL/JHP1AEEUnmqAY3qJql4/vOH7PGFSRtVrvfIy1W8eP0u/03Mx7ypATz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwkDrqHQDOxWtfbXEZ13TLusclSffBbAAAQMjJy7SHz83OVKnnZgd6MgD8bc8BvX/nhu1N7fHuTL3v1+IN+lh40/DkDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDB2PMHAAAAAAAYr3/q1mBPIWh48gcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgYZZlWcGeBAAAAAAAAPyDJ38AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADFbumj89evSQsLCwc77PxIkTJSwsTBYvXnzukwIQUNQBANQBANQBoGKjBpROUJo/YWFh6p+oqCiJjY2V5ORkGT58uKSlpUlRUVEwpiZhYWHSo0ePMr3ubP+8//77vp8wUE5RBwBQBwBQB4CKjRoQOGGWZVkBf9P/dedSU1NFRKSoqEhycnIkPT1dli9fLgUFBdKuXTuZMWOGNGvWTL12165dkpubKy1atDinOWRlZUlWVpbEx8dLdHS0mltKSkqpu34TJ04848+PHTsmL7zwgkRERMju3bulXr165zBrwBzUAQDUAQDUAaBiowYEkBUEImJ5euuMjAxr4MCBlohYjRs3tjIzMwM+t5SUFJ/d74033rBExOrfv7/P7gmYgDoAgDoAgDoAVGzUgMAJueaPZVlWUVGR1aNHD0tErNGjR6tcSkrKGV+bn59vpaamWklJSVZkZKSVmJhoPfbYY1Z+fv4Z/0dLTU21RMRatGiRZVmW9c4779jzcv8nNTW1zP+uycnJlohY8+bNK/M9ABNRBwBQBwBQB4CKjRoQOBHePiEUSOHh4TJ+/HhZvHixzJw5U6ZOnVriRk6WZcmNN94oc+fOlQsvvFBGjRolp06dknfffVfS09O9es/WrVtLamqqTJo0SRISEuSuu+6yc2VZ5yci8tNPP8maNWskMTFR+vTpU6Z7ABUVdQAAdQAAdQCo2KgBPhSMjpOcpbtnWf/t1kVERFgiYm3bts3++Zm6e++9954lIla3bt2skydP2j8/fPiw1bx5c6+6e65z89WjXSNGjLBExHrqqad8cj/AJNQBANQBANQBoGKjBgROyB71HhUVJTExMSIicvDgwRKvnT59uoiITJ48WSIjI+2f16xZUyZMmOC/SZbg+PHjMnPmTImIiJA//vGPQZkDUN5RBwBQBwBQB4CKjRrgGyHb/BH57yNbIlLiY10iImvXrpXw8HDp3LnzabmuXbv6ZW5nM3PmTDl27Jj069ePnfyBc0AdAEAdAEAdACo2asC5C9nmT35+vmRnZ4uISGxsbInXHjlyRGrXri0REadvYRQXF+eX+Z3NW2+9JSIiI0aMCMr7AyagDgCgDgCgDgAVGzXAN0K2+bNs2TIpLCyUuLg4SUxMLPHaGjVqSHZ2thQWFp6Wy8zM9NMMPfv5559l9erVkpSUxIZuwDmgDgCgDgCgDgAVGzXAN0Ky+VNcXCxPPfWUiIjceuutZ72+TZs2UlxcLCtWrDgtt2zZslK9d3h4uBQVFZXqNe7efPNNEREZPnz4WR9LA3Bm1AEA1AEA1AGgYqMG+E7INX8OHDgggwYNksWLF0t8fLyMGzfurK8ZMmSIiIiMHz9eCgoK7J8fOXJEnnzyyVK9f0xMjOzevbt0k3Zx4sQJ+fDDD4O+mRNQnlEHAFAHAFAHgIqNGuBbpy+EC6CJEyeKyH+7eTk5OZKeni7Lli2TgoIC6dChg8yYMUPq1Klz1vsMGTJEZs2aJfPmzZNWrVpJv3795NSpUzJ79mxp3769bNy4UcLDvetz9e7dW2bNmiV9+/aV5ORkqVy5snTv3l26d+/u1etnzZolR48elQEDBrChG+AF6gAA6gAA6gBQsVEDAiAY58uLiPonMjLSiomJsZKTk63hw4dbaWlpVlFR0Rlfm5KSYp1p2nl5edaECROsxMREKzIy0kpISLDGjRtn7dmzxxIR6/rrr1fXp6amWiJiLVq0SP08MzPTGjx4sFW3bl0rPDzcEhErNTXV63+3Dh06WCJizZs3z+vXABURdQAAdQAAdQCo2KgBgRNmWf87M81Q3377rfTp00fGjh0rU6ZMCfZ0AAQBdQAAdQAAdQCo2Cp6DQi5PX/Kat++faf97NChQzJ27FgREenfv3+gpwQgwKgDAKgDAKgDQMVGDTizoO7540tjxoyRdevWSefOnSU2Nlb27NkjaWlpkp2dLSNHjpQOHToEe4oA/Iw6AIA6AIA6AFRs1IAzM6b5M2DAAMnMzJQ5c+ZITk6OVKlSRVq2bCnDhg2TYcOGBXt6AAKAOgCAOgCAOgBUbNSAMzN+zx8AAAAAAICKzJg9fwAAAAAAAHA6mj8AAAAAAAAG83rPn7Cm/fw5D5SBtfWLYE8BFQx1IPRQBxBo1IHQQx1AoFEHQg91AIFEDQg93tQAnvwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADBYRLAnEGwP3NBMxZUjKtnj5gmWyt11wyKP90nf1ErFlw475IPZAQBgqMgaKqxTo47HS6+8qNgeh7nlEhtG+2Q6hYX6d/6/Fp+0x1nZu/XFxad88p4AgiDM+W/f1WomqdSwrvq/i9/QM98eN4vfr3LJ98Sr+EDWNl/NEICI+qyKiIy6tmnZbuP2xcFy+XVf6/xIlZv4p/kqvv+Z7md8nYjIJz8WqTjjYOjXAJ78AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADCYkXv+9Gqp1+C2SHDW8nW5TPe7Bl21RL84zG0xn6tiz6mLmqxX8a//usgeX/LHbM8vBBB6wivbwyZxjbx+2basg/oHp477akaAcd4fU1/Ft/5haZBmcmZT7nfGn37TVeUGTtXr/OVY6K/zByqSWjFN7PHgy/V3/5uucPbzSumwwut75p3Q+4sdKEgo4+wA2GrofXzubOv8wf3IXYdVrnmi29/t3gp32/Sn2Pu/9198xOW7idvrHsrUe4ZN/7KlPZ616ITKrd+946zTDASe/AEAAAAAADAYzR8AAAAAAACDhfSyryq1nMfA5owrVLkL4zM9vq5WjY0qrh7t8siY22NfP/zcVsWXX7q6tNP8723D9SPg1aPzynQfoDy5oYPzyPN/ftwZxJmcQbReUnJz6yh7fNs1lVQusrL+/FaOcJ757Nnxe6/fcvzLPVU85aP1Hq4EEFVZP1u9at1l9vhkQeC/nlSqpB/n7pS8xh4P6LNM5fp91UHFX5TtqwOAMrosUS+5evCWKiq+pY+znCsiSv8NsW2n8/fFi+/3UrkItzow6tZF9njud8l6Ekc3ez9hAGf08f36WZQBvb8L0kxKr1HcdhU/NsyJB1/VTOU+nu8sCRs3M0PfKP+Q7yfnAU/+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABgspPb8ueoyfUT7q4846+GSGvpmXW2LQZ1UvPmo7n9dWKOzPU6orfcjeGtclooTGmzx+D6/bnHdb4QjYGGm//ycE9g3PF8fB3l7a/0ZbdLIOYa1e5tjKtez4w8eb+u614iIyOdLa9vjqlFtVK5jm7Ue7zPhbn1k7JSPanq8Fqjobn5W/06VIpe98grzAzsZEZFKUSqc/0x7e9yz4yqV63hJdRWz5w/ge1dcov8ueONRZ1+MBrHrVC6q6kkVvz7L2YPvk4XHVW7xVpfvByfTVa5Nkt5LaNStzvi3bZFnnzSAUmnZ9GCZXpf6xpUqPnz0pIcrfeexYfpv+riYPR6vbRKvexeP3LXJHi/7uaPKfbWWPX8AAAAAAADgAzR/AAAAAAAADEbzBwAAAAAAwGAhtefPX26ppOLS7POTf9JZf//wS8kqt3rjKXu8ee+OEu+z2WUZ8KjrL1a5hAbfe3zdll3NVdzv2bAS3wcwQsGRgL7dr1MPq/jiZhv0BWEunzvL8pj7OK2rSg16pUhfe2S9Pcw81ESlOuotgNR9//mfzm7J9QLAg5OHz35NIEXFqNB9nx9Xn3933GMOgG/E1ND/jXrNhgb2+Ls1jVVuznd5Kv70x41OYLn9ji+j/JO+uQ8Ax03ja6r4/pt62+NhNyzw+LoBPQ+o+NrHzlNxxkHf77n76lwdN6nfRcWbZy33+Xv6Gk/+AAAAAAAAGIzmDwAAAAAAgMGCvuzrmtbOMY7dktNLuFLbtqeZiodNdo5mXrrB8xHspdGoboHX1362WB9HKUe9/3cB4B3L0ssp83Orqvin9c5Szec/0MtIf9zpxBlZW91urB/lfrC/s4zziT95XvohIrJ1u3P8/EMf+v+YSQAAKoJ/L9/hFvv/PSf+0fOfRs/NY7kn4Gvrd+9Q8YhXqtjjTbt6qdytVzlHol/WfJ3KDe15hYrfWeRs2+CPJWCllZHlLFU9eDR428Pw5A8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwYK+58+fBzpTqFY1x+N1i1d3VPEz03V+6YZdZZtAdb1Xz23Jzr4gvTqUvG+P65wWrjpWwpUAfOHRV6ureP3+y1S8ff/2Mt33hg4JKn7y3h/scVRVvY/Ptp1NVdxrTB17nH/YbS8hAOXGXR2jgj0FAEHWIJZ9fYCgKsy3h8/P/l2lPljq7OPz4aOXq9yVHfXf4u8squyHyWl3XxPt9bVffufsV7xq6wZ/TMcrPPkDAAAAAABgMJo/AAAAAAAABgv6sq/pXxbb49haySqXc8w56u26KfrY5sIc3yyvePjqaiqect8Cj9f+vFEvMbnqSedxMl/NB4Bnc9fs9Ml9XI9yFxGZOHKNil2Xeu3ak6hyV/01VsV7DpRtqRmA0FKjuudHxHfuTlLxD/uLPF7br51eRhpVuWxHun68ckeZXgfAd35Y28YJ8jKDNxEA6sj2XmMC//4XN05U8Y29PG87s2Cl3rJm5FtH/DGlUuPJHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACDBX3PH9c17R+v9P/7Xd9er8V/4p7lHq8tLNTHvr792fk6n7PFdxMD4Feun/0p9y1TuUqRxSresv0Ce/zg1Noqt62Mx8kDCLwWjRJVHB1p2eM61S2Ve+gOz5/thAb69/2RT2p5vLZ69e/1D8Kc/8524vh5KvXjby1U/MUSjpsHAqlWTBMVX3rhKhV/Mr+TExTvC8SUAISo2ZMPq7hpwia3K5w9/t7+j9t+f7mhUT948gcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYEHf8yfQPn3BbS1+sXXmC0Xkvmc7qPitr9njBygv+rXT+3u9/+QGe+y+x8/O3UkqvubhOvaYPX6AIKgSo8K+Lavb46SGel+cnm2LVNwo7rg9vuSCNSpXucopn0yv2NL/7Wz3/gb2eN7Kdiq3cNUxe7whs5LK7cygvgDBNPhy/VmOiCpU8T8+zw/kdACEmDt7On8jJDX8SSfd+gjrNl5mjz9ascOf0yoznvwBAAAAAAAwGM0fAAAAAAAAg1WIZV9PDGnpBOEH3LJF4smi3zznAIQe16Venz3vtsQzwul1b952oUpd8Ve9xGTPAZZiAAFXo6k9zJi+TaViY9f7/O127UlUcXyjHR6vHfhwJxV/+sNOtysOeRgDCGWXt9JLMaVIf/dff6BC/KkE4P+5fBcREXngthx7XLmyXga6O6OJiq997DyX6KCvZ+YTPPkDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDAzF7JGRKuwdbM8Jyh228fH7Yi2+57pbo8372PfDyCUXd/e83Hurnv8nPa6R2urmD1+gBBQmGsPDx+toVKxsRn2+Om3eqlc9lF9fPvURS6/591+x7uqq0+Ml59e10c8N6i/xx5/ut5tXxAARqhfR+/h8dP6Nio+kKX3HwNgtuduqaLiS5r/5gTF+tp35+j9gTIOpvtrWj7Dkz8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABjNnz5+oWvZwRA+9n8d13b5zicJUbvocvXfAa0uOO4Hltj8QgKCqU6eJih+/+5iKq1V34swD9VVu9HPx9njjvl1+mB2Ac5K73x4+/LLez+v86h3t8Qcrd+vXFRwt09sd0OVDMg7Fqdh1zx8AAGCeYVfovy3GDJmvLwh39vzbtktfO2vxCb/Ny1948gcAAAAAAMBgNH8AAAAAAAAMVn6XfVXVj2e/P7qmPb71mkUeX3a/y1HuIiKvzN2sL2CpFxBaajjHKK5+da9KNW64U8U7dyfZ4+GT66jcwvQdvp8bAL+Y89POs18EAGUR7SwL79pmnUotWtXG/WoApnH52+KP1+fpXLHldrHTGxgyKUZlft+zw8cT8z+e/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwWLnd86dFTFUVl7TPz8YdF9vjV77c7PE6AKGnV+NT9th9j5/iU/raOyc5+/x89zvHuQPw3pHjUfoHYWHBmQgAv7q9fRV7HFX1pMq99VlxoKcDIMDubOt8zjteurrEa2d/4+wXvHJnlt/mFCg8+QMAAAAAAGAwmj8AAAAAAAAGKzfLvpo3TFTxX/pHe7x2/dZWKr7ykZouUY7P5gTA/7JPOEsv8k7oz/3c75JV/N3vLOsEUDZfLK2i4p4d3Y97BWCCft09//mzq/yv6gBwFo/cddjra1//1GVp6EnvXxeqePIHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGDlZs+fR2/Va/Hv+MN8j9e+9nFtFWcc3OKXOQHwv593OMe7N7ntMpU7kHs80NMBAACGOHq0poq3Hik3fxoB8NKH45qpuHmTJS5RmMrN+ipFxa2SnP3/zq+WoHL/+XGnlDc8+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgsJBe2NqycaI9rlGtsMRrp/37Cns8/5dcf00JQBAdyNoW7CkAMNTLiwpU/PPm9k5wYneAZwPAXy5ukmWPsw7HqNzR7K2Bng4AP7vlmiX6B8XWmS8UkUFXL9bxtc6eQF2HdfTltIKCJ38AAAAAAAAMRvMHAAAAAADAYCG97OvW3tXs8fU9v1W5rbubq3ja585Sr817d/h1XgAAwDDHdqhw6YbgTAOAbz1wgz7m+aJmy+zxax/2cLt6vf8nBKDceOOj3vZ45c79QZyJb/DkDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDBQnrPnyVrjtnjsUN17pGXa6uYfX4AAAAAuKpVo7LH3JfLjgdwJgBC3YdpPVX851fM2geMJ38AAAAAAAAMRvMHAAAAAADAYCG97OubX3bZ40opDdyyOwM7GQAAAADlyuPvpbvFdV2iXQLAbJW61S/F1Rv9No9QwJM/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAYLsyzLCvYkAAAAAAAA4B88+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGo/kDAAAAAABgsHLX/OnRo4eEhYWd830mTpwoYWFhsnjx4nOfFICAog4AoA4AoA4AFRs1oHSC0vwJCwtT/0RFRUlsbKwkJyfL8OHDJS0tTYqKioIxNQkLC5MePXqU6bXbt2+Xe+65R1q0aCHR0dESFxcnnTp1krfeeksKCgp8O1GgnKMOAKAOAKAOABUbNSBwwizLsgL+pv/rzqWmpoqISFFRkeTk5Eh6erosX75cCgoKpF27djJjxgxp1qyZeu2uXbskNzdXWrRocU5zyMrKkqysLImPj5fo6Gg1t5SUlFJ3/VatWiU9e/aUvLw8ufrqq6VVq1Zy9OhRmTNnjuzdu1f69Okj8+bN80lnEjABdQAAdQAAdQCo2KgBAWQFgYhYnt46IyPDGjhwoCUiVuPGja3MzMyAzy0lJaXUr7v22mstEbHeffdd9fPjx49bF198sSUi1pIlS3w0S6D8ow4AoA4AoA4AFRs1IHBCbs+fuLg4mTVrlvTo0UN2794tTz/9tMp7Wtd38uRJmThxojRp0kSioqIkKSlJxo8fLydPnjzj41ru6/reffdd+75LlixRj55NnDjxrPPetm2biIj069dP/bxatWrSu3dvERE5ePCgN/8nACo86gAA6gAA6gBQsVEDfCvkmj8iIuHh4TJ+/HgREZk5c6ZYZ1mZZlmW3HjjjTJp0iSJiIiQUaNGSd++feXdd9+VQYMGefWerVu3th81S0hIkNTUVPsfb9b5tWzZUkRE5s6dq36em5srCxculOjoaOnUqZNXcwFAHQBAHQBAHQAqOmqADwX8WSOr5Ee7/l9+fr4VERFhiYi1bds2++cpKSmnvfa9996zRMTq1q2bdfLkSfvnhw8ftpo3b37Gx7VSU1MtEbEWLVp02tzK8mjXhg0brPr161uVKlWy+vbtaz3yyCPWn/70J6tx48ZW/fr1ra+++qrU9wRMRh0AQB0AQB0AKjZqQOCE5JM/IiJRUVESExMjImd/JGr69OkiIjJ58mSJjIy0f16zZk2ZMGGC/ybpokWLFrJq1Srp3LmzzJkzR5555hl5/fXXJSMjQ26//Xbp2LFjQOYBmIQ6AIA6AIA6AFRs1ADfCNnmj4jYj3SdbRfstWvXSnh4uHTu3Pm0XNeuXf0ytzPNoXPnzpKXlyffffedHDt2THbv3i1PPPGEvPDCC3L55ZfLkSNHAjIXwCTUAQDUAQDUAaBiowacu5Bt/uTn50t2draIiMTGxpZ47ZEjR6R27doSERFxWi4uLs4v83NVWFgoN998sxw8eFDmzJkjXbt2lerVq0ujRo1k7Nixct9998nmzZtl6tSpfp8LYBLqAADqAADqAFCxUQN8I2SbP8uWLZPCwkKJi4uTxMTEEq+tUaOGZGdnS2Fh4Wm5zMxMP83Q8fvvv8uWLVvkoosuknr16p2W79mzp4iI/PTTT36fC2AS6gAA6gAA6gBQsVEDfCMkmz/FxcXy1FNPiYjIrbfeetbr27RpI8XFxbJixYrTcsuWLSvVe4eHh0tRUVGpXnPy5EkREcnKyjpj/v/XJbquOQRQMuoAAOoAAOoAULFRA3wn5Jo/Bw4ckEGDBsnixYslPj5exo0bd9bXDBkyRERExo8fLwUFBfbPjxw5Ik8++WSp3j8mJkZ2795dqte0atVKatasKbt27ZK3335b5XJycuT5558XEZHevXuX6r5ARUUdAEAdAEAdACo2aoBvnb4QLoAmTpwoIv/t5uXk5Eh6erosW7ZMCgoKpEOHDjJjxgypU6fOWe8zZMgQmTVrlsybN09atWol/fr1k1OnTsns2bOlffv2snHjRgkP967P1bt3b5k1a5b07dtXkpOTpXLlytK9e3fp3r27x9dERUXJiy++KEOHDpW7775bZs2aJW3atJHDhw/LF198IQcPHpSOHTvKsGHDvJoDUJFQBwBQBwBQB4CKjRoQAME4X15E1D+RkZFWTEyMlZycbA0fPtxKS0uzioqKzvjalJQU60zTzsvLsyZMmGAlJiZakZGRVkJCgjVu3Dhrz549lohY119/vbo+NTXVEhFr0aJF6ueZmZnW4MGDrbp161rh4eGWiFipqale/XstWbLE6t+/v1WvXj0rIiLCqlatmpWcnGxNmTLFysvL8+oeQEVBHQBAHQBAHQAqNmpA4IRZ1v/OTDPUt99+K3369JGxY8fKlClTgj0dAEFAHQBAHQBAHQAqtopeA0Juz5+y2rdv32k/O3TokIwdO1ZERPr37x/oKQEIMOoAAOoAAOoAULFRA84sqHv++NKYMWNk3bp10rlzZ4mNjZU9e/ZIWlqaZGdny8iRI6VDhw7BniIAP6MOAKAOAKAOABUbNeDMjGn+DBgwQDIzM2XOnDmSk5MjVapUkZYtW8qwYcPYUA2oIKgDAKgDAKgDQMVGDTgz4/f8AQAAAAAAqMiM2fMHAAAAAAAAp6P5AwAAAAAAYDCv9/wJa9rPn/NAGVhbvwj2FFDBUAdCD3UAgUYdCD3UAQQadSD0UAcQSNSA0ONNDeDJHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAINFBHsCAAAA5dmY/s3t8RP3rFa58a+1tccvfr4pYHMCAABwxZM/AAAAAAAABqP5AwAAAAAAYDCaPwAAAAAAAAZjz59zUSXGHq58PlKlGsVl2ePL/9xY5fYd2ObfeQE4zcWNE+3xn2+IVrkGdU6quN8VK+3xp193Ubkffqvk8T2e//q4/sGJPaWcJYBAaelSE0REKleyPF57KE//t7Ldmds9Xls1WteBW68+Yo9f/LwUEwTgG2H69/a0Pzt7dLW7KFvlMrOrqfj3nVXs8ZfLjqncd+6/4o/vOodJAoD/8eQPAAAAAACAwWj+AAAAAAAAGIzmDwAAAAAAgMEq/J4/1Wo1VfGF5xd6vHbfcb1m+NpLnf0BOrT+XuU2bG7hvO7YiXOZIgAfuPMqZx3/PTfPL/niYmc44MqlKjXgSs8ve+SuOBWPf/VCe/zm11vPPkkAvnVeExW+M9LZn++Wq/Tv7agqefq1xc7v+KzDDVSq7Ygkr6ewfW8NlyjT69cB8BGrSIWvfJZrjyfVrKFy/XuuVHFUZFt7fOd1u1UuPLxYxe1HOvVmZ4bnfcEABFedOs5n9d6rqpZ47dC+zl698Y13lnjt6KedfUKnfbNf5cb8oaE9njRyjcqt29RMxV3HHHaCwlzxJZ78AQAAAAAAMBjNHwAAAAAAAIMZs+zr0oQEezyynz6mMaFevsfXtUjKUHFS4y0er5381hUqviixwAnCwlRud+b5ThBBjw0ItkF9PH+23a36pY093nOgutevu6rTOhXf2NtZRvrm117fBsA5SLko3h6PvVMvy+jTaWEJrwzzmKlTa5+Ka1dNUPHt1zjHRWcdqq9yt7x4soT3BOAT1eNVeFMr5/Ncq4b+c6dZgnN8+8Cr9DLwk/l6CcgLM5wasvFAosoteUkv65g7Jccetxp69ikD8KNIZ0nnXV1jVOrhO53f2c2bbPD+nsUlp18au9Rl7J7d7PF17Vv+on8Q4bIMjGVfAAAAAAAA8BbNHwAAAAAAAIPR/AEAAAAAADCYMXv+pFziHN96z80LvH6d+9redz7rZY/7dNRHM48fUcLx0Jbuo32Y5rJ3wLFtAiB0/b71YhV3HOeyb9iRs+wVdH5Te7izZW2Vanux89qhvZqr3Dur3fYXOcpR8EBZ9G2r99+Z+fRv9rhqlWMeX/fSh1eqOCzMUnFK8kF7nPyQPg56yo1VVHzJhT/a48xDDVVOju/yOAcApRDhfO4mDW6qUqNu+VnFNWsdFu9UUlFUdIGKP30u3R4PebyVyh0+qvcEbNnU2TskoV4HlePod8C/atTWNWHZczn2uGWzFR5fd/SY/u7+j0+TVbwn06kJF8ZXVrl7B3nfc3C1+MfLVTz5X24X5PvvewNP/gAAAAAAABiM5g8AAAAAAIDByu2yr/GDW6r40aErPV779mxnKVdWju53Pfap2zHwR51HNtsk6UfJ0144peLYOs7RrwcP1lO591ce8jgfAKHleG6U/sGREpZguR0n++hVzmsb1d/h8WVvT8pS8RVfpaj4tiklzxHAmf3n5VX6B8VF9nD1r/rx7cvHuizpPJqucr1b6c92fkFde/zBnwpV7orL16s4PMI5/3XusgvdZliKY2QBePTO/Yn2eEjfb1Xu182XqHjCaxfZ49eW6yWdSdHO5/Wmzvr3//jha1UcVTnPHtc8Ty8Rqxqll4i9NLOHPd6ZoesLAN8qaZmXiEjLZs4S8A1b9PYO0z5yjn7/z2p9fnvGQbfPbmVneee0kfp7Qmnk5Tr3mTpDb/2wKH1nme9bWjz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwcrvnT7Q+ZVWqRDtrcrfvukDlRk931uTmHy75OOWk+kn2+K+D9HFusbEZKs474azdm/SWXncoBWc5HhpAyEhsoD/bPVs6a3oX7dI98uVP6zX+HVt7ecxjkQ6/Wn7qzNcBKJ1itw9XsbO/x+oN+ghXyd/r8TYLftNHq0ZUcurAVy9/X+IU/rOguz0e+SJ7/AC+0DpR7705pO9Cezzjq54698xGt1d73ntz+xFn/NxsnXtunt6zq2Vt50+lF0bpvUEKTum/EyZ84raPKAC/eeIWvV+X6x4/IiL7Mxvb42vG1lS53Zne/51+9cXO94g/3VK2o91FRO6Y0Moef7kmcHv8uOPJHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACDlds9f+YsP67ia7tcbI8vaa7X/E0b6qzbH/5ajMpVq1pTxc+PDLPHN/ReonLZ2XEqnvx2C3v8etomL2YNIBTVidmv4vlv7PdwZels3dnMHv/9/boqN2NpyfuPAfDO3/55hYrHDv3WHt9z03yVuzjpcns85pWaKhcWpkL5x2Oe68CezCQVT57OHl6Ar+WdcvtQWk6c3OKAzkXX13FuGX+PF55Q4d1/cO57xeV6v48Rk7uo+Gz7igIInNz8qvZ499Fcr19XK6aJiq9PifJwZck+/ba7ij/7JbtM9/E1nvwBAAAAAAAwGM0fAAAAAAAAg5XbZV8rt+sjHFesc5Z2uS/76tPReQxzwHL9WOizo/URz0mNPR/9NunN5iqeNpelXkBFt3b9Zfa44yPnqVzhcZdjpQt5HBzwh8c+0J+t5gnOo9b9ey1Vue5tf7DHc//eSOXCwywVx9Z2Pr/uy7xuHFtPxWu3B+/YVsBUG/fuUPHfpjtLPMcOX6RyB2bGqnjCaxfY4zcXuC0BK/S8BOS1EfrvhGu7brPHtz7WSeX+vXybAAiOL5bqLWBuu0Zvz9I0wfk7/aO/6iVYN//d5fkXtyWiHzyovwtc3W2h13NauqqDPR743FGdzD8koYAnfwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMVm73/JFTep3f0dwwDxeKNKzvrMX/+Hm3dflhbv0vl2V+b3zUW6Vm/ZBfujkCCJoLGyaqOLpKGdfmF+lw+JP6aNd3vi9wghPs6wMEXKH+3XzTc85+Hk/tvFLlXI+Bj4vZo+8Trr9HZBxobI9veIQ9foBge2y6s4fHzAUdVO6V+/Uv69ceX2mP+3bvqHI3POd81sddV0XlBl75q4rbjWhqj3dmbC/ljAH4y8L0XSq+MzVRxXNezrTHN/bR+//923L2AHruwwSVq1GtQLx1OKeuil+Y4dJXyMuUUMSTPwAAAAAAAAaj+QMAAAAAAGCw8rvsy83eTO8f0SrJ5wu62eOps/NULiuLxz2BkFIpSoVXt3KOeXxt7AGVq13L8+OXR4/VVvHcpZfa4+dm6SUl63ZwtCsQ0nL32cM1v1fWuXDPS8QlvJIK9x2sY49Z5gWEgOJT9vC3XTtUqucYfendfZxj2Z9/4DeVOzn3mMe3uPqedipmqRdQPnyVfkTFd43vbI+ff0B/jm+6aqnL2Pv3cF/mNXRiExXP+Sn0vyvw5A8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwcrvnj/heh1/p0td1vG7H99egs/n62ObB0zcck7TAuA/jeomqXjE1dEqfuzu+WW676OvtFTxG2kby3QfAMHXvqlzbOvUMft1stiyhxlZjVUqLMxScWID57Wt4vW6fvf9RgCEln98s9Ue9+5wmcoNvGaZx9c1i49U8be/ergQQGgp0Hv+vL/EiY/m6uPcP33B7btBCbIPO/uJDntC/x1SHvb4cceTPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGK7d7/nw4Vq+5u+WaJU5giddKcSmAIHDdv+OjpzJULr7hNp+8x/Z9BT65D4DAS26i1/LPnuKs5W8Yt0Pl9mQ63x1ueKSeyt3Uo7qKx971rT0e2beVyt33apmmCsBfwiqp8LVRze3xwKsXqdzdqZ3s8aA+RSr38rgVKi4qcq59Y95WAVBOVImxh327RZdwYclmznN+/3+xev05TSkU8OQPAAAAAACAwWj+AAAAAAAAGCykl31Vq9XUHt93pT568ZZrF+uLXdZv/bguWaV+3lTTHo8YuFDl4mrnndMcAfhWmyS9hOOTKc5Sr0b1d5T42uJCp5/96YKuKnfTVUvPfXIAQs5Xf9+l4tjae+2x6zIvEZEbxzpLvdZu10e03tSjpcf36HRplv5B9XgdH9dzABBYt3fXn8mRgxbb4ydf76Vy/5qf7oyXx6ncL9MuUvHYoU492bxHv8eC3/jcAyEjqpYK7+5e0x4P7b9QPDmcU1fFhYW6PRIVadYmMTz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwkN7z58bLiu3xU/e5r9XTfatHX3LW8z47N1vlhlyeb49HDNR3Sd9Ww+2+ewVA8Ewff1zFJe3z8/mCbiqe9rFzZHurJvram64656kBCAFv3q/35Iit4/b9wPnqIPc/r49zX71N7/PjrRrV9P6AVSqfp+J8ARBQNZqq8OWHN3i8dNq8Evb3zMtU4chn9L6D86dtt8fj/xircgsm6jnIUY6CB4Il9aYGKn78nvker/346xR7PP5fp1TupVH62oaxJ899ciGEJ38AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADBZSe/70bhWv4hcf2ujx2v73d1DxF784e/U0qFlH5cb9McvjfXZmhJVmigD84I6UJHvcsukKlfv+57b2uMvDRfqFp/bouNDZeePO65r7boIAgmpID6dGDL/RbY+f8EoqfOG9nvb4sx/TvX6PalUst/s63w/W/F5fpfIPb/L6vgB877bWxSo+v2aOirdsc/bjycrJ8Pq+KzfpfcFGP9faHr858XuV+0tKFxW/PMfrtwFwjh695WIV3zfoF4/XPvh8iopf/PaYE+Tud7s6SUzGkz8AAAAAAAAGo/kDAAAAAABgsJBa9tW9jT469fzzD9vjBSs6qtwXq92Oa42oYg/7t9c9rVo1jjpBmM4dPlIgAALrwoaJKn58xEEn0Cs4pKjI5TObt8/r9xh01QqPuYwDjXR8hOWfQCjr3SHSCYr18qyiU/rz+93a417ds2XjRBXfd+sCFW/ffYE9vutVtyWnAIIqqWF0ifk3P3U59rnQ8zYSZ/P2j059ee2krjXVq1Uu830BlF7Pls4WMX+9Y53K1Txfb/Py6bfd7fGL3xzRN8rLdMY1mqpUg1h97d4DNcoy1ZDFkz8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABgupPX8sy/0HxS45tz05XPb4ERG55fJ69vjlsd+pXPbhOHv8xr97qtzraevLMFMA5yImWn/Y68VkerhSJG2F57W2FzTQxzHe0dvZAyAi4lu3q50a8skCfQz8uh3UASCUXdwkx2Pu6InaKj5V6NSXa1rHq9yF8c53h7/d92OJ7/nTemfPkPzDm72ZJoAQ8fOmPJ/cJzmm0B5XqlxYwpUAfK5KjAo/fc75XVzjvGyV271P/00w8FmXfP4hj2/x7ki9d9elLX5V8dxlV3g11fKCJ38AAAAAAAAMRvMHAAAAAADAYCG17KtOTfd1X46Dh6NU/O2UOBX36rjS42uHP+k8Bvb5KpZ3AMHW9ZKqKo6udtTjtQ/e4Tx+2f5ivVzr0mb7VZzUeItLpJeKzl/RyR6Pfvewt1MFEALSt9ayx8kX6VytmgdU/OVLnpeRlmTaTP1o9+j3vDsyHkDgFRXpvxmK3I5h/yWzbH/iuC8nf+DmSCcIr6RyB7NPluk9AHhn3A367/0a5/1mj3NP6G0h7nm6rn5x/i6P9x3au4k9vrnP9yq3Z3+iiv/5Ta43Uy03ePIHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGAhtefPpl2ej1AcfN1i/YMw3bfKznbWBL486xKV+3zdnnOeGwDfmbc6X8X3Zza2x/Xjdquc634e1/fWe3uUJC+3uopf/dhlf4Dc/QKg/Hj1U+fY5r0Hr1S5scPme3zdynXtVRxdpcAe/2H8eSq37wB7AgLlxT8Wun2PuDVWxfP/dsQeT/+ymcf7xJyvj3l+dMQiFeefqGKPn3itp57DN+neTRZAmURHhXnMffP9ZSpulhDuFjuf++tTdL3o2nqVPY6ILFC5+56tp+Lt+7d7N9lygid/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAwWUnv+vOy2fjcywllbmzriJ5Vb+cvFKp67LMoev/QFa3CBUPbbrh0qvvbBBHuc9oKlcvXqer9n15eLu9rjaR+dUrlvf91ZihkCCCWrtu50GevcY+/WL+GVe0vIHTynOQEInqysbSq+a2K8it9/0tmn4/mHNnh930/Suqj4sX85+5Fu2cffF0CouKH3d25x2e7z/PQrVPzFmk1lnVK5wJM/AAAAAAAABqP5AwAAAAAAYLAwy7Kss18mEta0n7/nglKytn4R7CmggqEOhB7qAAKNOhB6qAMINOpA6KEOIJACUgOiG6iw6OufPFx4ukPZzpHtr/y7lcp9vvy4Pf5ll9v2ElZRKSYYWrypATz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwkDrqHQAAAAAAVHC5+1RYqVv9Mt4o/dznYgie/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIOFWZZlBXsSAAAAAAAA8A+e/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg9H8AQAAAAAAMBjNHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACD0fwBAAAAAAAwWLlr/vTo0UPCwsLO+T4TJ06UsLAwWbx48blPCkBAUQcAUAcAUAcAUAe8F5TmT1hYmPonKipKYmNjJTk5WYYPHy5paWlSVFQUjKlJWFiY9OjRo0yvXbNmjQwcOFDi4uIkMjJS4uPj5d5775XMzEzfThIwAHUAAHUAAHUAAHUgMMIsy7IC/qb/68ylpqaKiEhRUZHk5ORIenq6LF++XAoKCqRdu3YyY8YMadasmXrtrl27JDc3V1q0aHFOc8jKypKsrCyJj4+X6OhoNbeUlJRSd/y+/PJLGTBggBQWFkrfvn2lWbNm8vvvv8vcuXOlYcOGsnz5comPjz+nOQMmoQ4AoA4AoA4AoA4EiBUEImJ5euuMjAxr4MCBlohYjRs3tjIzMwM+t5SUlFK9Ji8vz4qLi7NExJo9e7bKffjhh5aIWH379vXhLIHyjzoAgDoAgDoAgDoQGCHX/LEsyyoqKrJ69OhhiYg1evRolUtJSTnja/Pz863U1FQrKSnJioyMtBITE63HHnvMys/PP+P/YKmpqZaIWIsWLbIsy7Leeecde17u/6Smppb477NgwQJLRKx27dqdMX/ZZZdZYWFh1o4dO0q8D1CRUAcAUAcAUAcAUAcCIyQ3fA4PD5fx48eLiMjMmTPFOsvKNMuy5MYbb5RJkyZJRESEjBo1Svr27SvvvvuuDBo0yKv3bN26tf2YWUJCgqSmptr/nG2NX0ZGhoiINGnS5Iz5Jk2aiGVZsnDhQq/mAoA6AIA6AIA6AIA64CsRAX23UujatatERETIgQMHZMeOHZKUlOTx2g8++EDmzp0r3bp1k/nz50tkZKSIiDzxxBPSsWNHr96vdevW0rp1a5k0aZIkJibKxIkTvZ5rnTp1RERk+/btZ8xv27ZNREQ2btzo9T0BUAcAUAcAUAcAUAd8ISSf/BERiYqKkpiYGBEROXjwYInXTp8+XUREJk+ebP8PKyJSs2ZNmTBhgv8m+T9dunSRmjVryqpVq+Tzzz9XuY8++kjWrVsnIiKHDx/2+1wAk1AHAFAHAFAHAFAHzl3INn9ExH6c6/93//Zk7dq1Eh4eLp07dz4t17VrV7/MzVW1atXkpZdekrCwMBkwYID0799fHn74YenXr58MGjRIWrduLSL/fVwNQOlQBwBQBwBQBwBQB85NyC77ys/Pl+zsbBERiY2NLfHaI0eOSO3atSUi4vR/nbi4OL/Mz92QIUOkcePG8swzz8jixYvlq6++kosuukjeffddOXDggPz8889St27dgMwFMAV1AAB1AAB1AAB14NyFbPNn2bJlUlhYKHFxcZKYmFjitTVq1JDs7GwpLCw87X/gzMxMP85S69mzp/Ts2fO0nw8ZMkRERNq3bx+wuQAmoA4AoA4AoA4AoA6cu5B83rC4uFieeuopERG59dZbz3p9mzZtpLi4WFasWHFabtmyZaV67/DwcCkqKirVa0qSk5Mjc+bMkdjYWLnyyit9dl/AdNQBANQBANQBANQB3wi55s+BAwdk0KBBsnjxYomPj5dx48ad9TX/3zkbP368FBQU2D8/cuSIPPnkk6V6/5iYGNm9e3fpJi0ix44dO+1nubm5cuedd0pOTo488cQTEhUVVer7AhURdQAAdQAAdQAAdcB3grrs6/+PSysuLpacnBxJT0+XZcuWSUFBgXTo0EFmzJhhH5NWkiFDhsisWbNk3rx50qpVK+nXr5+cOnVKZs+eLe3bt5eNGzd6vZlS7969ZdasWdK3b19JTk6WypUrS/fu3aV79+4lvm769Ony97//XXr06CH169eXQ4cOyZw5c2T//v0yevRoueeee7x6f6CioQ4AoA4AoA4AoA74mRUEIqL+iYyMtGJiYqzk5GRr+PDhVlpamlVUVHTG16akpFhnmnZeXp41YcIEKzEx0YqMjLQSEhKscePGWXv27LFExLr++uvV9ampqZaIWIsWLVI/z8zMtAYPHmzVrVvXCg8Pt0TESk1NPeu/08qVK61rrrnGqlevnlW5cmUrJibGuuaaa6yvvvrK2/+zABUKdQAAdQAAdQAAdSAwwizrf+elGerbb7+VPn36yNixY2XKlCnBng6AIKAOAKAOAKAOAKjIdSDk9vwpq3379p32s0OHDsnYsWNFRKR///6BnhKAAKMOAKAOAKAOAKAOnC5kj3ovrTFjxsi6deukc+fOEhsbK3v27JG0tDTJzs6WkSNHSocOHYI9RQB+Rh0AQB0AQB0AQB04nTHNnwEDBkhmZqbMmTNHcnJypEqVKtKyZUsZNmyYDBs2LNjTAxAA1AEA1AEA1AEA1IHTGb/nDwAAAAAAQEVmzJ4/AAAAAAAAOB3NHwAAAAAAAIN5vedPWNN+/pwHysDa+kWwp4AKhjoQeqgDCDTqQOihDiDQqAOhhzqAQKIGhB5vagBP/gAAAAAAABiM5g8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwWj+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwWj+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDBaP4AAAAAAAAYjOYPAAAAAACAwWj+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABiM5g8AAAAAAIDBIoI9AQAAgGCrF9tExQ/eUNUe33/rQpULr6xf+8EX3e3xnc9u9P3kAACAT7j+vv/gkUKV69lxlb7YKnbGYfq5mfSNF6v40gdqOsHRrec0R3/hyR8AAAAAAACD0fwBAAAAAAAwmJHLvjo1S1Dxzb2q2eOU5IMqd9lFv3i8z5jnUlR8ILtAxR1aRdvjjxaeULmVm3Z6N1kAgXFeoj28p6sufY8O3a3iRvV3eLzNuJd7qfiZjzec89QABN4fr9DLvKY+mK7iDduS7PGIyZ1Urn5sFRU/+ecF9vjpGfrajXt3nMs0AfjRpQn6b4ZOLfT3g9cmrPD84rAwFX61qKM9fvPTIpX7cg1/FwDB0qJRooon3uV8znu2X6YvtvTnWootZxxerFItL/hVxV883NUe9xtf+nkGAk/+AAAAAAAAGIzmDwAAAAAAgMFo/gAAAAAAABjMmD1/BndNtMd/H7NL5eJi9jhBuF7H9+0KvTa/bi1n754XHlxc8pu63KtOzR4qtfJvJb8UgH91aR6v4r/de8oed05eqS92Xc8rIqKX9CpPj1qg4qaNetvjEVPZ/wcIKRHRKnz4Bmd/jyfuWa5yz0zvpuLUT484wYltKteuid4n5Mk/O+P9eZXKMlMAfnJLl0QV9+0WZY/799LHOlepqvfwFLevByXlrk1xakqbFo1VbsLrzh5j7yzQ9QSA792R4uzb99z9O1SuevRRe/zjr21V7p05+nvDWy5/Mlx3gT4W/ouXvlfxhh1VyzLVgOLJHwAAAAAAAIPR/AEAAAAAADBY+Vn2VSlKhV0uiFPxm+PX2+NqVXNUbv73ne3xcx/o9RzzN2To94k4zx5+/ph+BPwP3b/zOL21Gz2mAARKjab2cNpfj6jUpc1+sceZBxup3OwFLVT89Urnse+br9RHOt927SIVd77UpYa4LTGRwtyzzxmA34zsXV/FU0YvtMej/9Zd5abN1Ue9l6RnG/1od8YBp6Yczd5amikC8IPbXZZ8PO+25CO2zj57/K9Pe6ncotV5Ko6s7Gzx8PQofVx7XOxej+9fP263iuvVaVbyhAH4lOtSr4xDtVXuyWecrSFmf68/165/S4iIjHDZIebmK/Vaz3+npaj4kY/yyzDTwOLJHwAAAAAAAIPR/AEAAAAAADAYzR8AAAAAAACDlZs9f4amNFTx26nLPF4797uuKu73tMv63dz9Jb7PHV2c/QFK2uNHRGTXXufYxhcWFJZwJYBAWDLppD123eNHROTLpc4eXtdPcN+TY714sn5/koqv7qz3EEmov8cet250scr9vMNtHTEA/zsv0R4+8Sf9WZ85t4c9njZvh9e3rBfbRMXDb/C81weA4Lv1amfsusePiMjDL/Swx3//Uu/NI6eO69hlL78LGl+kUn8ZrK+tVs1lr8EifZsTefydAPjT2Jv15zM2drE9/uhbnZv9vefv/T0bn1JxrRrVnFzHFSr36oc99YuPbvJmqkHFkz8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABgvpPX8m3t7SHk8YOV8ni3X44odX2uMH38/WybPs8+PqkbtyvL52zAt1neCo+x4iAAIt96TnkvaV523CzknOsZr2eNvRkC6pgJkqRanwt5edfTf2Z8Wo3O2vOvuCSdFJ8daHj+o9AC5I2qLiZ/7ZyyVK9/q+APyjRrUCj7nCIsslyFO5fu0SPL6u06XHVKz2+HHzz897qfjlLzZ4vBbAubu+e47+gVV8xuvOZlH6LhU/MNhlzz+3e27Zrb8blAc8+QMAAAAAAGAwmj8AAAAAAAAGC6k1CuMG6WOSXZd6nTqpH+ues/RyFT/4jnPcshR4fgxTImuo8IbWtVSc1PBXJwgPU7mJr1+h4s9+5NFuIJSEuX5k3T6/R467nLvqVgdaxtVW8W1XOMc6tm1xQuX2HohV8Z2TnWuPZrP8Ewi0wZ3rq/iiC1ba4yvv0d8V5NgOr+97W/cke5ySrI93PXH8fBWP+zTf6/sC8L+MQ1U95lJH/maPH7y9usodyz2q4hZNPR8J7e7jr1Ps8YjXMrx+HYBzN/RvuleQPst5xqVH20yVe+XPTs8hbcVxlRtyXRUVX5eyxB7v3NNU5V5eUralZcHEkz8AAAAAAAAGo/kDAAAAAABgMJo/AAAAAAAABgv+nj/VGtnDUTf/rnPFzlGM7nv8DHxis9dvkVTfWbf/wTh9tGvHS1e6X26bObenip/8lPW7QCi79EKX4xld6oeIyKhbnOMY7xuk1/h3vFTv5+Fq8KOdVfzRiiy3K9xjAIE0+Cr937HWb25pjxduPOD1fSJq6rX8f39gmxNU0tdOndFe/+AIewACoeSPrzjfAerFtFO5Tm1W2+Pzaxzy2XsuXOWyt+DJwz67L4Cz+33PDhV/lNbNHt/cZ7HKtWzm7Pt178367wX3PUNd/57IPKT3+5Oj5W+vT578AQAAAAAAMBjNHwAAAAAAAIMFf9lXhHMsW1zMHo+X/fm1Iv2D8/Xj2X+9orI97ttVH9nWuoVzTGP16jn6Pu4ntFnOo14fL3A7ujXfd4+GAvC9rMO17PH5bp/1TpetcoLTHunU4Ym8mvb4t90+mhwAv7iuh16+Pf5llyXbhSUc0xytj4hfkKqXhcfW2WePX/mwt8qlzs4u5SwBBNLRbGc5Rtcx0ToZmewS6O8DI7rpI+JfH7/M43v889NeKn5rwfbSTRKA3wx+apM9nr2gk8pd29X5nF+UeETlqkfr7wIXX+gs635njlstKYd48gcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYMHf86fQWVeXkdVYperVcTbb2P/Zcv06t2OcS7I7o4k9zjlWU+Uaxen1uRmHnKPnP1+10+v3ABB8lw5z9uXq3KyVyjWq4/S6Zz6j9whx9++v29rj9bs3+Gh2AHyhz6Xxbj/JVNHcH054fO21bZzXTns4S+USGuvvA5u2trDH90/P0TfK3X/2iQIIDYW5HuMWjRJVavRgz/t5zV+h9w0Z8bbeG0QK3fYKBRASPvl+p1vs+dqi79x+v5ei51Ae8OQPAAAAAACAwWj+AAAAAAAAGIzmDwAAAAAAgMGCv+fPiT328KZHE1Tq8+cK7XFM7QyV27jjYhV/trihPZ4xX6/333Sskj1Oe6xQ5dz3/JmR1sIlSi9h4gBC2YpNu1R86ckED1ee7pl/s24fCFUZR8JUfDIvSsUzUo/Z4xrVda5ezFp7nF8QrW8cpu/7+id1nSB3c1mmCiAE1a3j7AU66oYqKteiqd4TsLAg0h6Pf8tt749j23w/OQABdVNHt78PwvQ+ggez69njt77eGogp+RVP/gAAAAAAABiM5g8AAAAAAIDBgr/sy8XKTfoYtrrXuz6CXd/t6pyzxI6eLZ2jXXt1Xq2Txfox7137C0qcI4DyqVW8y2c9vJJbtiigcwFQdr/s1N8V/vR0GxXfcZ3zeV67MVblPl3kPL794oP60e6Vv1yk4pe/1nkAZrjmEmf51p9uWaCTbsc6j3+tiz1elel2ZDyAcqlFo0R7/M5Et21erGIVDp3U2CXSW0qURzz5AwAAAAAAYDCaPwAAAAAAAAaj+QMAAAAAAGCwkNrzx1+qRroExW57e7it7Z32nV7nB8AM+QUun3W3OvDtik4qzi3Ue4EBCF3TF213i12CMP3fuJ6/29nXp0Gs3gPwlsfi9I0LjvpkfgCCq3WiPsr57ce/dwK3vwNennmFip+bvd5v8wIQHE3rOp/76Gr6d/2P65JVnPZz+d/nxxVP/gAAAAAAABiM5g8AAAAAAIDBaP4AAAAAAAAYrELs+fPVWrPW6gE4uxaNElV8+zVOucs82FDl/vW5fu2+A9v8NS0AAdS7ZSMVP3DHfHs86Q29t8cPW9IDMicA/lelVlN7PGmYzoVHOPt7/riurco98AZ7/ACmcf+b4J8TdjqB275fz38QKSbjyR8AAAAAAACD0fwBAAAAAAAwWIVY9nVtm3iXaH/Q5gHAz6Ib2MO05zNVKr7+Vnv816k9VO6jFZv8Oi0AwTF94j4V79mfaI+fmH0gwLMBEChLnz5uj9u2+lnltuxoZo/ve8HsJR4ARG7oXFXFsbEZ9vig21YQs7/fKSbjyR8AAAAAAACD0fwBAAAAAAAwGM0fAAAAAAAAg1WIPX8S6rGeF6gIpv2xpj2Or79a5d6b08seT/3P74GaEoAA63BBgj2Oi/lR5e57prMT5G8VAOVUeGUV3tGtkYrbXrzcHuflnqdyL34YZ49Xb9vih8kBCLoaTe3h8Bt26ZxVbA+f/tcFbi/c7MdJBR9P/gAAAAAAABiM5g8AAAAAAIDBKsSyr+XrTzlBeCW3bFFA5wLAd666LF7FQ/t9b49z885XuS+W5gVkTgACLLKGCqc9mG+Pd+5vonJvLM4JxIwA+Fm7xAYqfvLe/So+nlvLHj827VKVez3N7GUdAERm/cX5mz+psV7e+fz0K+zxy3PSAzanUMCTPwAAAAAAAAaj+QMAAAAAAGAwmj8AAAAAAAAGqxB7/vyyc6c93rTlQpVrlrhBxZ3rOfsDrTjq33kBKJ0GdfX+HTMme163P3TixSr+7MedHq4EUJ7d07uuitu2+s4et7m9o744nzoAlFfNGyba46+mble5mNoZKv7bP509Pd5Yni8AzHZTxwQVD7xqiT3+dWMrlXvko4pbE3jyBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgFWLPH1fPTo9R8dupOv/4MKcfNmZaosqt373DT7MC4FHk+fbwoQFVVKpWjUwVz0zraY8/+X6jf+cFICT8eWC2itekt7bHv+zJEADlVHQDFc54PNceu+/xM39FJxVP/TLPHhfmbPPD5AAEVRX9N/3jdx/X+fBK9vCzRfV07mi6v2YV8njyBwAAAAAAwGA0fwAAAAAAAAxW4ZZ9vfN9nooHfNdVxdd2W2aPH8vpoXK3TdWPl0n+IZ/ODcDp/tQ71h7/5bYFKrd01eUqvv3vewMyJwChI6Zmjoqf/EcTJyjSS0MBhLa6dZzP75K/6+/Z9WIP2OMXP7hC5R78pz76XQpzBYC52jesruKWF6xU8UKXpaCvf63//q/IePIHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGAVbs8fyd2vwr5P11fxC3dcaY9H3/qtyj31QWcVr9/Nnj+Ar3W8MEHFjw7dZY8nvq7X+E9zX8N7yu2YRwDGazCg2O0nW4IyDwBlUClKhU/d4cSr1uvjmWd+fb49Tvu54h7VDEBk1dadKq6U0sDtip2C0/HkDwAAAAAAgMFo/gAAAAAAABis4i37cue2DGzMm/tdxu6Pj+3w/3yACu77zfoxzfibXSMe8wYAwBhFJ1V499QNQZoIAJiPJ38AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADBZmWZYV7EkAAAAAAADAP3jyBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBgNH8AAAAAAAAMRvMHAAAAAADAYDR/AAAAAAAADEbzBwAAAAAAwGA0fwAAAAAAAAxG8wcAAAAAAMBg/wedXCRC3gltWwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1400x1400 with 50 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "###################################################\n",
        "## Visualize 5 samples from each class (1 point) ##\n",
        "##################[Your Code]######################\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(10, 5, figsize=(14, 14))\n",
        "\n",
        "for digit in range(10):\n",
        "    digit_indices = np.where(y_train == digit)[0][:5]\n",
        "    \n",
        "    for i, idx in enumerate(digit_indices):\n",
        "        axes[digit, i].imshow(x_train[idx].reshape(28, 28), cmap='cividis')  # Changed color map to 'cividis'\n",
        "        axes[digit, i].set_title(f\"Digit {digit}\")\n",
        "        axes[digit, i].axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0_ki4uUcAMR"
      },
      "source": [
        "Now normalize the data (mean=0 and std=1) and also flattern the input because we have just linear layers and we need two have a vector as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0bgNGW_9S6kE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape = (42000, 784), x_val shape = (14000, 784), x_test shape = (14000, 784)\n"
          ]
        }
      ],
      "source": [
        "#####################################################\n",
        "##        Normalize and flatten X (2 points)       ##\n",
        "####################[Your Code]######################\n",
        "x_train_normalized = (x_train - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-8)\n",
        "x_val_normalized = (x_val - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-8)\n",
        "x_test_normalized = (x_test - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-8)\n",
        "x_train_normalized = x_train.reshape(x_train.shape[0], -1)\n",
        "x_val_normalized = x_val.reshape(x_val.shape[0], -1)\n",
        "x_test_normalized = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "#####################################################\n",
        "# Print shapes to verify\n",
        "print(f\"x_train shape = {x_train_normalized.shape}, x_val shape = {x_val_normalized.shape}, x_test shape = {x_test_normalized.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Qo6s49S6kE"
      },
      "source": [
        "## Train and Test Model (35 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrtIQY1KS6kE"
      },
      "source": [
        "### Batch Sampler (10 Points)\n",
        "We need to sample bathces from our dataset to train model. Complete the following class to have a random sampler (8 Points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QR3sYrH1S6kE"
      },
      "outputs": [],
      "source": [
        "class RandomSampler(object):\n",
        "    def __init__(self, batch_size, dataset, dtype):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch_size : sampler batch size\n",
        "            dataset : dataset we want to get batch from that\n",
        "            dtype : one of {'train', 'test', 'val'}\n",
        "        \"\"\"\n",
        "        #####################################################\n",
        "        ##          Initialize your RandomSampler          ##\n",
        "        ####################[Your Code]######################\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset\n",
        "        self.dtype = dtype\n",
        "        \n",
        "        self.data, self.labels = dataset[0], dataset[1]\n",
        "        \n",
        "        self.num_batches = len(self.data) // self.batch_size\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        #####################################################\n",
        "        ##          Return length of your sampler          ##\n",
        "        ####################[Your Code]######################\n",
        "\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        This function is called when we iterate an object of this class and\n",
        "        yields one batch on each call.\n",
        "\n",
        "        Yields:\n",
        "            (x, y) : tuple of bathces of x and y\n",
        "        \"\"\"\n",
        "        #######################################################\n",
        "        ##  Yield a tuple of x and y from specified dataset  ##\n",
        "        #####################[Your Code]#######################\n",
        "        indices = np.arange(len(self.data))\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "        for start_idx in range(0, len(self.data), self.batch_size):\n",
        "            end_idx = start_idx + self.batch_size\n",
        "            if end_idx > len(self.data):\n",
        "                break\n",
        "            \n",
        "            batch_indices = indices[start_idx:end_idx]\n",
        "            yield self.data[batch_indices], self.labels[batch_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR5ZLmCPS6kF"
      },
      "source": [
        "Fill the following functions to update a confusion matrix and calculate f1 score for a confusion matrix (2 Points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "X6XIJfSBS6kF"
      },
      "outputs": [],
      "source": [
        "def update_confusion_matrix(conf_matrix, preds, reals):\n",
        "    \"\"\"\n",
        "    Updates confusion matrix\n",
        "\n",
        "    Args:\n",
        "        conf_matrix : input confusion matrix\n",
        "        preds : array of predicted labels\n",
        "        reals : array of real labels\n",
        "\n",
        "    Returns:\n",
        "        conf_matrix : updated confusion matrix\n",
        "    \"\"\"\n",
        "    #######################################################\n",
        "    ##           Update the confusion matrix             ##\n",
        "    #####################[Your Code]#######################\n",
        "\n",
        "    for real, pred in zip(reals, preds):\n",
        "        conf_matrix[real, pred] += 1\n",
        "        \n",
        "    return conf_matrix\n",
        "\n",
        "\n",
        "def f1_score(confusion_matrix):\n",
        "    \"\"\"\n",
        "    calculate macro f1 score from given confusion matrix\n",
        "\n",
        "    Args:\n",
        "        confusion_matrix : given confusion matrix\n",
        "\n",
        "    Returns:\n",
        "        f1 : macro f1 score\n",
        "    \"\"\"\n",
        "    #######################################################\n",
        "    ##                Calculate f1 score                 ##\n",
        "    #####################[Your Code]#######################\n",
        "\n",
        "    num_classes = confusion_matrix.shape[0]\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        tp = confusion_matrix[i, i]\n",
        "        fp = np.sum(confusion_matrix[:, i]) - tp\n",
        "        fn = np.sum(confusion_matrix[i, :]) - tp\n",
        "        if tp + fp > 0:\n",
        "            precision = tp / (tp + fp)\n",
        "        else:\n",
        "            precision = 0\n",
        "        if tp + fn > 0:\n",
        "            recall = tp / (tp + fn)\n",
        "        else:\n",
        "            recall = 0\n",
        "        if precision + recall > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        else:\n",
        "            f1 = 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return np.mean(f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Amgy2PS6kF"
      },
      "source": [
        "### Define Model (5 Points)\n",
        "Define an MLP model to solve the classification problem. Try to define a good initializer function (not just a simple random initializer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "93R2oHjOS6kJ"
      },
      "outputs": [],
      "source": [
        "def initializer(x, y):\n",
        "    #######################################################\n",
        "    ##         Define your initilizer (1 Point)          ##\n",
        "    #####################[Your Code]#######################\n",
        "    \n",
        "    std_dev = np.sqrt(2 / x)\n",
        "    return np.random.normal(0, std_dev, size=(x, y))\n",
        "\n",
        "\n",
        "model = MLPModel([\n",
        "    #######################################################\n",
        "    ##           Define your model (1 Point)             ##\n",
        "    #####################[Your Code]#######################\n",
        "    LinearLayer(784, 256, initializer=initializer, reg=True, alpha=0.001),\n",
        "    Dropout(0.2),\n",
        "    ReLU(),\n",
        "    BatchNorm(256),\n",
        "    LinearLayer(256, 128, initializer=initializer, reg=True, alpha=0.001),\n",
        "    ReLU(),\n",
        "    BatchNorm(128),\n",
        "    LinearLayer(128, 10, initializer=initializer, reg=True, alpha=0.001),\n",
        "    LogSoftMax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "c2mr6NZPS6kK"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "##           Initialize hyperparameters              ##\n",
        "#####################[Your Code]#######################\n",
        "\n",
        "n_epochs = 15\n",
        "batch_size = 64\n",
        "\n",
        "def lr(epoch):\n",
        "    #######################################################\n",
        "    ## Define LR Scheduler for your optimizer (2 Points) ##\n",
        "    #####################[Your Code]#######################\n",
        "    \n",
        "    return 0.09 / (1 + 0.6* epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Ry5WVxRIS6kK"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "## Define optimizer, loss and sampler (1 Point) ##\n",
        "##################[Your Code]#####################\n",
        "optimizer = Optimizer(\n",
        "    layers=model.get_parameters(),\n",
        "    strategy='adam',\n",
        "    lr=lr\n",
        ")\n",
        "\n",
        "cross_entropy = CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRiV9H-jS6kK"
      },
      "source": [
        "### Train Model (17 Points)\n",
        "\n",
        "Fill in the below cell to train the model. Store each epoch loss, accuracy and f1-score. Use f1-score to choose best epoch.\n",
        "\n",
        "**Note1**: To do backpropagation you need to first call `backward` function of criterion with 1 as its argument to have gradient of loss w.r.t output of this module and then using model `backward` function with `criterion.grads['x']` argument.\n",
        "\n",
        "**Note2**: You can ignore regularization term in your total loss value and just use criterion, but you must consider that during updating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ecZeuL1VS6kK",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Train - Loss: 14.6442, Accuracy: 0.2280, F1-score: 0.1878\n",
            "Epoch [1/15], Validation - Loss: 25.1651, Accuracy: 0.1547, F1-score: 0.0843\n",
            "Epoch [2/15], Train - Loss: 12.3048, Accuracy: 0.3803, F1-score: 0.2846\n",
            "Epoch [2/15], Validation - Loss: 16.9874, Accuracy: 0.2818, F1-score: 0.1845\n",
            "Epoch [3/15], Train - Loss: 9.9564, Accuracy: 0.4631, F1-score: 0.3466\n",
            "Epoch [3/15], Validation - Loss: 12.7952, Accuracy: 0.4149, F1-score: 0.3206\n",
            "Epoch [4/15], Train - Loss: 8.5303, Accuracy: 0.4958, F1-score: 0.3720\n",
            "Epoch [4/15], Validation - Loss: 9.5951, Accuracy: 0.4799, F1-score: 0.3689\n",
            "Epoch [5/15], Train - Loss: 8.0530, Accuracy: 0.5120, F1-score: 0.3852\n",
            "Epoch [5/15], Validation - Loss: 8.4471, Accuracy: 0.5003, F1-score: 0.3824\n",
            "Epoch [6/15], Train - Loss: 7.8245, Accuracy: 0.5208, F1-score: 0.3912\n",
            "Epoch [6/15], Validation - Loss: 7.5563, Accuracy: 0.4809, F1-score: 0.3821\n",
            "Epoch [7/15], Train - Loss: 7.3232, Accuracy: 0.5606, F1-score: 0.4567\n",
            "Epoch [7/15], Validation - Loss: 6.5175, Accuracy: 0.5920, F1-score: 0.4939\n",
            "Epoch [8/15], Train - Loss: 7.4753, Accuracy: 0.5939, F1-score: 0.4908\n",
            "Epoch [8/15], Validation - Loss: 8.3880, Accuracy: 0.5696, F1-score: 0.4820\n",
            "Epoch [9/15], Train - Loss: 7.5338, Accuracy: 0.5981, F1-score: 0.4945\n",
            "Epoch [9/15], Validation - Loss: 8.2323, Accuracy: 0.5613, F1-score: 0.4790\n",
            "Epoch [10/15], Train - Loss: 7.3533, Accuracy: 0.6023, F1-score: 0.4984\n",
            "Epoch [10/15], Validation - Loss: 7.8578, Accuracy: 0.6009, F1-score: 0.5071\n",
            "Epoch [11/15], Train - Loss: 7.6994, Accuracy: 0.6044, F1-score: 0.5003\n",
            "Epoch [11/15], Validation - Loss: 7.5780, Accuracy: 0.5770, F1-score: 0.4916\n",
            "Epoch [12/15], Train - Loss: 8.0396, Accuracy: 0.6066, F1-score: 0.5030\n",
            "Epoch [12/15], Validation - Loss: 9.2688, Accuracy: 0.5772, F1-score: 0.4933\n",
            "Epoch [13/15], Train - Loss: 8.0394, Accuracy: 0.6132, F1-score: 0.5088\n",
            "Epoch [13/15], Validation - Loss: 8.5888, Accuracy: 0.5821, F1-score: 0.4926\n",
            "Epoch [14/15], Train - Loss: 7.8371, Accuracy: 0.6168, F1-score: 0.5109\n",
            "Epoch [14/15], Validation - Loss: 8.5299, Accuracy: 0.6093, F1-score: 0.5177\n",
            "Epoch [15/15], Train - Loss: 7.7777, Accuracy: 0.6164, F1-score: 0.5109\n",
            "Epoch [15/15], Validation - Loss: 7.7676, Accuracy: 0.6168, F1-score: 0.5163\n",
            "Best validation F1-score: 0.5177 at epoch 14\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "##    Train and validation loop (15 Points)     ##\n",
        "##################[Your Code]#####################\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "train_f1, val_f1 = [], []\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "\n",
        "train_metrics_list = []\n",
        "val_metrics_list = []\n",
        "\n",
        "train_sampler = RandomSampler(batch_size, (x_train_normalized, y_train), 'train')\n",
        "val_sampler = RandomSampler(batch_size, (x_val_normalized, y_val), 'val')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "        ##################################################\n",
        "        ##                 Train Phase                  ##\n",
        "        ##################[Your Code]#####################\n",
        "        \n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_train_conf_matrix = np.zeros((10, 10), dtype=int)\n",
        "\n",
        "    for x_batch, y_batch in train_sampler:\n",
        "        predictions = model._forward(x_batch, mode='train')\n",
        "        \n",
        "        train_loss = cross_entropy._forward(predictions, y_batch)\n",
        "        epoch_train_loss += train_loss\n",
        "        \n",
        "        upstream_grad = 1\n",
        "        cross_entropy.backward(upstream_grad)\n",
        "        model.backward(cross_entropy.grads['x'])\n",
        "        \n",
        "        optimizer.step(epoch)\n",
        "        \n",
        "        epoch_train_conf_matrix = update_confusion_matrix(epoch_train_conf_matrix, \n",
        "                                                         np.argmax(predictions, axis=1), \n",
        "                                                         y_batch)\n",
        "    \n",
        "    epoch_train_loss /= len(train_sampler)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    \n",
        "    epoch_train_acc = np.trace(epoch_train_conf_matrix) / np.sum(epoch_train_conf_matrix)\n",
        "    train_accs.append(epoch_train_acc)\n",
        "    \n",
        "    epoch_train_f1 = f1_score(epoch_train_conf_matrix)\n",
        "    train_f1.append(epoch_train_f1)\n",
        "\n",
        "        ##################################################\n",
        "        ##      Save epoch metrics for train phase      ##\n",
        "        ##################[Your Code]#####################\n",
        "        \n",
        "    train_metrics = {\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': epoch_train_loss,\n",
        "        'accuracy': epoch_train_acc,\n",
        "        'f1_score': epoch_train_f1\n",
        "    }\n",
        "    train_metrics_list.append(train_metrics)\n",
        "    print(f\"Epoch [{epoch + 1}/{n_epochs}], Train - Loss: {epoch_train_loss:.4f}, Accuracy: {epoch_train_acc:.4f}, F1-score: {epoch_train_f1:.4f}\")\n",
        "    \n",
        "        ##################################################\n",
        "        ##               Validation Phase               ##\n",
        "        ##################[Your Code]#####################\n",
        "    \n",
        "    epoch_val_loss = 0.0\n",
        "    epoch_val_conf_matrix = np.zeros((10, 10), dtype=int)\n",
        "\n",
        "    for x_batch, y_batch in val_sampler:\n",
        "        predictions = model._forward(x_batch, mode='valid')\n",
        "        \n",
        "        val_loss = cross_entropy._forward(predictions, y_batch)\n",
        "        epoch_val_loss += val_loss\n",
        "        \n",
        "        epoch_val_conf_matrix = update_confusion_matrix(epoch_val_conf_matrix, \n",
        "                                                       np.argmax(predictions, axis=1), \n",
        "                                                       y_batch)\n",
        "    \n",
        "    epoch_val_loss /= len(val_sampler)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    \n",
        "    epoch_val_acc = np.trace(epoch_val_conf_matrix) / np.sum(epoch_val_conf_matrix)\n",
        "    val_accs.append(epoch_val_acc)\n",
        "    \n",
        "    epoch_val_f1 = f1_score(epoch_val_conf_matrix)\n",
        "    val_f1.append(epoch_val_f1)\n",
        "\n",
        "        ##################################################\n",
        "        ##    Save epoch metrics for validation phase   ##\n",
        "        ##################[Your Code]#####################\n",
        "        \n",
        "    validation_metrics = {\n",
        "        'epoch': epoch + 1,\n",
        "        'loss': epoch_val_loss,\n",
        "        'accuracy': epoch_val_acc,\n",
        "        'f1_score': epoch_val_f1\n",
        "    }\n",
        "    val_metrics_list.append(validation_metrics)\n",
        "    print(f\"Epoch [{epoch + 1}/{n_epochs}], Validation - Loss: {epoch_val_loss:.4f}, Accuracy: {epoch_val_acc:.4f}, F1-score: {epoch_val_f1:.4f}\")\n",
        "\n",
        "        ##################################################\n",
        "        ##          Save best model and best f1         ##\n",
        "        ##################[Your Code]#####################\n",
        "    \n",
        "    if epoch_val_f1 > best_f1:\n",
        "        best_f1 = epoch_val_f1\n",
        "        best_model = model\n",
        "        \n",
        "print(f\"Best validation F1-score: {best_f1:.4f} at epoch {np.argmax(val_f1) + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "tF76W08HS6kL"
      },
      "outputs": [],
      "source": [
        "##################################################################################################\n",
        "## Plot train and validation loss, accuracy and f1 graphs and lr value at each epoch (2 Points) ##\n",
        "#######################################[Your Code]################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqVkMjxS6kK"
      },
      "source": [
        "### Test Model (3 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nH7WTpBES6kL"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "##                  Test your best model                  ##\n",
        "##          Report loss, accuracy and f1 metrics          ##\n",
        "##      Also plot the confusion matrix for test data      ##\n",
        "#######################[Your Code]##########################\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "4cdab81cfa56356ab5b949f60df5e1e80f63026375d4bba17a1c6608f8c1fd40"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
